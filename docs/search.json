[
  {
    "objectID": "interaction/miao_2025_40410536.html",
    "href": "interaction/miao_2025_40410536.html",
    "title": "PIGEON: a statistical framework for estimating gene-environment interaction for polygenic traits",
    "section": "",
    "text": "PubMed: 40410536 DOI: 10.1038/s41562-025-02202-9 Overview generated by: Gemini 2.5 Flash, 28/11/2025"
  },
  {
    "objectID": "interaction/miao_2025_40410536.html#key-findings-and-motivation",
    "href": "interaction/miao_2025_40410536.html#key-findings-and-motivation",
    "title": "PIGEON: a statistical framework for estimating gene-environment interaction for polygenic traits",
    "section": "Key Findings and Motivation",
    "text": "Key Findings and Motivation\nUnderstanding Gene-Environment Interaction (GxE) is crucial for deciphering the genetic architecture of human complex traits. The authors introduce the PIGEON (PolygenIc Gene-Environment interactiON) framework to address the challenges in scalability and interpretability faced by current GxE methods. PIGEON provides a unified and statistically grounded approach for quantifying polygenic GxE effects, requiring only summary statistics data as input."
  },
  {
    "objectID": "interaction/miao_2025_40410536.html#study-design-and-methods",
    "href": "interaction/miao_2025_40410536.html#study-design-and-methods",
    "title": "PIGEON: a statistical framework for estimating gene-environment interaction for polygenic traits",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\n\nPIGEON Framework\nPIGEON is a unified statistical framework designed to model polygenic GxE effects for complex traits using a variance component analytical approach. It allows researchers to define the parameters of interest and systematically compare different existing GxE methodologies, providing a clear map of the GxE landscape.\n\n\nEstimation Procedure\nThe core methodology of PIGEON is an extension of Linkage Disequilibrium (LD) score regression adapted for GxE analysis. The key feature of the PIGEON estimation procedure is its reliance solely on Genome-Wide Interaction Study (GWIS) and Genome-Wide Association Study (GWAS) summary statistics, avoiding the need for individual-level genotype data. The framework outlines two main objectives in polygenic GxE inference:\n\nDetecting GxE: Estimating the GxE variance component (\\(\\sigma_I^2\\)), where a value greater than zero indicates the presence of GxE.\nInterpreting GxE: Estimating covariant GxE (\\(\\rho_{GI}\\)) and Oracle Polygenic Score GxE (PGSxE), which quantifies the interaction between the environment and an individual’s true additive genetic component (PGS). The estimation of Oracle PGSxE is shown to be equivalent to estimating covariant GxE.\n\n\n\nAnalytical Advantages\nThe PIGEON method is robust to issues such as arbitrary sample overlap between GWAS and GWIS data and heteroskedasticity (non-constant residual variance across environments), which often complicate traditional GxE methods."
  },
  {
    "objectID": "interaction/miao_2025_40410536.html#results-and-empirical-application",
    "href": "interaction/miao_2025_40410536.html#results-and-empirical-application",
    "title": "PIGEON: a statistical framework for estimating gene-environment interaction for polygenic traits",
    "section": "Results and Empirical Application",
    "text": "Results and Empirical Application\nThe paper demonstrates the effectiveness of PIGEON through extensive theoretical and empirical analyses:\n\nGene-by-Education Interaction: A quasi-experimental study of gene-by-education interaction was performed on health outcomes, showcasing PIGEON’s ability to analyze real-world policy-relevant exposures.\nGene-by-Sex Interaction: The method was successfully applied to quantify gene-by-sex interaction for a large set of 530 traits using data from the UK Biobank.\nGene-by-Treatment Interaction: PIGEON was used to identify genetic interactors that help explain the heterogeneity of treatment effects in a clinical trial focused on smoking cessation."
  },
  {
    "objectID": "interaction/miao_2025_40410536.html#conclusions-and-recommendations",
    "href": "interaction/miao_2025_40410536.html#conclusions-and-recommendations",
    "title": "PIGEON: a statistical framework for estimating gene-environment interaction for polygenic traits",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nPIGEON provides an innovative and rigorous solution to long-standing challenges in polygenic GxE inference. By leveraging summary statistics and a unified variance component framework, the paper suggests a promising path that may fundamentally reshape analytical strategies in future GxE studies for complex human traits."
  },
  {
    "objectID": "proteomics/brandes_2020_32665031.html",
    "href": "proteomics/brandes_2020_32665031.html",
    "title": "PWAS: proteome-wide association study-linking genes and phenotypes by functional variation in proteins",
    "section": "",
    "text": "PubMed: 32665031 DOI: 10.1186/s13059-020-02089-x Overview generated by: Gemini 2.5 Flash, 28/11/2025"
  },
  {
    "objectID": "proteomics/brandes_2020_32665031.html#key-findings-pwas-as-a-protein-centric-association-method",
    "href": "proteomics/brandes_2020_32665031.html#key-findings-pwas-as-a-protein-centric-association-method",
    "title": "PWAS: proteome-wide association study-linking genes and phenotypes by functional variation in proteins",
    "section": "Key Findings: PWAS as a Protein-Centric Association Method",
    "text": "Key Findings: PWAS as a Protein-Centric Association Method\nThe authors introduce Proteome-Wide Association Study (PWAS), a novel, protein-centric computational method designed to detect gene-phenotype associations that are mediated by alterations in protein function.\nPWAS offers several key advantages over traditional genome-wide association studies (GWAS) and other gene-level methods:\n\nAggregation and Power: It aggregates the signal from all coding variants jointly affecting a protein-coding gene, allowing it to uncover associations that are too weak or spread out to be detected by per-variant GWAS, especially those involving rare variants.\nRecessive Inheritance: PWAS is explicitly designed to model both dominant and recessive modes of heritability, which the authors show to be substantial in complex traits, addressing a mode often neglected in traditional GWAS.\nFunctional Interpretability: By explicitly quantifying the functional damage to the protein, PWAS provides highly interpretable results and is better posed to suggest a causal relationship between the gene and the phenotype."
  },
  {
    "objectID": "proteomics/brandes_2020_32665031.html#study-design-and-methods",
    "href": "proteomics/brandes_2020_32665031.html#study-design-and-methods",
    "title": "PWAS: proteome-wide association study-linking genes and phenotypes by functional variation in proteins",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\nPWAS is implemented as a two-stage association pipeline using genetic and phenotypic data from large cohorts, such as the UK Biobank (UKBB).\n\nStage 1: Quantifying Protein Functional Damage\n\nVariant Selection: PWAS considers all variants that affect the coding regions of genes (e.g., missense, nonsense, frameshift).\nDamage Prediction: For each variant, a pre-trained machine learning model called FIRM (Functional Interpretation of Rare Missense variants) is used to estimate a variant effect score, which is interpreted as the probability of the protein retaining its function (a score between 0 and 1).\n\n\n\nStage 2: Gene-Level Association Testing\n\nScore Aggregation: The variant effect scores are combined with individual genotype data to create per-gene functional effect scores for each person in the cohort.\nInheritance Modeling: Two separate effect scores are calculated for each gene, explicitly covering dominant (at least one damaging hit) and recessive (at least two damaging hits) inheritance models.\nStatistical Test: These per-gene scores are then statistically tested against the phenotype of interest (binary or continuous) alongside covariates (e.g., sex, age, principal components) to identify significant associations."
  },
  {
    "objectID": "proteomics/brandes_2020_32665031.html#results",
    "href": "proteomics/brandes_2020_32665031.html#results",
    "title": "PWAS: proteome-wide association study-linking genes and phenotypes by functional variation in proteins",
    "section": "Results",
    "text": "Results\n\nPerformance and Comparison\n\nSimulation Analysis: Simulations, based on a protein-centric causal model, demonstrated that PWAS’s advantage is particularly pronounced in detecting associations under the recessive inheritance model.\nReal Data Application (UKBB): PWAS was applied to 49 diverse phenotypes using a cohort of 333,424 filtered UKBB samples.\nComparison to GWAS: The method discovered 2743 gene-phenotype associations that were missed by standard GWAS, which represents 22% of all PWAS-discovered associations.\nComparison to SKAT: PWAS was found to be complementary to SKAT (Sequentially-adjusted association Test), another popular gene-level association method. PWAS recovered more high-quality, known gene-disease associations from the OMIM database (12 associations compared to 7 for SKAT).\n\n\n\nCase Study: Colorectal Cancer\n\nIn a case study of colorectal cancer (2822 cases, 260,127 controls), the well-known predisposition gene MUTYH was not found to be exome-wide significant by standard GWAS (p-values were 6.3E-04 and 1.2E-03).\nIn contrast, PWAS detected the MUTYH association with overwhelming significance (FDR q-value = 2.3E-06) by aggregating the signal from multiple variants. Crucially, the association was found to be significant only under the recessive model, which is consistent with the gene’s known biallelic mutation mechanism for increased cancer risk."
  },
  {
    "objectID": "proteomics/brandes_2020_32665031.html#conclusions-and-recommendations",
    "href": "proteomics/brandes_2020_32665031.html#conclusions-and-recommendations",
    "title": "PWAS: proteome-wide association study-linking genes and phenotypes by functional variation in proteins",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nPWAS represents a shift toward using detailed, functional machine learning models to improve gene-phenotype association studies. By focusing on protein function and explicitly modeling different inheritance modes, especially recessive effects, it can recover causal protein-coding genes that are typically missed by variant-centric or expression-centric methods. The authors recommend PWAS as an effective, complementary tool for genetic association studies, providing functionally interpretable results without the need for post-analysis fine-mapping."
  },
  {
    "objectID": "proteomics/suhre_2024_38412862.html",
    "href": "proteomics/suhre_2024_38412862.html",
    "title": "Genetic associations with ratios between protein levels detect new pQTLs and reveal protein-protein interactions",
    "section": "",
    "text": "PubMed: 38412862 DOI: 10.1016/j.xgen.2024.100506 Overview generated by: Gemini 2.5 Flash, 28/11/2025"
  },
  {
    "objectID": "proteomics/suhre_2024_38412862.html#key-findings-the-power-of-ratio-qtls-rqtls",
    "href": "proteomics/suhre_2024_38412862.html#key-findings-the-power-of-ratio-qtls-rqtls",
    "title": "Genetic associations with ratios between protein levels detect new pQTLs and reveal protein-protein interactions",
    "section": "Key Findings: The Power of Ratio-QTLs (rQTLs)",
    "text": "Key Findings: The Power of Ratio-QTLs (rQTLs)\nThis study introduces a novel concept in proteomic genetics: the systematic analysis of ratios between protein levels to identify ratio quantitative trait loci (rQTLs). The central finding is that using protein ratios dramatically enhances the statistical power to detect genetic variants influencing protein regulation, particularly those linked to functional biological mechanisms like Protein-Protein Interactions (PPIs).\n\nEnhanced Detection and Biological Relevance\n\nIncreased Association Strength: Analyzing ratios between protein pairs, rather than individual protein levels, strengthened associations at known pQTL loci by several hundred orders of magnitude (p-gain).\nNew Loci: The use of ratios increased the number of proteogenomic loci by 25% and helped detect 39 previously unidentified cis-pQTLs.\nPPI Enrichment: The identified rQTLs were 7.6-fold enriched in established protein-protein interactions (PPIs) from the BioGRID database. This high enrichment indicates that the ratio method is particularly effective at capturing genetic effects that modulate the functional interaction or shared regulation of two proteins."
  },
  {
    "objectID": "proteomics/suhre_2024_38412862.html#methods-and-study-design",
    "href": "proteomics/suhre_2024_38412862.html#methods-and-study-design",
    "title": "Genetic associations with ratios between protein levels detect new pQTLs and reveal protein-protein interactions",
    "section": "Methods and Study Design",
    "text": "Methods and Study Design\n\nData and Cohort\n\nCohort: A large-scale analysis using proteomic and genetic data from 52,705 samples in the UK Biobank.\nProteome: Measured levels for 1,473 plasma proteins using the Olink Proximity Extension Assay (PEA).\nStatistical Approach: The study analyzed approximately 1.1 million protein ratios, testing each ratio against genetic variants (SNPs) to find rQTLs.\n\n\n\nTheoretical Interpretation of rQTLs\nAn rQTL signal between two proteins (\\(P_1/P_2\\)) can arise from three primary scenarios:\n\nShared Confounder: A genetic variant influences a common confounding factor (e.g., cell type composition, inflammation) that affects both proteins similarly.\nShared Pathway: A variant affects a shared regulatory pathway (e.g., transcription factor) controlling both proteins.\nProtein-Protein Interaction (PPI): A variant directly or indirectly alters the functional or physical interaction between \\(P_1\\) and \\(P_2\\). The strong enrichment of rQTLs in known PPIs suggests this method excels at identifying the latter two, especially PPIs."
  },
  {
    "objectID": "proteomics/suhre_2024_38412862.html#results-case-studies-and-functional-insights",
    "href": "proteomics/suhre_2024_38412862.html#results-case-studies-and-functional-insights",
    "title": "Genetic associations with ratios between protein levels detect new pQTLs and reveal protein-protein interactions",
    "section": "Results: Case Studies and Functional Insights",
    "text": "Results: Case Studies and Functional Insights\n\nHigh-Impact rQTLs\nThe study highlighted several high-impact rQTLs with strong biological relevance:\n\nComplement Components: The strongest associations involved proteins in the complement system, an essential part of the innate immune response. The rQTL analysis revealed variants impacting the balance of activation and regulation components, such as C3/C4 and C5/C9.\nDuffy Blood Group (ACKR1): A classic example was the rQTL signal for the Duffy blood type variant (rs12075 in ACKR1). While this variant is known to affect multiple cytokine levels individually, the ratio analysis significantly amplified the association, indicating a shared, highly coordinated regulatory effect on a panel of cytokines.\nNFATC1 Pathway: The analysis revealed rQTLs regulating the ratio of NFATC1 with several other proteins, pointing to complex regulatory mechanisms related to the NFATC1-mediated signaling pathway."
  },
  {
    "objectID": "proteomics/suhre_2024_38412862.html#conclusions-and-recommendations",
    "href": "proteomics/suhre_2024_38412862.html#conclusions-and-recommendations",
    "title": "Genetic associations with ratios between protein levels detect new pQTLs and reveal protein-protein interactions",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe concept of rQTLs provides a powerful and generalizable theoretical framework for detecting subtle genetic effects that modulate the relative abundance of protein pairs. By focusing on ratios, researchers can effectively filter out much of the biological noise and shared technical variation that obscures signals in single-protein pQTL analyses. The high enrichment for known PPIs validates rQTLs as a superior method for identifying functional protein relationships and should be a standard approach in future proteogenomic studies for drug target discovery. The authors recommend further theoretical development and generalization of the ratio concept."
  },
  {
    "objectID": "proteomics/kirsher_2025_40999057.html",
    "href": "proteomics/kirsher_2025_40999057.html",
    "title": "Current landscape of plasma proteomics from technical innovations to biological insights and biomarker discovery",
    "section": "",
    "text": "PubMed: 40999057 DOI: 10.1038/s42004-025-01665-1 Overview generated by: Gemini 2.5 Flash, 27/11/2025"
  },
  {
    "objectID": "proteomics/kirsher_2025_40999057.html#background-and-objective",
    "href": "proteomics/kirsher_2025_40999057.html#background-and-objective",
    "title": "Current landscape of plasma proteomics from technical innovations to biological insights and biomarker discovery",
    "section": "Background and Objective",
    "text": "Background and Objective\nPlasma proteins are critical biomolecules that reflect health and disease states, driving the rapidly expanding field of plasma proteome profiling for biomarker discovery. However, researchers often lack comprehensive, head-to-head comparisons of the diverse and evolving plasma proteomics platforms available.\nThe objective of this study was to address this gap by conducting a direct, systematic comparison of multiple state-of-the-art proteomics platforms to assess their performance, identify key differences, and provide valuable guidance for researchers in selecting appropriate technologies for biomarker discovery and clinical applications."
  },
  {
    "objectID": "proteomics/kirsher_2025_40999057.html#methods-platform-benchmarking-on-a-shared-cohort",
    "href": "proteomics/kirsher_2025_40999057.html#methods-platform-benchmarking-on-a-shared-cohort",
    "title": "Current landscape of plasma proteomics from technical innovations to biological insights and biomarker discovery",
    "section": "Methods: Platform Benchmarking on a Shared Cohort",
    "text": "Methods: Platform Benchmarking on a Shared Cohort\nThe study performed a rigorous technical evaluation of plasma proteomics technologies:\n\nPlatform Selection: Eight different plasma proteomics platforms were directly compared. These platforms represented both affinity-based methods (e.g., proximity extension assays) and various mass spectrometry (MS) approaches.\nCoverage: The platforms collectively covered over 13,000 proteins.\nExperimental Design: All platforms were applied to the same human cohort to ensure a direct and unbiased assessment of performance, allowing for systematic comparison of their capabilities, including protein coverage, dynamic range, and reproducibility."
  },
  {
    "objectID": "proteomics/kirsher_2025_40999057.html#key-results-and-significance",
    "href": "proteomics/kirsher_2025_40999057.html#key-results-and-significance",
    "title": "Current landscape of plasma proteomics from technical innovations to biological insights and biomarker discovery",
    "section": "Key Results and Significance",
    "text": "Key Results and Significance\nThe systematic assessment provided critical insights into the performance characteristics and trade-offs of the different technologies:\n\nKey Differences and Complementary Strengths: The comparison identified significant differences in platform performance, as well as complementary strengths, highlighting that different platforms are best suited for different research goals.\nTrade-offs in Coverage: The findings specifically illustrated the trade-offs in protein coverage—the number of quantifiable proteins—versus other performance metrics like sensitivity and throughput.\nEssential Resource: This study serves as an essential resource for the research community, offering both technical evaluation and biological insights to support the development of novel diagnostics and therapeutics by informing the critical choice of technology for plasma proteomics profiling."
  },
  {
    "objectID": "proteomics/carrascozanini_2024_39039249.html",
    "href": "proteomics/carrascozanini_2024_39039249.html",
    "title": "Proteomic signatures improve risk prediction for common and rare diseases",
    "section": "",
    "text": "PubMed: 39039249 DOI: 10.1038/s41591-024-03142-z Overview generated by: Gemini 2.5 Flash, 28/11/2025"
  },
  {
    "objectID": "proteomics/carrascozanini_2024_39039249.html#key-findings-proteomics-enhances-disease-risk-prediction",
    "href": "proteomics/carrascozanini_2024_39039249.html#key-findings-proteomics-enhances-disease-risk-prediction",
    "title": "Proteomic signatures improve risk prediction for common and rare diseases",
    "section": "Key Findings: Proteomics Enhances Disease Risk Prediction",
    "text": "Key Findings: Proteomics Enhances Disease Risk Prediction\nThis large-scale study, utilizing the United Kingdom Biobank Pharma Proteomics Project (UKB-PPP), demonstrates that incorporating plasma proteomic signatures significantly improves the prediction of the 10-year incidence risk for a wide range of common and rare diseases compared to models based solely on clinical information or polygenic risk scores (PRS).\n\nSuperior Predictive Performance\nThe core finding is that sparse prediction models using as few as 5 to 20 plasma proteins were superior to models built on basic clinical information alone for 67 pathologically diverse diseases. The median improvement in predictive performance (delta C-index) was 0.07, with gains as high as 0.31 for certain conditions.\n\n\nOutperformance over Clinical Assays\nCrucially, the protein models also outperformed models that combined basic clinical information with data from 37 commonly used clinical assays for 52 diseases. This indicates that the plasma proteome captures unique biological signals not fully reflected in standard clinical blood tests. Diseases where protein models showed significant improvement include: * Hematological Cancers: Multiple myeloma and non-Hodgkin lymphoma. * Neurodegenerative/Neurological Disorders: Motor neuron disease. * Cardiopulmonary Diseases: Pulmonary fibrosis and dilated cardiomyopathy."
  },
  {
    "objectID": "proteomics/carrascozanini_2024_39039249.html#study-design-and-methods",
    "href": "proteomics/carrascozanini_2024_39039249.html#study-design-and-methods",
    "title": "Proteomic signatures improve risk prediction for common and rare diseases",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\nThe study integrated deep proteomic data with extensive clinical follow-up data from the UK Biobank cohort.\n\nData and Cohort\n\nCohort: 41,931 individuals from the UKB-PPP.\nProteome: Measurements for approximately 3,000 plasma proteins using Olink Proximity Extension Assays.\nTarget Outcomes: 10-year incidence for 218 common and rare diseases.\n\n\n\nStatistical Modeling\n\nModel Type: Sparse prediction models (using elastic net regression) were developed to predict disease incidence.\nBaseline Model: All models were compared against a clinical model incorporating basic demographic information (age, sex, BMI, smoking status) and a comorbidity score.\nThree Tiers of Models:\n\nClinical model only.\nClinical model + 5 to 20 selected proteins (sparse proteomic signature).\nClinical model + data from 37 clinical assays.\n\nComparison: The performance of these models was quantified using the C-index (Area Under the Curve, AUC) for time-to-event data."
  },
  {
    "objectID": "proteomics/carrascozanini_2024_39039249.html#results-disease-specific-insights",
    "href": "proteomics/carrascozanini_2024_39039249.html#results-disease-specific-insights",
    "title": "Proteomic signatures improve risk prediction for common and rare diseases",
    "section": "Results: Disease-Specific Insights",
    "text": "Results: Disease-Specific Insights\n\nMultiple Myeloma\nFor multiple myeloma, a plasma cell cancer, a proteomic signature improved the C-index by 0.28 over the clinical model. The top predictor was Immunoglobulin free light chain kappa (IGFKL), a known marker of plasma cell activity, demonstrating the model’s ability to highlight clinically relevant biomarkers.\n\n\nMotor Neuron Disease (MND)\nFor MND, a fatal neurological disorder, the model’s C-index improved by 0.17. The primary predictors were TDP-43 (Tar-DNA binding protein 43) and proteins associated with neuroinflammation, providing evidence for distinct biological pathways at play years before diagnosis."
  },
  {
    "objectID": "proteomics/carrascozanini_2024_39039249.html#robustness-confounding-and-determinants",
    "href": "proteomics/carrascozanini_2024_39039249.html#robustness-confounding-and-determinants",
    "title": "Proteomic signatures improve risk prediction for common and rare diseases",
    "section": "Robustness, Confounding, and Determinants",
    "text": "Robustness, Confounding, and Determinants\nThe authors stress the importance of understanding the determinants of protein variation to avoid spurious associations and Type 1 errors.\n\nImpact of Residual Confounding\nPrioritization analyses revealed that after regressing out variance explained by demographic and other clinical factors, over 80% of non-null associations attenuated, strongly suggesting the presence of residual confounding in protein-outcome relationships. However, robust associations highlighted well-established clinical markers, such as Prostate-Specific Antigen (PSA), demonstrating the method’s ability to retain true biological signals when accounting for confounders.\n\n\nAncestral and Sex Differences\n\nAncestry: The study noted significant ancestral differences in the variance of protein levels explained by genetic factors (pQTLs), primarily driven by cis- and trans-pQTL effects (e.g., differences in minor allele frequency, MAF), which suggests differential effects for the same variant across ancestral groups. These differences were not solely attributable to differences in sample size (N).\nSex: Although few sex-differential genetic effects were found (consistent with findings from large-scale GWASs), approximately one-third of the tested proteins exhibited differences in levels due to varying participant characteristics between males and females (e.g., medication use).\n\n\n\nComparison to Polygenic Risk Scores (PRS) and MR\nThe study confirmed that the proteomic signature provided a greater improvement in prediction over the clinical model than the corresponding PRS for most diseases. However, when attempting to compare protein prediction using genetically imputed plasma protein levels (e.g., in Mendelian Randomization analyses) versus directly measured protein levels with outcomes, there was very little consistency, even after adjusting for protein factors. This highlights a limitation in using imputed protein levels for causal inference in this context."
  },
  {
    "objectID": "proteomics/carrascozanini_2024_39039249.html#conclusions-and-future-directions",
    "href": "proteomics/carrascozanini_2024_39039249.html#conclusions-and-future-directions",
    "title": "Proteomic signatures improve risk prediction for common and rare diseases",
    "section": "Conclusions and Future Directions",
    "text": "Conclusions and Future Directions\nThe study strongly supports the use of high-throughput plasma proteomics as an objective, non-invasive method to improve disease risk prediction across a diverse array of conditions. The sparse signatures identified represent potential targets for early intervention and monitoring. The findings underscore the critical importance of characterizing and accounting for protein determinants to distinguish biologically relevant findings from false positives and to ensure accurate risk stratification in clinical applications. The authors suggest that integrating these proteomic biomarkers into clinical practice could facilitate earlier diagnosis and personalized screening programs."
  },
  {
    "objectID": "proteomics/gadd_2024_38987645.html",
    "href": "proteomics/gadd_2024_38987645.html",
    "title": "Blood protein assessment of leading incident diseases and mortality in the UK Biobank",
    "section": "",
    "text": "PubMed: 38987645 DOI: 10.1038/s43587-024-00655-7 Overview generated by: Gemini 2.5 Flash, 28/11/2025"
  },
  {
    "objectID": "proteomics/gadd_2024_38987645.html#key-findings-protein-scores-predict-incident-disease-and-mortality",
    "href": "proteomics/gadd_2024_38987645.html#key-findings-protein-scores-predict-incident-disease-and-mortality",
    "title": "Blood protein assessment of leading incident diseases and mortality in the UK Biobank",
    "section": "Key Findings: Protein Scores Predict Incident Disease and Mortality",
    "text": "Key Findings: Protein Scores Predict Incident Disease and Mortality\nThis large-scale study leveraged proteomics data from the UK Biobank (n=47,600) to identify and validate protein biomarkers for the risk of 23 common age-related diseases and all-cause mortality. The core finding is that multi-protein scores (“ProteinScores”) significantly enhance the prediction of incident diseases and mortality, even when accounting for comprehensive clinical and lifestyle information.\n\nDiscovery of Associations\n\nThe study reported 3,209 associations between 963 unique plasma protein levels and 21 incident outcomes (including diseases and mortality).\nCardiovascular disease (CVD), specifically coronary artery disease (CAD) and atrial fibrillation (AF), had the largest number of associated proteins, highlighting the strong systemic link between the proteome and heart health.\n\n\n\nPredictive Power of ProteinScores\n\nProteinScores were developed using penalized Cox regression (elastic net) to combine multiple protein measurements into a single risk score for each outcome.\nThese scores were applied to independent test sets and were found to improve the Area Under the Curve (AUC) estimates for the 10-year onset of incident outcomes, beyond a minimally adjusted model that already included age, sex, and a comprehensive set of 24 lifestyle and clinical factors.\nProteinScores were validated for six major outcomes, including:\n\nAll-cause mortality\nCoronary artery disease (CAD)\nAtrial fibrillation (AF)\nType 2 diabetes (T2D)\nColorectal cancer (CRC)\nGlaucoma"
  },
  {
    "objectID": "proteomics/gadd_2024_38987645.html#methods-and-design",
    "href": "proteomics/gadd_2024_38987645.html#methods-and-design",
    "title": "Blood protein assessment of leading incident diseases and mortality in the UK Biobank",
    "section": "Methods and Design",
    "text": "Methods and Design\n\nCohort and Data\n\nParticipants: Up to 47,600 individuals from the UK Biobank.\nProteome: Measured levels for 1,468 plasma proteins using the Olink Proximity Extension Assay (PEA) platform.\nOutcomes: Incident diagnoses for 23 age-related diseases and all-cause mortality, monitored over a 10-year follow-up period.\n\n\n\nStatistical Modeling\n\nIndividual Protein Associations: Cox proportional hazards (PH) models were used to test the association between each individual protein and each outcome.\nProteinScore Development: Penalized Cox regression (elastic net) was employed across 50 randomized iterations of training/testing to select and weight the most predictive proteins for each outcome, resulting in a single, robust ProteinScore.\nBenchmarking: ProteinScore performance was rigorously compared to a minimally adjusted model (age, sex, and 24 clinical/lifestyle factors) using the incremental AUC difference (\\(\\Delta\\)AUC), with 10-year AUC being the primary metric."
  },
  {
    "objectID": "proteomics/gadd_2024_38987645.html#results-biological-insights-and-clinical-relevance",
    "href": "proteomics/gadd_2024_38987645.html#results-biological-insights-and-clinical-relevance",
    "title": "Blood protein assessment of leading incident diseases and mortality in the UK Biobank",
    "section": "Results: Biological Insights and Clinical Relevance",
    "text": "Results: Biological Insights and Clinical Relevance\n\nMortality and Aging\nThe ProteinScore for all-cause mortality was consistently one of the strongest performers. The proteins contributing most to the mortality score were related to fundamental biological pathways such as inflammation (e.g., C-Reactive Protein, CRP) and cellular stress/damage. The study demonstrated that protein levels capture biological information related to underlying aging processes that are independent of standard clinical risk factors.\n\n\nDisease Specificity\n\nCAD and AF: ProteinScores for these cardiovascular outcomes showed significant predictive improvement, highlighting the utility of proteomics in risk stratification for heart disease.\nCRC and Glaucoma: The successful prediction of these conditions demonstrates that plasma proteomics can capture systemic signals of diseases that are often viewed as localized."
  },
  {
    "objectID": "proteomics/gadd_2024_38987645.html#conclusions-and-recommendations",
    "href": "proteomics/gadd_2024_38987645.html#conclusions-and-recommendations",
    "title": "Blood protein assessment of leading incident diseases and mortality in the UK Biobank",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study confirms that plasma protein levels are powerful predictors of future health outcomes and mortality. The ProteinScore approach provides a robust, validated, and clinically actionable method for integrating this proteomic information into risk stratification models. The authors advocate for the routine use of multi-protein panels to improve personalized risk assessment for major age-related diseases and overall longevity."
  },
  {
    "objectID": "statistics.html",
    "href": "statistics.html",
    "title": "statistics",
    "section": "",
    "text": "Glossary of terms\nImproving Your Statistical Inferences\n\n\nLarge-scale composite hypothesis testing procedure for omics data analyses\n\nNovel method (qch_copula) for testing composite hypotheses across multiple traits/omics levels using mixture models with copula functions\nControls Type I error effectively while achieving superior detection power compared to 8 state-of-the-art methods across diverse correlation scenarios\nMemory-efficient implementation enables analysis of 105-106 markers with up to 20 traits, validated through psychiatric disorder pleiotropy and plant virus resistance studies\n\n\n\nSifting the evidence—what’s wrong with significance tests?\n\nCore Critique: This classic commentary criticizes the widespread misuse and over-reliance on statistical significance tests (p-values) in medical literature, arguing that this practice fundamentally distorts evidence.\nCentral Problem: The authors identify publication bias (accentuating positive results over null results) as the key factor causing the mistaken publication of chance findings as real effects, thereby eroding confidence in science.\nRecommendation: Researchers should shift their focus from the binary question of whether P &lt; 0.05 to assessing the direction and the magnitude of the effect, specifically asking if the effect is of public health or clinical importance.\n\n\n\nStatistical tests, P values, confidence intervals, and power: a guide to misinterpretations\n\nCore Principle: This influential essay details the pervasive misinterpretations and abuse of fundamental statistics—P-values, Confidence Intervals (CIs), and statistical power—in scientific literature.\nP-value Clarification: The authors correct 12 common misconceptions, asserting that the P-value is not the probability of the null hypothesis (\\(H_0\\)) being true, but rather the probability of the observed data (or more extreme) given that \\(H_0\\) is true.\nMisuse of Significance: The essay strongly condemns the practice of drawing dichotomous conclusions based on an arbitrary threshold (e.g., P&lt;0.05), which falsely implies certainty and hinders scientific progress.\nRecommendation: Researchers are urged to shift focus from the P-value to the effect magnitude and its precision, which are better conveyed through Confidence Intervals, and to integrate statistical results with contextual, external evidence.\n\n\n\nThe ASA’s Statement on p-Values: Context, Process, and Purpose\n\nCore Principle: This article presents the American Statistical Association’s (ASA) six core principles for the proper use of the P-value, aiming to correct pervasive misinterpretations in scientific research.\nKey Principles: The ASA states that the P-value does not measure the probability that a hypothesis is true (Principle 2) or the size/importance of an effect (Principle 5). Instead, it measures how incompatible the data are with a statistical model (Principle 1).\nCrucial Admonitions: The statement strongly advises against drawing dichotomous conclusions based solely on a fixed threshold (e.g., P&lt;0.05, Principle 3) and requires full reporting and transparency of all results to combat selective reporting (Principle 4).\nRecommendation: The ASA advocates for a move beyond the single P-value to integrate statistical results with effect sizes, Confidence Intervals, context, and external evidence (Principle 6).\n\n\n\nTriangulation in aetiological epidemiology: Approaches to causal inference\n\nCore Principle: This article formally advocates for Triangulation in aetiological epidemiology, defined as the integration of results from multiple research approaches where each approach possesses uncorrelated sources of bias.\nCausal Inference: If different methods (e.g., observational studies, Mendelian Randomization, and clinical trials) all converge on the same causal conclusion, confidence in that finding is significantly strengthened, especially when method-specific biases would predict opposing results.\nAddressing Inconsistency: When results are inconsistent, the framework helps identify which source of bias (e.g., confounding, reverse causation) is most likely distorting a given approach, thereby guiding the direction of future, more focused research.\n\n\n\nCausal Inference Is Not Just a Statistics Problem\n\nCore Argument: This article argues that causal inference relies primarily on study design and domain knowledge to establish untestable assumptions (like exchangeability and no unmeasured confounding), with statistical methods serving only to quantify the effect under those assumptions.\nDesign Trumps Analysis: The authors emphasize that a causal estimate is invalid if the study design fails to account for critical (especially unmeasured) confounders, regardless of the sophistication of the statistical modeling used.\nThe Role of Causal Diagrams: Identifying which variables to control is a causal question, not a statistical one, necessitating the use of Directed Acyclic Graphs (DAGs) and scientific expertise to avoid introducing bias by controlling for mediators or colliders.\nConclusion: Teaching and practicing causal inference requires prioritizing the formulation of a causal question and the design of the study before the application of statistical tools.\n\n\n\nMoving to a World Beyond \\(p &lt; 0.05\\)\n\nCore Argument: This editorial calls for the abandonment of the term “statistically significant” and the practice of dichotomizing results based on the arbitrary \\(p &lt; 0.05\\) threshold.\nCentral Problem: Relying on the binary threshold distorts evidence, fuels poor practices like P-hacking and publication bias, and leads to the false equating of statistical significance with scientific importance.\nRecommendation: Researchers must treat the p-value as a continuous measure of incompatibility between the data and the null hypothesis. They should focus their interpretation on the magnitude of the effect and its precision, primarily communicated via Confidence Intervals (CIs).\n\n\n\nTesting For Baseline Balance In Clinical Trials\n\nCore Principle: This paper argues that performing tests of baseline homogeneity (p-value tests) in randomized controlled trials (RCTs) is philosophically unsound, of no practical value, and potentially misleading, as randomization ensures comparability in expectation.\nThe Flaw: A non-significant baseline difference does not prove balance (it reflects low power), and a significant difference is merely a chance event that does not invalidate the randomization.\nRecommendation: The authors recommend that researchers must abandon baseline testing and, instead, identify known prognostic variables in the trial-plan and fit them in an Analysis of Covariance (ANCOVA) model regardless of their baseline p-value, which increases the precision and power of the final treatment effect estimate.\n\n\n\nCovariate Imbalance and Adjustment for Logistic Regression Analysis of Clinical Trial Data\n\nCore Problem: This study demonstrates that, for binary outcomes analyzed with logistic regression, the unadjusted Odds Ratio (OR) is generally a biased estimate of the conditional OR due to the non-collapsibility of the OR, even in a perfectly randomized trial.\nKey Finding: Adjusting for prognostic covariates in logistic regression is critical, as it significantly increases statistical power (by up to 17.5% in simulations) and reduces bias in the treatment effect estimate.\nRecommendation: While good baseline balance in covariates is desirable, it never fully alleviates the shortcomings of unadjusted analyses in the logistic setting; researchers should pre-specify and use covariate adjustment for prognostic variables to ensure the most precise and efficient estimate.\n\n\n\nStep away from stepwise\n\nCore Argument: This report critiques stepwise regression, arguing it is a flawed statistical method, especially in the context of Big Data, because it uses statistical significance to select variables.\nKey Flaw: Stepwise procedures may include nuisance variables that are coincidentally significant and exclude true explanatory variables that happen to be non-significant, leading to models that fit data well in-sample but perform poorly out-of-sample.\nBig Data Effect: The belief that stepwise is more useful with more variables is false; Big Data exacerbates its failings by increasing the chance of selecting spurious correlations, causing a dramatic deterioration in out-of-sample predictive accuracy.\n\n\n\nThe Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis\n\nCore Argument: This paper argues that retrospective power calculations (or observed power), performed after a study yields a non-significant result, are fundamentally flawed and should not be used to aid in interpretation.\nThe Flaw (Circularity): Retrospective power is a simple monotonic transformation of the p-value; it provides no new information. A non-significant test will always have low retrospective power, as both measures are driven by the large variance or small observed effect.\nRecommended Alternative: To interpret a non-significant finding, researchers should focus on the confidence interval, which reveals the range of plausible true effect sizes and indicates whether biologically or clinically important effects have been reasonably ruled out.\n\n\n\nThe cost of dichotomising continuous variables\n\nAvoidable Costs: The practice of dichotomizing continuous variables for statistical analysis leads to a substantial loss of statistical power and an inflation of the Type I error rate (false positives) in multivariable models.\nFlawed Interpretation: Dichotomization uses arbitrary or unstable cut-points, provides misleading effect estimates, and ignores the true shape of the relationship across the continuous scale.\nRecommendation: Continuous variables should be analyzed on their original scale (or using methods like fractional polynomials/splines) to preserve information and avoid these serious statistical drawbacks.\n\n\n\nSpurious interaction as a result of categorization\n\nCore Argument: Categorizing continuous exposure variables in regression models, a common practice in epidemiology, can introduce spurious interaction effects that do not exist in the original continuous scale.\nMechanism: This phenomenon is demonstrated analytically and is interpreted as a form of differential measurement error or residual confounding caused by the categorization process.\nConclusion: Spurious interaction is induced unless the correlated variables are dichotomized at the median or are uncorrelated. The paper strongly recommends avoiding categorization to ensure valid results, suggesting non-parametric models as a preferred alternative.\n\n\n\nSynthesizing Subject-matter Expertise for Variable Selection in Causal Effect Estimation: A Case Study\n\nObjective: To empirically assess different strategies for Directed Acyclic Graph (DAG) creation and the resulting minimal adjustment sets for estimating a known causal effect (adherence on mortality in the CDP trial).\nKey Finding (Efficiency): The results confirm that including nonconfounding prognostic factors (outcome predictors) significantly reduced variance (increased efficiency) of the causal effect estimate, aligning with theoretical statistical advice.\nRecommendation: Researchers should prioritize the exhaustive identification of all potential outcome prognostic factors as the initial and most crucial step when constructing DAGs for causal effect estimation.\n\n\n\nCan algorithms replace expert knowledge for causal inference? A case study on novice use of causal discovery\n\nObjective: To test whether causal discovery algorithms used by novices could replace expert knowledge in selecting covariates for estimating the known null effect of placebo adherence on mortality in the CDP trial.\nKey Finding: Adjustment sets derived from causal discovery algorithms resulted in more residual bias in complex time-varying analyses compared to expert-selected sets.\nConclusion: Due to high subjectivity in selecting algorithms/parameters and resolving complex graph outputs, the authors do not recommend novice use of causal discovery without an expert’s guidance.\n\n\n\nA Structural Description of Biases That Generate Immortal Time\n\nCore Argument: The true source of immortal time bias is the underlying selection or misclassification of subjects, not the period of event-free time itself.\nTwo Mechanisms: The paper structurally describes two ways the bias arises: 1) Selection bias when eligibility is applied after assignment; and 2) Misclassification bias when assignment is defined using post-eligibility data for strategies indistinguishable at baseline.\nPrevention: The definitive solution is to use Target Trial Emulation to achieve synchronization of eligibility and treatment assignment at the start of follow-up, thereby eliminating the generating biases.\n\n\n\nComparison of Imputation Strategies for Incomplete Longitudinal Data in Life-Course Epidemiology\n\nTopic: A study comparing the performance of three Multiple Imputation (MI) methods for handling incomplete longitudinal data in life-course epidemiology, focusing on the effect of longitudinal depressive symptoms on mortality.\nMethods Compared: Normal Linear Regression (MICE), Predictive Mean Matching (PMM), and Variable-Tailored Specification were tested under nine scenarios varying missingness rate and mechanism (MCAR, MAR, MNAR).\nFinding: All methods showed similar levels of bias in estimating the causal effect (Hazard Ratios), but Predictive Mean Matching (PMM) was identified as the most appealing strategy due to its consistently low root mean square error (RMSE) and competitive computation time.\n\n\n\nAn efficient, not-only-linear correlation coefficient based on clustering\n\nNovel Statistic: The paper introduces the Clustering-based Correlation Coefficient (CCC), a computationally efficient measure that detects both linear and non-linear relationships, unlike traditional coefficients like Pearson’s \\(r\\).\nMechanism: CCC operates by assessing how well the marginal information of two variables aligns with the clusters formed by their joint distribution, allowing it to capture complex, non-monotonic associations.\nBiological Utility: In an application to human gene expression data, CCC successfully identified biologically meaningful non-linear patterns, including those driven by sex differences, that were missed by standard linear methods."
  },
  {
    "objectID": "genetics/monsees_2009_19365863.html",
    "href": "genetics/monsees_2009_19365863.html",
    "title": "Genome-wide association scans for secondary traits using case-control samples",
    "section": "",
    "text": "PubMed: 19365863\nDOI: 10.1002/gepi.20424\nOverview generated by: Gemini 2.5 Flash, 26/11/2025"
  },
  {
    "objectID": "genetics/monsees_2009_19365863.html#key-findings",
    "href": "genetics/monsees_2009_19365863.html#key-findings",
    "title": "Genome-wide association scans for secondary traits using case-control samples",
    "section": "Key Findings",
    "text": "Key Findings\nThis paper addresses a critical methodological issue in genetic epidemiology where researchers attempt to maximize the return on investment from an expensive case-control Genome-Wide Association Study (GWAS) by analyzing additional, or secondary, quantitative traits (e.g., body mass index, mammographic density) collected on the same subjects. The core finding is that a naïve analysis (ignoring the case-control ascertainment) can lead to biased estimates of the association between a genetic marker and the secondary trait, particularly when both the marker and the secondary trait are independently associated with the primary disease risk. The authors demonstrate that the use of Inverse-Probability-of-Sampling-Weighted (IPW) regression provides unbiased estimates of the marker-secondary trait association in all scenarios, although it may suffer from reduced statistical power compared to the biased naïve methods."
  },
  {
    "objectID": "genetics/monsees_2009_19365863.html#statistical-problem-ascertainment-bias",
    "href": "genetics/monsees_2009_19365863.html#statistical-problem-ascertainment-bias",
    "title": "Genome-wide association scans for secondary traits using case-control samples",
    "section": "Statistical Problem: Ascertainment Bias",
    "text": "Statistical Problem: Ascertainment Bias\nCase-control studies are designed to test the association between a genetic marker and a primary binary disease (e.g., breast cancer). When using these same samples to study a quantitative trait (the secondary trait, e.g., mammographic density), the selection process (ascertainment) based on the disease status introduces a bias.\n\nNaïve Analysis Scenarios\nThe study mathematically and via simulation tested the performance of two “naïve” approaches for testing the association between a marker (\\(G\\)) and a secondary trait (\\(T\\)) using case-control data, where \\(D\\) is the primary disease status:\n\nIgnoring \\(D\\): Regressing \\(T\\) on \\(G\\) in the combined sample, ignoring case-control status.\nStratifying on \\(D\\): Regressing \\(T\\) on \\(G\\) separately within cases and controls, and then combining the results (e.g., via meta-analysis).\n\nThe paper shows that both naïve methods have:\n\nProper Type I Error Rates (Unbiased Test): When testing the null hypothesis of no \\(G-T\\) association, the methods maintain the correct Type I error rate unless both \\(G\\) and \\(T\\) are independently associated with the primary disease \\(D\\).\nUnbiased Estimates (Under Alternative): Under the alternative hypothesis (i.e., a true \\(G-T\\) association exists), the estimated effect size is unbiased only if the secondary trait \\(T\\) is not associated with the primary disease \\(D\\).\n\n\n\nSource of Bias\nThe bias in the naïve methods occurs when a significant confounding pathway exists: \\(G \\rightarrow D \\leftarrow T\\). Since the case-control study non-randomly samples based on \\(D\\), this selection distorts the observed association between \\(G\\) and \\(T\\)."
  },
  {
    "objectID": "genetics/monsees_2009_19365863.html#solution-inverse-probability-of-sampling-weighting-ipw",
    "href": "genetics/monsees_2009_19365863.html#solution-inverse-probability-of-sampling-weighting-ipw",
    "title": "Genome-wide association scans for secondary traits using case-control samples",
    "section": "Solution: Inverse-Probability-of-Sampling Weighting (IPW)",
    "text": "Solution: Inverse-Probability-of-Sampling Weighting (IPW)\nThe authors propose using IPW regression to correct for the ascertainment bias. IPW uses weights in the regression calculation that are inversely proportional to the probability of an individual being sampled into the study.\n\nThe weights are calculated based on the sampling fractions of cases and controls.\nPerformance: IPW regression yielded unbiased estimates of the \\(G-T\\) association and maintained the proper Type I error rate in all scenarios considered, regardless of the association between \\(G\\), \\(T\\), and \\(D\\).\nTrade-off: IPW regression consistently demonstrated lower statistical power than the naïve analyses in situations where the naïve analyses were also unbiased."
  },
  {
    "objectID": "genetics/monsees_2009_19365863.html#practical-recommendations",
    "href": "genetics/monsees_2009_19365863.html#practical-recommendations",
    "title": "Genome-wide association scans for secondary traits using case-control samples",
    "section": "Practical Recommendations",
    "text": "Practical Recommendations\nThe study concludes with practical recommendations for GWAS analysis of secondary traits:\n\nGeneral Markers: For the vast majority of markers tested in a GWAS (which are not associated with the primary disease \\(D\\)), the naïve analyses are valid tests of association and provide nearly unbiased estimates of the \\(G-T\\) association.\nDisease-Associated Markers: Care must be taken when both the marker (\\(G\\)) and the secondary trait (\\(T\\)) are associated with the primary disease (\\(D\\)). In this scenario, the naïve estimates will be biased, and IPW regression is the statistically valid method to obtain an unbiased estimate of the \\(G-T\\) effect.\nIllustration: The authors illustrate the potential for bias using an analysis of the relationship between a marker in the FGFR2 gene (a known breast cancer risk locus) and mammographic density in a breast cancer case-control sample."
  },
  {
    "objectID": "genetics/benson_2023_37582364.html",
    "href": "genetics/benson_2023_37582364.html",
    "title": "Protein-metabolite association studies identify novel proteomic determinants of metabolite levels in human plasma",
    "section": "",
    "text": "PubMed: 37582364 DOI: 10.1016/j.cmet.2023.07.012 Overview generated by: Gemini 2.5 Flash, 27/11/2025"
  },
  {
    "objectID": "genetics/benson_2023_37582364.html#background-and-objective",
    "href": "genetics/benson_2023_37582364.html#background-and-objective",
    "title": "Protein-metabolite association studies identify novel proteomic determinants of metabolite levels in human plasma",
    "section": "Background and Objective",
    "text": "Background and Objective\nCirculating levels of proteins and metabolites in human plasma reflect the physiological state of an individual and are strongly associated with the risk of various complex diseases, particularly cardio-metabolic disorders. While many associations have been identified, distinguishing between causal relationships (where a protein concentration change directly causes a metabolite level change) and confounded associations (where both are affected by a third factor) remains a major challenge.\nThe primary objective of this study was to integrate proteomic, metabolomic, and genomic data using Mendelian Randomization (MR) to systematically identify putative causal relationships between circulating proteins and metabolites in human plasma."
  },
  {
    "objectID": "genetics/benson_2023_37582364.html#study-methods-and-data-integration",
    "href": "genetics/benson_2023_37582364.html#study-methods-and-data-integration",
    "title": "Protein-metabolite association studies identify novel proteomic determinants of metabolite levels in human plasma",
    "section": "Study Methods and Data Integration",
    "text": "Study Methods and Data Integration\nThe study employed a large-scale, multi-stage, multi-omics approach:\n\nData Cohorts: Quantitative proteomic (1,302 proteins) and metabolomic (365 metabolites) data were meta-analyzed across three large population studies (Jackson Heart Study, Multi-Ethnic Study of Atherosclerosis, and Health, Risk Factors, Exercise Training and Genetics), totaling 3,626 individuals.\nPairwise Association Analysis: The study first identified 172,000 significant pairwise correlations between proteins and metabolites across the three cohorts.\nCausal Inference via Mendelian Randomization (MR): To overcome confounding, two-sample MR was applied using genetic instruments derived from protein-quantitative trait loci (pQTLs), specifically those located near the coding region of 535 proteins. These genetic variants served as instrumental variables to assess the causal effect of protein levels (exposure) on metabolite levels (outcome).\nMeta-Analysis and Validation: Causal estimates were meta-analyzed across the three studies. Sensitivity analyses (MR-Egger, weighted median) were performed to check for robustness against pleiotropy.\nIn Vivo Validation: To provide biological proof-of-concept, the top-ranking protein-to-metabolite causal associations were validated in vivo using metabolomic profiling of mouse knockout strains for the corresponding genes."
  },
  {
    "objectID": "genetics/benson_2023_37582364.html#key-results-and-findings",
    "href": "genetics/benson_2023_37582364.html#key-results-and-findings",
    "title": "Protein-metabolite association studies identify novel proteomic determinants of metabolite levels in human plasma",
    "section": "Key Results and Findings",
    "text": "Key Results and Findings\n\nCausal Protein-Metabolite Associations\n\nThe MR analysis identified 224 putative causal associations between 95 proteins and 96 metabolites.\nNovel Findings: Many of these causal links were novel, offering new insights into metabolic regulation. For instance, the study confirmed the causal role of Apolipoprotein C-III (ApoC3) in increasing triglycerides but also identified novel links, such as the causal role of protein ADAMTSL3 in regulating several plasma metabolites, particularly branched-chain amino acid (BCAA) metabolites.\n\n\n\nValidation of Causal Links\n\nIn Vivo Confirmation: Over 50% of the tested protein-to-metabolite causal associations were successfully validated in the mouse knockout models, providing strong experimental support for the causal nature of the in-silico findings. For example, knocking out the gene encoding the protein CSHL1 resulted in predicted changes in specific metabolites, validating the MR findings.\n\n\n\nNetwork and Pathway Insights\n\nThe study confirmed known metabolic hubs but also identified novel networks where proteins regulate metabolites. Many of the causal links highlighted pathways relevant to lipid metabolism and amino acid catabolism, known drivers of cardio-metabolic risk."
  },
  {
    "objectID": "genetics/benson_2023_37582364.html#conclusions-and-significance",
    "href": "genetics/benson_2023_37582364.html#conclusions-and-significance",
    "title": "Protein-metabolite association studies identify novel proteomic determinants of metabolite levels in human plasma",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nThis research successfully established a robust multi-omics-to-causality pipeline by integrating large-scale human proteomic, metabolomic, and genomic data. By using Mendelian Randomization and subsequent in vivo validation, the study identified 224 high-confidence protein-to-metabolite causal associations.\nThese findings significantly advance the understanding of the molecular determinants of metabolic traits, providing a valuable resource for identifying novel therapeutic targets for cardiovascular and metabolic diseases."
  },
  {
    "objectID": "genetics/aschard_2015_25640676.html",
    "href": "genetics/aschard_2015_25640676.html",
    "title": "Adjusting for Heritable Covariates Can Bias Effect Estimates in Genome-Wide Association Studies",
    "section": "",
    "text": "PubMed: 25640676\nDOI: 10.1016/j.ajhg.2014.12.021\nOverview generated by: Gemini 2.5 Flash, 26/11/2025"
  },
  {
    "objectID": "genetics/aschard_2015_25640676.html#key-findings",
    "href": "genetics/aschard_2015_25640676.html#key-findings",
    "title": "Adjusting for Heritable Covariates Can Bias Effect Estimates in Genome-Wide Association Studies",
    "section": "Key Findings",
    "text": "Key Findings\nThis seminal methodological report examines the statistical consequences of adjusting Genome-Wide Association Studies (GWAS) for heritable covariates (correlated traits that are themselves genetically influenced), conclusively demonstrating that this common practice introduces a significant collider bias.\n\nMain Discoveries\n\nCollider Bias Introduction: Adjusting a standard GWAS regression for a heritable covariate (\\(C\\)) that is correlated with the outcome (\\(Y\\)) and influenced by the SNP (\\(G\\)) introduces bias (also known as index event bias or selection bias). This happens because conditioning on the covariate (the collider) opens a spurious path between the SNP and unobserved confounders, which can lead to false positive associations with the primary outcome.\nUnbiased Estimation Condition: The resulting adjusted effect, \\(\\beta_{G \\rightarrow Y \\mid C}\\), accurately estimates the direct genetic effect on the outcome only under two strict causal models:\n\nThe SNP has no effect on the covariate (\\(\\beta_{G \\rightarrow C} = 0\\)).\nThe covariate (\\(C\\)) is a pure mediator, where the correlation between \\(C\\) and \\(Y\\) is entirely explained by a direct causal effect of \\(C\\) on \\(Y\\).\n\nBias Formula: For scenarios involving shared genetic or environmental risk factors, the bias (\\(\\text{Bias}\\)) in the adjusted genetic effect estimate is well-approximated by the equation: \\[\\text{Bias} \\approx -\\beta_{G \\rightarrow C} \\cdot \\rho_{C Y} \\cdot \\sqrt{\\frac{\\text{Var}(C)}{\\text{Var}(Y)}}\\] Where \\(\\beta_{G \\rightarrow C}\\) is the genetic effect on the covariate, and \\(\\rho_{C Y}\\) is the phenotypic correlation between the covariate and the outcome. The magnitude and direction of the bias are dependent on these terms."
  },
  {
    "objectID": "genetics/aschard_2015_25640676.html#study-design",
    "href": "genetics/aschard_2015_25640676.html#study-design",
    "title": "Adjusting for Heritable Covariates Can Bias Effect Estimates in Genome-Wide Association Studies",
    "section": "Study Design",
    "text": "Study Design\nThe study employed a rigorous statistical approach using established causal inference frameworks, detailed theoretical modeling, and numerical simulations.\n\nTheoretical Framework\n\nModeling of the True Direct Effect: The study defined the desired quantity, the direct genetic effect \\(\\beta_{G \\rightarrow Y}\\), as the effect of the SNP (\\(G\\)) on the outcome (\\(Y\\)) independent of the covariate (\\(C\\)). This is achieved by adjusting for all causal factors of the covariate.\nLinear Regression Model: The estimation was performed using a standard linear regression: \\(Y = \\alpha + \\beta_{G \\rightarrow Y \\mid C} G + \\gamma C + \\epsilon\\). The authors derived the expected value of the adjusted coefficient, \\(\\mathbb{E}[\\beta_{G \\rightarrow Y \\mid C}]\\), showing that it equals the true direct effect plus the bias term under various causal scenarios.\nBias Derivation: The bias was derived by considering the correlation structure induced when the covariate and the outcome share unobserved causes, specifically showing how adjustment for \\(C\\) introduces conditioning on a collider related to the SNP’s effect. The full mathematical expression for the bias was derived, from which the simplified approximation was obtained.\n\n\n\nSimulation Methods\n\nScenarios Tested: Simulations covered all three major causal scenarios: C is a mediator of the effect of \\(G\\) on \\(Y\\); Y is a mediator of the effect of \\(G\\) on \\(C\\); and \\(G\\) is a shared cause (pleiotropy) where the effect of \\(G\\) on \\(Y\\) is mediated by \\(C\\), or the traits share a hidden common environmental cause (\\(U\\)).\nParameters: Simulations varied key parameters, including heritability of the traits (up to \\(h^2 = 0.5\\)), the phenotypic correlation (\\(\\rho_{C Y}\\), up to \\(0.5\\)), and the genetic effect sizes (\\(\\beta_G\\)).\nEvaluation Metrics: The primary metrics used to evaluate the consequences of the bias were the Type I error rate (false positive rate) and the statistical power of the adjusted association test. Results showed Type I error inflation when the adjusted model was used in biased scenarios.\n\n\n\nReal-World Data Application\n\nData Source: GWAS summary statistics from the GIANT consortium meta-analysis of anthropometric traits were used, specifically:\n\nGWAS of Waist-to-Hip Ratio (WHR) adjusted for BMI (\\(Y \\mid C\\)).\nGWAS of BMI (\\(C\\)).\n\nEmpirical Test: A test was performed to look for an enrichment of SNPs with marginal effects in opposite directions on WHR and BMI. A highly significant enrichment (\\(p=0.005\\)) was found, providing empirical evidence that the statistical bias was present and inflating the number of significant loci."
  },
  {
    "objectID": "genetics/aschard_2015_25640676.html#major-results",
    "href": "genetics/aschard_2015_25640676.html#major-results",
    "title": "Adjusting for Heritable Covariates Can Bias Effect Estimates in Genome-Wide Association Studies",
    "section": "Major Results",
    "text": "Major Results\n\nPower Paradox: Adjustment for a heritable covariate results in increased statistical power when the signs of \\(\\beta_{G \\rightarrow Y \\mid C}\\) and the bias term are in opposite directions. This effect explains the increased detection of specific loci in the WHR adjusted for BMI GWAS.\nInterpretation Challenge: Since the adjusted estimates reflect a combination of the true direct effect and the bias term, they are neither the total genetic effect nor the direct genetic effect in most scenarios involving shared genetic or environmental risk factors.\nRelevance to Ratio Traits: The findings are directly relevant to analyses of ratio traits (e.g., WHR, fasting glucose/insulin ratios), as these are mathematically equivalent to performing a regression adjusted for the denominator."
  },
  {
    "objectID": "genetics/aschard_2015_25640676.html#practical-implications",
    "href": "genetics/aschard_2015_25640676.html#practical-implications",
    "title": "Adjusting for Heritable Covariates Can Bias Effect Estimates in Genome-Wide Association Studies",
    "section": "Practical Implications",
    "text": "Practical Implications\n\nRecommendations for Future GWAS\n\nPrioritize Unadjusted Analysis: For general genetic discovery and estimation of the total genetic effect (which includes effects mediated through other traits), the unadjusted GWAS of the primary outcome is the statistically unbiased standard.\nUse Bivariate Methods: To gain statistical power and accurately account for correlated traits without introducing collider bias, researchers should prefer multivariate or bivariate methods (e.g., those simultaneously modeling both \\(C\\) and \\(Y\\)) over simple regression adjustment.\nCausal Inference: If the research goal is specifically to estimate the direct causal effect, more advanced methods like Multivariable Mendelian Randomization (MVMR) should be considered to isolate the effect while mitigating the bias."
  },
  {
    "objectID": "genetics/aschard_2015_25640676.html#related-concepts",
    "href": "genetics/aschard_2015_25640676.html#related-concepts",
    "title": "Adjusting for Heritable Covariates Can Bias Effect Estimates in Genome-Wide Association Studies",
    "section": "Related Concepts",
    "text": "Related Concepts\n\nCollider Bias: A form of selection bias where conditioning on a variable that is a common effect (a collider) of two other variables induces a spurious association between the two causes.\nDirect vs. Total Genetic Effect: The direct effect is the association independent of the covariate; the total effect includes the portion mediated through the covariate.\nWaist-to-Hip Ratio (WHR) adjusted for BMI: The primary empirical example, where the adjustment for the highly heritable BMI creates a biased estimate for the remaining variance of WHR."
  },
  {
    "objectID": "genetics/tambets_2024_39210598.html",
    "href": "genetics/tambets_2024_39210598.html",
    "title": "Extensive co-regulation of neighboring genes complicates the use of eQTLs in target gene prioritization",
    "section": "",
    "text": "PubMed: 39210598 DOI: 10.1016/j.xhgg.2024.100348 Overview generated by: Gemini 2.5 Flash, 28/11/2025"
  },
  {
    "objectID": "genetics/tambets_2024_39210598.html#key-findings-limitations-of-eqtl-colocalization-in-causal-gene-prioritization",
    "href": "genetics/tambets_2024_39210598.html#key-findings-limitations-of-eqtl-colocalization-in-causal-gene-prioritization",
    "title": "Extensive co-regulation of neighboring genes complicates the use of eQTLs in target gene prioritization",
    "section": "Key Findings: Limitations of eQTL Colocalization in Causal Gene Prioritization",
    "text": "Key Findings: Limitations of eQTL Colocalization in Causal Gene Prioritization\nThe study addresses the fundamental problem in human genetics of linking non-coding regulatory variants from genome-wide association studies (GWASs) to their causal target genes. It establishes that the practice of using gene expression quantitative trait loci (eQTLs) for colocalization analysis to prioritize targets is complicated by the extensive co-regulation of neighboring genes. The authors found that existing colocalization methods are generally outperformed by a much simpler heuristic: assigning the fine-mapped variant to the closest protein-coding gene."
  },
  {
    "objectID": "genetics/tambets_2024_39210598.html#study-design-and-benchmarking-methods",
    "href": "genetics/tambets_2024_39210598.html#study-design-and-benchmarking-methods",
    "title": "Extensive co-regulation of neighboring genes complicates the use of eQTLs in target gene prioritization",
    "section": "Study Design and Benchmarking Methods",
    "text": "Study Design and Benchmarking Methods\n\nGround Truth Dataset\nThe researchers created a large ground truth dataset by re-analyzing fine-mapped plasma protein QTL (pQTL) data from 3,301 individuals in the INTERVAL cohort. Focusing on variants located within or close to the affected protein (cis-pQTLs), they assumed that the gene coding for the protein was the most likely causal gene for 793 proteins. This assumption provided a robust ground truth set for systematic benchmarking.\n\n\nMethods Evaluated\nThe study systematically compared three Bayesian colocalization methods and five Mendelian Randomization (MR) approaches:\n\nColocalization Methods:\n\ncoloc.susie (supports multiple causal variants).\ncoloc.abf (assumes a single causal variant).\nCLPP (colocalization posterior probability defined at the variant level).\n\nMendelian Randomization (MR) Methods: Five varieties were tested, including standard inverse-variance weighted MR (IVW-MR), MR-RAPS, and MRLocus."
  },
  {
    "objectID": "genetics/tambets_2024_39210598.html#results",
    "href": "genetics/tambets_2024_39210598.html#results",
    "title": "Extensive co-regulation of neighboring genes complicates the use of eQTLs in target gene prioritization",
    "section": "Results",
    "text": "Results\n\nPerformance of Prioritization Strategies\n\nClosest Gene Heuristic: Assigning pQTLs to their closest protein coding gene achieved the highest performance overall: 76.9% recall and 71.9% precision.\nColocalization Methods: When comparing performance using only the strongest signal, coloc.susie had the highest recall (46.3%) but lowest precision (45.1%). Conversely, CLPP was the most precise (68.5%) but yielded the correct gene for less than a fifth of the proteins (17.5% recall).\nColocalization + MR: Combining colocalization evidence with MR—restricting analysis to cases with two or more independent colocalizing signals—increased precision substantially to 81%. However, this led to a massive reduction in recall to just 7.1%, primarily due to the limited power of eQTL datasets to detect secondary eQTL signals.\n\n\n\nMR Assumption Violations\nThe standard inverse-variance-weighted MR often produced many false positives. The authors found that cis-eQTLs frequently violated MR assumptions, underscoring that using robust inference methods to account for these violations is essential to avoid inaccurate results."
  },
  {
    "objectID": "genetics/tambets_2024_39210598.html#conclusions-and-recommendations",
    "href": "genetics/tambets_2024_39210598.html#conclusions-and-recommendations",
    "title": "Extensive co-regulation of neighboring genes complicates the use of eQTLs in target gene prioritization",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study demonstrates that colocalization methods using eQTL data alone are insufficient and less effective than expected for accurately prioritizing causal genes underlying GWAS variants. The complexity of co-regulation highlights the need for robust methods. Ultimately, prioritizing novel targets in human genetics requires triangulation of evidence from multiple sources (e.g., integrating pQTLs, eQTLs, and other functional data) rather than relying on a single data modality."
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html",
    "href": "genetics/dewalsche_2025_39792927.html",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "",
    "text": "PubMed: 39792927\nDOI: 10.1371/journal.pgen.1011553\nOverview generated by: Claude Sonnet 4.5, 25/11/2025"
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#key-findings",
    "href": "genetics/dewalsche_2025_39792927.html#key-findings",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Key Findings",
    "text": "Key Findings\nThis study introduces metaGE, a meta-analysis approach for detecting quantitative trait loci (QTL) in multi-environment trial (MET) experiments by jointly analyzing summary statistics from individual environment GWAS, addressing the challenge of genotype-by-environment (GxE) interactions in plant genetics.\n\nMain Discoveries\n\nSuperior Type I error control: metaGE effectively controls false discovery rate (FDR ≤ 0.05) across simulated scenarios, while competing methods (METAL, mash) show severe inflation (FDR &gt; 0.84)\nComputational efficiency: Analyzes large-scale datasets (22 environments × 600K markers) in ~2 minutes, dramatically faster than existing mixed-model approaches\nNovel QTL detection: Identified new loci in Arabidopsis (competition-responsive flowering QTLs) and maize (heat-stress responsive yield QTLs) not detected by original single-environment analyses\nFlexible testing framework: Enables detection of QTLs with stable effects, environment-specific effects, or effects correlated with environmental covariates"
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#study-design",
    "href": "genetics/dewalsche_2025_39792927.html#study-design",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Study Design",
    "text": "Study Design\n\nMethodological Framework\nmetaGE jointly analyzes summary statistics (effect sizes and P-values) from per-environment GWAS without requiring raw phenotypic or genotypic data.\nKey innovations: - Accounts for inter-environment correlations arising from overlapping genotype panels - Supports both controlled (fixed-effect) and uncontrolled (random-effect) environments - Includes meta-regression to detect QTL-environment covariate relationships\n\n\nStatistical Models\nZ-score transformation: \\[Z_{mk} = \\Phi^{-1}(0.5 \\cdot p_{mk}) \\times \\text{sign}(\\beta_{mk})\\]\nwhere β_mk is the marker effect and p_mk is the P-value for marker m in environment k.\nFixed Effect (FE) Model (controlled environments): \\[Z_m = X\\mu_m + E_m\\] \\[E_m \\sim N(0_K, \\Sigma_m)\\]\n\nEnvironments classified into J groups with stable effects within groups\nTests for marker association: H₀: {μ_m = 0_J} vs H₁: {∃j, μ^j_m ≠ 0}\nTests for effect heterogeneity across groups\n\nRandom Effect (RE) Model (uncontrolled environments): \\[Z_m = \\mu_m \\mathbf{1}_K + A_m + E_m\\] \\[A_m \\sim N(0_K, \\tau_m^2 \\Lambda)\\]\n\nRandom marker effects account for heterogeneity\nCorrelation matrices Σ and Λ estimated from data\n\nMeta-Regression Test: \\[H_0: \\{\\text{cov}(\\mu_m \\mathbf{1}_K + A_m, X) = 0\\} \\text{ vs } H_1: \\{\\text{cov}(\\mu_m \\mathbf{1}_K + A_m, X) \\neq 0\\}\\]\nTest statistic: \\(\\frac{Z_m^T X}{\\sqrt{X^T \\Sigma X}} \\sim N(0,1)\\) under H₀\n\n\nCorrelation Matrix Estimation\nTwo filtering approaches to identify H₀ markers:\n\nP-value threshold: Include markers with p_mk &gt; λ in all environments\nPosterior probability (default): Mixture model-based filtering excluding markers with P(H₁) &gt; 0.6\n\nCorrelation estimate: \\[\\hat{\\Sigma}_{k,k'} = \\text{cor}(Z_k, Z_{k'}) = \\frac{\\sum_{m \\in H_0} (Z_{mk} - \\bar{Z}_k)(Z_{mk'} - \\bar{Z}_{k'})}{\\sqrt{\\sum (Z_{mk} - \\bar{Z}_k)^2} \\sqrt{\\sum (Z_{mk'} - \\bar{Z}_{k'})^2}}\\]"
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#simulation-study",
    "href": "genetics/dewalsche_2025_39792927.html#simulation-study",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Simulation Study",
    "text": "Simulation Study\n\nDesign\n\nGenotypes: 247 maize F1 hybrids, 506,460 SNPs\nEnvironments: 22 trials\nQTL types:\n\nFixed effect (constant across environments)\nCompletely random effect\nRandom effects correlated with environment similarities\n\nCovariate-dependent (proportional to Tmax, Tnight, or Psi)\n\nSimulations: 50 runs per QTL type, 12 QTLs per run\nHeritability: 0.5, QTLs explain 44% of genetic variance\n\n\n\nType I Error Control\nFDR on H₀ chromosomes (most stringent):\n\n\n\nMethod\nQTL Type\nFDR_chr\n\n\n\n\nmetaGE_FE\nFixed\n0.00\n\n\nmetaGE_RE\nRandom\n0.04\n\n\nmetaGE_RE\nRandomCov\n0.02\n\n\nmetaGE_MR\nCovariate\n0.00-0.02\n\n\nMETAL_FE\nFixed\n1.00\n\n\nMETAL_RE\nRandom/Cov\n0.88-1.00\n\n\nmash\nAll types\n0.14-0.88\n\n\n\nWhole genome FDR (5 Mb window):\n\n\n\nMethod\nRange across scenarios\n\n\n\n\nmetaGE\n0.09-0.18\n\n\nMETAL\n0.93-0.94\n\n\nmash\n0.32-0.85\n\n\n\n\n\nDetection Power\n5 Mb detection window:\n\n\n\nQTL Type\nmetaGE\nMETAL\nmash\n\n\n\n\nFixed effect\n0.09\n0.98\n0.20\n\n\nRandom effect\n0.51\n0.16\n0.79\n\n\nRandomCov\n0.26\n0.58\n0.53\n\n\nCovariate (avg)\n0.37\n0.06\n0.41\n\n\n\nDespite lower raw power than competitors, metaGE’s proper FDR control makes identified associations reliable.\nMAF effect on power (metaGE_FE): - Low MAF [0.20-0.25]: 0.04 - Medium MAF [0.30-0.35]: 0.08 - High MAF [0.40-0.45]: 0.14\n\n\nMeta-Regression Specificity\nTesting covariate-dependent QTLs:\n\n\n\nMR test\nTarget QTLs detected\nCross-detected\n\n\n\n\nTnight\n34.5% (Tnight QTLs)\n5.5% (Tmax QTLs)\n\n\nTmax\n26.5% (Tmax QTLs)\n7% (Tnight QTLs)\n\n\nPsi\n52% (Psi QTLs)\n&lt;1% (others)\n\n\n\nCross-detection correlated with environmental covariate correlation (r_Tnight-Tmax = 0.71, r_Tnight-Psi = -0.11)."
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#application-i-arabidopsis-competition-response",
    "href": "genetics/dewalsche_2025_39792927.html#application-i-arabidopsis-competition-response",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Application I: Arabidopsis Competition Response",
    "text": "Application I: Arabidopsis Competition Response\n\nDataset\n\n195 accessions, 981,278 SNPs\n6 controlled micro-habitats (3 soils × competition/no competition)\nTrait: Bolting time\nCompetition: Poa annua weed in environments B, D, F\n\n\n\nResults\nmetaGE FE procedure: - 191 SNPs in 61 QTLs identified - 51/61 significant in at least one individual GWAS - Enrichment ratio = 4.13 for candidate flowering genes (q₀.₀₅ = 0.066, q₀.₉₅ = 3.2)\nComparison with METAL: - METAL: &gt;165,000 P-values &lt;0.01 (expected ~10,000 under H₀) - Declared 15% of markers significant (severe inflation)\nContrasted FE test (competition vs. no competition): - 221 SNPs in 72 QTLs with environment-specific effects - 160 candidate genes enriched for: - Development (P = 8.9×10⁻³) - Cell processes (P = 1.5×10⁻³) - Tetrapyrrole synthesis (P = 0.020) - 71/72 QTLs were novel (not detected by standard FE test)\n\n\nMajor Finding: QTL5_22.0\nLocation: Chromosome 5, AtCNGC4 genomic region - 22 markers with sign-switching effects based on competition - Positive effects without competition, negative/null with competition - AtCNGC4 known roles: - Floral transition regulation - Plant immunity impairment - Consistent with development-defense tradeoffs"
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#application-ii-maize-drought-response",
    "href": "genetics/dewalsche_2025_39792927.html#application-ii-maize-drought-response",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Application II: Maize Drought Response",
    "text": "Application II: Maize Drought Response\n\nDataset\n\n244 dent maize lines (as hybrids), 602,356 SNPs\n22 environments (location × year × treatment)\nTrait: Grain yield\nEnvironmental covariates: Psi, Tmax, Tnight, Rad, VPDmax, ET0, Tnight.Fill\n\n\n\nmetaGE RE Results\n52 genomic regions identified, including:\n\n\n\nQTL\nChr\nLocal Score\nDetection status\n\n\n\n\nQTL3_120.0\n3\n38\nPreviously reported\n\n\nQTL6_20.3\n6\n415\nPreviously reported\n\n\nQTL7_41.4\n7\n18\nNovel\n\n\n\nQTL6_20.3 analysis: - Strong effects in 6 environments with severe heatwaves: - Night temperature ~22°C - Maximum temperature &gt;36°C - High evaporative demand (3.6 KPa) - All 6 environments: P-values &lt;1×10⁻⁶ - Colocalizes with 2.4 Mb presence/absence variant - Contains ABA-induced genes for water deficit response - Shows selection signatures during domestication/improvement\nQTL7_41.4 (novel): - Moderate positive effects across ~10/22 environments - Significant in only 2 individual GWAS (P &lt;0.01 in 10) - Harbors QTLs for plant growth rate and biomass under water deficit - Demonstrates power gain from meta-analysis\n\n\nMeta-Regression Results\nEvapotranspiration (ET0): 14 QTLs detected\nKey finding - QTL2_153.8 (marker AX-91538480): - Effects vary linearly from negative to positive with ET0 - Colocalizes with aquaporin eQTLs (PIP2.2, PIP2.1) - Related to water use efficiency and stomatal conductance\nNight temperature during flowering (Tnight): 21 QTLs - Main association &lt;0.6 Mb from QTL6_20.3 - Corroborates previous findings on heat stress response\nNight temperature during grain filling (Tnight.Fill): 15 QTLs\nExample - QTL9_28.6 (marker AX-91123283): - Positive effects on cool nights - Negative effects on hot nights - Dramatic effect reversal with temperature"
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#application-iii-multi-parent-population",
    "href": "genetics/dewalsche_2025_39792927.html#application-iii-multi-parent-population",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Application III: Multi-Parent Population",
    "text": "Application III: Multi-Parent Population\n\nEU-NAM Flint Dataset\n\n11 biparental populations (8 analyzed)\n5,263 SNPs, double haploid lines\n4 locations: La Coruna, Roggenstein, Einbeck, Ploudaniel\nTrait: Biomass dry matter yield\n32 analyses (8 populations × 4 locations)\n\n\n\nResults\n16 QTLs identified, including: - 2 major QTLs also found in original publication (Garin et al.): - QTL1_117.6: Consistent across populations except F2 - QTL6_84.2: Ancestral allele (6 parents) with strong negative effect in TUM\n10 novel QTLs, including: - 5 QTLs with effect inversions between populations\nExample - QTL5_23.9: - Positive effect in F03802 population - Negative effect in F64 population - Suggests genetic background effects or allelic series\n3 QTLs associated with flowering time: - Flowering time is simpler trait and yield driver - Correlation with yield varies by environment (negative/null/positive)\n\n\nAdvantages Over Original Analysis\nOriginal study (Garin et al.): - Limited to 2/4 locations - Analyzed with computationally intensive mixed models\nmetaGE approach: - Included all 4 locations - Revealed 10 additional QTLs - Completed in 12 seconds vs. hours for mixed models"
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#application-iv-wheat-supplementary",
    "href": "genetics/dewalsche_2025_39792927.html#application-iv-wheat-supplementary",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Application IV: Wheat (Supplementary)",
    "text": "Application IV: Wheat (Supplementary)\n\nDataset\n\n210 wheat lines, 108,410 SNPs\n16 environments (location × year × treatment)\nTrait: Grain yield\n\n\n\nKey Findings\n\nAll QTLs identified by metaGE RE were not significant in any single environment\nDemonstrates power gain for complex traits with small-effect QTLs\nHighlights importance of joint analysis for yield traits"
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#computational-performance",
    "href": "genetics/dewalsche_2025_39792927.html#computational-performance",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Computational Performance",
    "text": "Computational Performance\nRuntime comparison (dataset: marker count):\n\n\n\nDataset\nEnvironments\nmetaGE\nMETAL\nmash\n\n\n\n\nSimulation (500K)\n22\n49s (31s*)\n2.6min\n16.6min\n\n\nArabidopsis (1M)\n6\n1.2min (26s*)\n2.6min\n29s\n\n\nMaize (600K)\n22\n2.25min (41s*)\n3.3min\n25.3min\n\n\nEU-NAM (6K)\n32\n12s (8s*)\n3s\n1.8min\n\n\nWheat (100K)\n16\n47s (30s*)\n22s\n3.3min\n\n\n\n*Time for correlation matrix inference (needs to be done only once)\nMemory efficiency: - Handles 10⁵-10⁶ markers efficiently - Single correlation matrix estimation per analysis - Independent processing of multiple hypotheses without re-estimation"
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#methodological-advantages",
    "href": "genetics/dewalsche_2025_39792927.html#methodological-advantages",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Methodological Advantages",
    "text": "Methodological Advantages\n\nOver Classical Meta-Analysis (METAL)\nDependency handling: - METAL assumes independence between GWAS - Ignoring dependencies in MET causes severe FDR inflation (&gt;0.84) - metaGE explicitly models inter-environment correlations\nResult: METAL unusable for MET analysis due to Type I error inflation\n\n\nOver Mixture Models (mash)\nEnvironmental factors: - mash models different effect patterns but not environmental influences - Cannot incorporate environmental covariates - Limited ability to test specific biological hypotheses about GxE\nResult: mash suitable for pleiotropy but not designed for MET analysis\n\n\nOver Mixed Models\nScalability: - Mixed models computationally prohibitive for large-scale GWAS - Require raw phenotypic and genotypic data - metaGE: summary statistics only, minutes vs. hours/days\nFlexibility: - Easy addition/removal of environments - Handles missing data (monomorphic markers in subpopulations) - Supports unbalanced/incomplete designs without imputation\n\n\nComparison to Subgroup Meta-Analysis\nPrevious work (human genetics): - Subgroup MA and meta-regression developed for independent studies - Not adapted to correlated studies (MET with overlapping panels)\nmetaGE contribution: - First adaptation of these approaches to non-independent studies - Enables plant genetics applications"
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#novel-testing-capabilities",
    "href": "genetics/dewalsche_2025_39792927.html#novel-testing-capabilities",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Novel Testing Capabilities",
    "text": "Novel Testing Capabilities\n\n1. Standard Association Test\nH₀: {μ_m = 0} - marker has no effect in any environment - Detects QTLs with any non-zero effect\n\n\n2. Heterogeneity Test\nH₀: {μ¹_m = μ²_m = … = μᴶ_m} - effects constant across groups - Identifies environment-dependent QTLs\n\n\n3. Contrast Test\nTests specific hypotheses about effect patterns - Example: Competition vs. no competition in Arabidopsis - Detected 71 new QTLs missed by standard test\n\n\n4. Meta-Regression\nGenome-wide scan for QTL-covariate relationships - Quantifies how QTL effects vary with environmental variables - Identifies adaptive QTLs responding to specific stresses"
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#biological-insights",
    "href": "genetics/dewalsche_2025_39792927.html#biological-insights",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Biological Insights",
    "text": "Biological Insights\n\nPower Gain Through Joint Analysis\nArabidopsis AtCNGC4 region: - Not genome-wide significant in individual environments - Highly significant in joint analysis - Biological relevance confirmed (floral transition, immunity)\nMaize QTL7_41.4: - Significant in only 2/22 environments individually - Detected through meta-analysis - Contains known water deficit response QTLs\nWheat QTLs: - None significant in individual environments - Multiple QTLs detected jointly - Critical for complex yield traits\n\n\nInterpreting Effect Variability\nCompetition response (Arabidopsis): - Sign-switching effects indicate context-dependent gene function - Development-defense tradeoffs - Identifies condition-specific adaptive alleles\nHeat stress response (Maize): - QTL6_20.3 effects clustered in heatwave environments - Presence/absence variant under selection - Adaptive response to temperature stress\nCovariate-dependent effects: - Linear relationships between effects and ET0, temperature - Aquaporin-mediated water transport regulation - Plant growth sensitivity to water potential"
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#data-sharing-and-privacy",
    "href": "genetics/dewalsche_2025_39792927.html#data-sharing-and-privacy",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Data Sharing and Privacy",
    "text": "Data Sharing and Privacy\n\nAdvantages of Summary Statistics\nConfidentiality: - No raw phenotypic or genotypic data required - Only effect sizes and P-values needed - Enables data sharing between private breeding programs\nParallel to human genetics: - Global Biobank Meta-analysis Initiative (2.2M participants, 24 BioBanks) - Consortium approach without individual data sharing\nPlant breeding applications: - Private companies can share GWAS results - Preserve competitive advantages - Collaborative QTL discovery\n\n\nTechnical Benefits\nUnbalanced designs: - Different markers tested per environment - Missing data due to monomorphism in subpopulations - No imputation required\nScale flexibility: - Different technologies/sequencing depths - Easy environment addition/removal - Post-hoc quality control\nMulti-parent populations: - Different marker sets per family - Handles genetic background effects - Detects allelic series and epistasis"
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#practical-recommendations",
    "href": "genetics/dewalsche_2025_39792927.html#practical-recommendations",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Practical Recommendations",
    "text": "Practical Recommendations\n\nWhen to Use metaGE\nIdeal scenarios: - MET experiments with overlapping genotype panels - Need to control Type I error rate - Testing specific GxE hypotheses - Limited computational resources - Data privacy concerns\nNot recommended: - Single environment analysis (use standard GWAS) - Completely independent populations (classical MA sufficient) - Need individual-level covariate adjustments\n\n\nModel Selection\nFixed Effect (FE) model: - Controlled environments with a priori classification - Testing specific group contrasts - Example: Stress vs. control treatments\nRandom Effect (RE) model: - Uncontrolled field conditions - Unknown/complex environment relationships - Heterogeneous QTL effects expected\nMeta-Regression: - Quantitative environmental covariates available - Hypothesis about specific environmental drivers - Want to identify adaptive QTLs\n\n\nMultiple Testing Control\nLocal score approach (default): - Controls FDR while accounting for LD - Accumulates evidence across linked markers - Threshold ξ typically 3-4 - Chromosome-specific significance thresholds\nAlternative: Adaptive Benjamini-Hochberg - For low-density markers (e.g., MPP with &lt;10K SNPs) - When LD structure unknown"
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#implementation-details",
    "href": "genetics/dewalsche_2025_39792927.html#implementation-details",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Implementation Details",
    "text": "Implementation Details\n\nR Package: metaGE\nAvailable on CRAN\nKey functions: - Fixed effect meta-analysis - Random effect meta-analysis\n- Contrast testing - Meta-regression - Local score multiple testing correction\nInput requirements: - Per-environment GWAS summary statistics (effects, P-values) - Marker positions - Optional: Environmental covariates\nOutputs: - Meta-analysis P-values - Estimated correlation matrices - Significant genomic regions - Effect size estimates per environment/group"
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#limitations-and-considerations",
    "href": "genetics/dewalsche_2025_39792927.html#limitations-and-considerations",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Limitations and Considerations",
    "text": "Limitations and Considerations\n\nStatistical Assumptions\n\nMarker independence: Assumes unlinked markers\n\nAddressed by local score accounting for LD\n\nCorrelation matrix: Assumed common across markers\n\nReasonable for inter-environment correlations\nReduces computational burden\n\nNormal distribution: Z-scores assumed Gaussian under H₀\n\nStandard assumption in GWAS\nViolated if P-values not uniformly distributed under null\n\n\n\n\nDesign Considerations\nEnvironment classification: - FE model requires a priori grouping - Misclassification reduces power - RE model robust to classification uncertainty\nSample size: - Power increases with more environments - Individual environment sample sizes affect P-value quality - Minimum ~5-10 environments recommended\nCovariate correlation: - Meta-regression may detect QTLs correlated with related covariates - Careful interpretation needed with high covariate correlation - Consider testing multiple covariates independently"
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#related-concepts",
    "href": "genetics/dewalsche_2025_39792927.html#related-concepts",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Related Concepts",
    "text": "Related Concepts\n\nMulti-environment trial (MET): Same genotypes evaluated across locations/years\nGenotype-by-environment (GxE) interaction: Differential genotypic response to environments\nZ-score: Standardized test statistic combining effect sign and P-value\nInter-environment correlation: Similarity between environments due to overlapping panels\nLocal score: LD-aware multiple testing procedure accumulating evidence\nMeta-regression: Modeling association between study-level covariates and effect sizes\nAllelic series: Multiple haplotypes at a locus with varying effects"
  },
  {
    "objectID": "genetics/timpson_2017_29225335.html",
    "href": "genetics/timpson_2017_29225335.html",
    "title": "Genetic architecture: the shape of the genetic contribution to human traits and disease",
    "section": "",
    "text": "PubMed: 29225335\nDOI: 10.1038/nrg.2017.101\nOverview generated by: Gemini 2.5 Flash, 26/11/2025"
  },
  {
    "objectID": "genetics/timpson_2017_29225335.html#key-findings",
    "href": "genetics/timpson_2017_29225335.html#key-findings",
    "title": "Genetic architecture: the shape of the genetic contribution to human traits and disease",
    "section": "Key Findings",
    "text": "Key Findings\nThis comprehensive review synthesizes the field’s understanding of genetic architecture—the characteristics of genetic variation responsible for heritable phenotypic variability—in the era following the advent of large-scale Genome-Wide Association Studies (GWAS) and Next-Generation Sequencing (NGS). The authors systematically define the components of genetic architecture and explain how recent technological advances have begun to reveal the complex interplay of factors contributing to human traits and diseases.\n\nCore Components of Genetic Architecture\nThe genetic architecture of any complex trait is defined by four interacting components, which the review explores in depth:\n\nNumber of Causal Variants: The sheer count of genetic variants (SNPs, indels, SVs) that collectively affect the trait. Most complex traits are highly polygenic, involving thousands of variants.\nEffect Size Distribution: The magnitude of the effect that each variant contributes to the phenotype. GWAS has revealed that many common variants have small, additive effects, while rare variants often have large effects.\nAllele Frequency Spectrum: The distribution of causal variant frequencies in the population. Common traits are often influenced by variants across the entire frequency spectrum, supporting the ‘common disease, common variant’ and ‘common disease, rare variant’ hypotheses in tandem.\nInteractions: The complexity added by non-additive relationships:\n\nAllelic Interactions: Dominance (interaction between alleles at the same locus).\nLocus Interactions: Epistasis (interaction between alleles at different loci).\nEnvironmental Interactions: Gene-by-Environment (GxE) interaction."
  },
  {
    "objectID": "genetics/timpson_2017_29225335.html#impact-of-modern-genomic-technologies-on-architecture-discovery",
    "href": "genetics/timpson_2017_29225335.html#impact-of-modern-genomic-technologies-on-architecture-discovery",
    "title": "Genetic architecture: the shape of the genetic contribution to human traits and disease",
    "section": "Impact of Modern Genomic Technologies on Architecture Discovery",
    "text": "Impact of Modern Genomic Technologies on Architecture Discovery\nThe review highlights how different technologies have been instrumental in characterizing specific aspects of the genetic architecture:\n\nGWAS and Common Variant Architecture\n\nGWAS Success: GWAS has been highly successful in identifying thousands of common, low-effect variants for hundreds of traits, confirming the extreme polygenicity of complex traits.\nMissing Heritability: The review addresses the historical problem of “missing heritability”—the gap between heritability estimated from twin/family studies (broad-sense heritability) and that explained by all detected common SNPs (SNP-heritability). Explanations include:\n\nThe contribution of rare variants missed by GWAS arrays.\nThe residual influence of non-additive effects (dominance and epistasis) captured by family studies but not fully by linear GWAS models.\nThe contribution of structural variation and gene-environment interactions.\n\nLocus Heterogeneity: GWAS often reveals multiple independent associated signals within the same locus, indicating allelic series or complex local regulation.\n\n\n\nNext-Generation Sequencing (NGS) and Rare Variants\n\nSequencing Role: NGS studies (e.g., whole-exome sequencing, whole-genome sequencing) are essential for characterizing the role of rare variants.\nBurden Tests: These tests aggregate the effects of multiple rare variants within a single gene or region. The review notes that rare, high-penetrance variants often reside in genes under strong negative selection, explaining why their overall contribution to population variance (though individually large) may be limited.\nClinical Relevance: Rare variants are crucial for understanding Mendelian disease and for identifying genes with large effects that are strong candidates for drug targets."
  },
  {
    "objectID": "genetics/timpson_2017_29225335.html#complexity-of-genetic-effects",
    "href": "genetics/timpson_2017_29225335.html#complexity-of-genetic-effects",
    "title": "Genetic architecture: the shape of the genetic contribution to human traits and disease",
    "section": "Complexity of Genetic Effects",
    "text": "Complexity of Genetic Effects\n\nPleiotropy and Shared Genetic Etiology\n\nWidespread Pleiotropy: The authors emphasize that pleiotropy (a single genetic variant affecting multiple distinct traits) is the norm, not the exception, for common variants identified by GWAS. This is supported by analyses showing extensive genetic correlation between traits.\nConfounding: Pleiotropy complicates causal inference. The review discusses how techniques like Mendelian Randomization (MR) are used to distinguish true causal effects from horizontal pleiotropy (a single variant affecting multiple outcomes through different pathways).\n\n\n\nGene-Environment (GxE) Interactions\n\nDefinition: GxE occurs when the effect of a genetic variant on a phenotype depends on the individual’s environment (e.g., diet, smoking, stress).\nDetection Challenge: GxE interactions are notoriously difficult to detect and estimate accurately due to requiring large samples with precise environmental measures. The review notes that population-based cohorts like the UK Biobank are vital for making progress in this area."
  },
  {
    "objectID": "genetics/timpson_2017_29225335.html#future-directions-and-clinical-goals",
    "href": "genetics/timpson_2017_29225335.html#future-directions-and-clinical-goals",
    "title": "Genetic architecture: the shape of the genetic contribution to human traits and disease",
    "section": "Future Directions and Clinical Goals",
    "text": "Future Directions and Clinical Goals\nThe review concludes by outlining the necessary steps to fully characterize genetic architecture and achieve the field’s clinical goals:\n\nComprehensive Mapping: Moving from association studies to causal variant identification, focusing on non-coding variants and improving fine-mapping methods.\nAccounting for Non-Additivity: Developing statistical methods that are better powered to detect and estimate dominance and epistatic effects in large cohorts.\nIntegrating Environment: Robustly incorporating environmental exposure data into models to quantify the contribution of GxE interactions and improve personalized risk prediction.\nClinical Translation: Leveraging the understanding of genetic architecture to improve disease screening, diagnosis, prognosis, and therapeutic development. This includes prioritizing genes for drug development based on the magnitude and specificity of their genetic effects."
  },
  {
    "objectID": "multi-omics.html",
    "href": "multi-omics.html",
    "title": "multi-omics",
    "section": "",
    "text": "Integrating untargeted metabolomics, genetically informed causal inference, and pathway enrichment to define the obesity metabolome\n\nApproach: A multi-omics framework was developed, combining untargeted metabolomics (measuring both known and unknown metabolites) with two-sample Mendelian Randomization (MR) using metabolite-QTLs (mQTLs).\nCausal Findings: 23 metabolites (15 known and 8 unknown) were identified as causally associated with BMI, with specific pathways like amino acid catabolism and lipid metabolism being implicated in the obesity metabolome.\nInnovation: A novel pathway enrichment method was used to infer the metabolic function of the causally associated unknown metabolites based on their shared genetic links with identified metabolites.\n\n\n\nCausal integration of multi-omics data with prior knowledge to generate mechanistic hypotheses\n\nMethod: The paper introduces COSMOS (Causal Oriented Search of Multi-Omics Space), a computational tool that causally integrates phosphoproteomics, transcriptomics, and metabolomics with a vast prior knowledge network.\nCausal Inference: COSMOS infers the activity of key regulators (kinases, TFs) and then calculates a causal score for regulatory links to generate testable hypotheses that cross the molecular layers.\nHypotheses: Applied to Renal Cell Carcinoma data, COSMOS generated specific hypotheses, such as a causal cascade from the TSSK4 kinase to the ZHX2 transcription factor and finally to the regulation of the GAPDH enzyme/metabolism.\n\n\n\nProtein-metabolite association studies identify novel proteomic determinants of metabolite levels in human plasma\n\nApproach: A large-scale multi-omics resource was created by meta-analyzing proteomic and metabolomic data from three cohorts, followed by the use of Mendelian Randomization (MR) with pQTLs to infer causal relationships.\nCausal Findings: The study identified 224 putative causal associations between 95 proteins and 96 metabolites, including novel links like the causal role of ADAMTSL3 in regulating BCAA metabolites.\nValidation: Over 50% of the top causal findings were experimentally validated through metabolomic profiling of mouse knockout strains, providing strong biological proof for the in-silico MR results.\n\n\n\nMulti-INTACT: integrative analysis of the genome, transcriptome, and proteome identifies causal mechanisms of complex traits\n\nMethod: Multi-INTACT is a novel statistical framework that jointly analyzes GWAS, eQTL, and pQTL summary statistics to model the causal chain from genetic variant \\(\\rightarrow\\) gene expression \\(\\rightarrow\\) protein level \\(\\rightarrow\\) complex trait.\nCausal Partitioning: The method successfully partitions GWAS heritability and identifies the precise molecular layer (transcriptome or proteome) mediating the genetic effect, showing that many effects are primarily mediated by protein levels.\nImpact: Applied to complex traits like lipids, Multi-INTACT confirmed known genes and revealed novel gene-trait associations by providing the mechanistic evidence (the specific regulatory path) driving the GWAS signal.\n\n\n\nVariable selection for generalized canonical correlation analysis\n\nMethod: The paper introduces SGCCA (Sparse Generalized Canonical Correlation Analysis), an extension of the RGCCA framework designed for integrating three or more multi-omics data blocks.\nKey Innovation: SGCCA incorporates a sparse (\\(L_1\\)) penalty to simultaneously perform dimension reduction and variable selection.\nSignificance: SGCCA pinpoints a minimal, highly relevant set of features from each omics layer that drives the shared correlation structure across the integrated datasets, significantly improving the biological interpretability of multi-omics results.\n\n\n\nDIABLO: an integrative approach for identifying key molecular drivers from multi-omics assays\n\nMethod: DIABLO (Data Integration Analysis for Biomarker discovery using Latent variable approaches for Omics datasets) is a supervised multi-block PLS/GCCA method for joint analysis of heterogeneous omics data.\nFeature Selection: It uses a sparse penalty (L1) to select a minimal set of key molecular drivers that are highly correlated across omics layers and maximally associated with a specific clinical outcome (e.g., disease status).\nImpact: DIABLO demonstrated superior classification accuracy and biological coherence in identifying integrated biomarkers for complex diseases, such as the molecular drivers distinguishing breast cancer subtypes.\n\n\n\nMulti-Omics Factor Analysis a framework for unsupervised integration of multi-omics data sets\n\nMethod: MOFA (Multi-Omics Factor Analysis) is an unsupervised statistical framework based on Factor Analysis to integrate multi-omics data.\nKey Innovation: MOFA infers a set of latent factors that capture the principal sources of variation and, crucially, disentangle shared heterogeneity (variation common to multiple omics layers) from modality-specific heterogeneity.\nImpact: Applied to Chronic Lymphocytic Leukemia (CLL) data, MOFA identified factors associated with key clinical drivers (e.g., IGHV mutation status) and enabled robust patient subtyping and data imputation.\n\n\n\nIdentifying temporal and spatial patterns of variation from multimodal data using MEFISTO\n\nMethod: MEFISTO (Multi-omics Factor Analysis Informed by Spatial and Temporal Omics) is an extension of MOFA that uses Gaussian Process (GP) priors on latent factors to model spatial or temporal dependencies between samples.\nKey Capabilities: It performs spatio-temporally informed dimensionality reduction, allowing factors to change smoothly over time/space. It also enables robust interpolation of data for unobserved locations or time points.\nApplication: MEFISTO successfully analyzed data from spatial transcriptomics, longitudinal microbiome studies, and single-cell multi-omics atlases to align and extract common developmental or temporal patterns.\n\n\n\nA fully Bayesian latent variable model for integrative clustering analysis of multi-type omics data\n\nMethod: The paper proposes a fully Bayesian latent variable model for integrative clustering analysis of multi-omics data, building on the iCluster framework.\nKey Innovation: The Bayesian approach incorporates adaptive shrinkage priors to enforce sparsity (feature selection) on the omics-specific loading matrices, which simultaneously identifies robust disease subtypes and their minimal molecular signatures.\nImpact: Applied to TCGA cancer data, the model demonstrated superior performance in identifying clinically relevant, stable subtypes and the specific genes/loci driving the differences across mRNA, methylation, and CNV data.\n\n\n\nA General Framework for Integrative Analysis of Incomplete Multi-omics Data\n\nProblem: Multi-omics analysis is challenged by missing values (incomplete subject profiling) and detection limits (censored data).\nMethod: A general statistical framework based on a joint likelihood function and an Expectation-Maximization (EM) algorithm was developed to rigorously model and integrate multi-omics data while accounting for arbitrary missingness and censoring.\nImpact: Applied to the SPIROMICS cohort, the framework demonstrated superior statistical power and reduced bias compared to ad-hoc imputation methods, particularly in identifying protein quantitative trait loci (pQTLs) and biomarker-phenotype associations.\n\n\n\nMulti-Omic Graph Diagnosis (MOGDx): a data integration tool to perform classification tasks for heterogeneous diseases\n\nMethod: MOGDx (Multi-Omic Graph Diagnosis) is a data integration tool that utilizes Graph Convolutional Networks (GCNs) to perform supervised classification tasks for heterogeneous diseases.\nIntegration Principle: It models patients as nodes in a graph, with omics data as node features and molecular similarity as edges, allowing the GNN to learn complex, non-linear cross-omics patterns.\nImpact: MOGDx achieved superior classification accuracy in cancer subtyping tasks, offers robustness to missing data, and facilitates the identification of the key molecular biomarkers driving disease heterogeneity.\n\n\n\nStatistical Methods for Integrative Clustering of Multi-omics Data\n\nTopic: A detailed overview and tutorial on Statistical Methods for Integrative Clustering of Multi-omics Data, primarily focusing on model-based approaches for identifying cancer subtypes.\nMethods Covered: Reviews and contrasts iCluster/iClusterPlus (latent variable models with sparsity for feature selection) and Similarity Network Fusion (SNF) (nonparametric, network-based fusion).\nImpact: Emphasizes the methods’ ability to yield robust and biologically relevant disease subtypes and their defining molecular signatures by integrating heterogeneous data (e.g., mRNA, methylation, CNV), exemplified using TCGA cancer cohorts.\n\n\n\nDisease prediction with multi-omics and biomarkers empowers case-control genetic discoveries in the UK Biobank\n\nMethod: MILTON (Machine Learning with Phenotype Associations), an ensemble machine-learning framework, was developed to integrate multi-omics (including plasma proteomics) and biomarker data from the UK Biobank to predict disease risk.\nObjective: To demonstrate how these biomarker-based predictions can augment genetic association analyses in a phenome-wide context.\nImpact: MILTON outperformed Polygenic Risk Scores (PRSs) in predicting incident disease. Its application in a PheWAS improved signals for 88 known and 14 novel genetic associations, showing its utility in empowering genetic discovery for complex diseases by improving disease classification.\n\n\n\nBenchmarking joint multi-omics dimensionality reduction approaches for the study of cancer\n\nTopic: A systematic benchmarking of nine joint Dimensionality Reduction (jDR) methods for integrating multi-omics data, using simulated data, TCGA cancer cohorts, and single-cell data.\nKey Findings: intNMF excelled in unsupervised clustering tasks, while MCIA (Multiple Co-Inertia Analysis) was identified as the most robust, all-around performer across various prediction and integration tasks.\nResource: The study created a reproducible code platform called momix to aid researchers in selecting and applying jDR methods, offering practical guidelines for multi-omics integration.\n\n\n\nBiomarker identification by interpretable maximum mean discrepancy\n\nTopic: Introduction of SpInOpt-MMD (Sparse, Interpretable, and Optimized Maximum Mean Discrepancy), a novel method for simultaneously performing two-sample testing and biomarker feature selection in high-dimensional omics data.\nMethod: SpInOpt-MMD integrates sparse and interpretable optimization into the Maximum Mean Discrepancy (MMD) test, allowing it to detect statistically significant group differences and identify the features (biomarkers) responsible in a single step.\nImpact: The method is effective for small sample sizes and outperforms other feature selection approaches (like SHAP) in several contexts, offering a powerful, unified approach for biomarker discovery in multi-omics and biomedical applications."
  },
  {
    "objectID": "pgs/ding_2023_37198491.html",
    "href": "pgs/ding_2023_37198491.html",
    "title": "Polygenic scoring accuracy varies across the genetic ancestry continuum",
    "section": "",
    "text": "PubMed: 37198491 DOI: 10.1038/s41586-023-06079-4 Overview generated by: Gemini 2.5 Flash, 26/11/2025"
  },
  {
    "objectID": "pgs/ding_2023_37198491.html#key-findings-the-genetic-distance-penalty-on-pgs-accuracy",
    "href": "pgs/ding_2023_37198491.html#key-findings-the-genetic-distance-penalty-on-pgs-accuracy",
    "title": "Polygenic scoring accuracy varies across the genetic ancestry continuum",
    "section": "Key Findings: The Genetic Distance Penalty on PGS Accuracy",
    "text": "Key Findings: The Genetic Distance Penalty on PGS Accuracy\nThis study provides a critical, individual-level assessment of Polygenic Score (PGS) portability, which is necessary for the equitable clinical application of genetic risk prediction. The authors argue that assessing PGS performance using traditional discrete genetic ancestry clusters (e.g., European, African) obscures crucial inter-individual variation and biases estimates. They introduce a framework that evaluates accuracy along a genetic ancestry continuum using a precise metric: Genetic Distance (GD).\n\nCore Discovery: Continuous and Steep Decay of Accuracy\nThe central finding is the demonstration that PGS accuracy decreases individual-to-individual along the continuum of genetic ancestries in a highly predictable, linear fashion .\n\nMetric Definition: Genetic Distance (GD) is defined as the distance of a target individual’s genotype (e.g., PCA projection) from the population used to train the PGS model. The higher the GD, the more genetically dissimilar the individual is from the training set.\nQuantification of Decay: Across a large set of 84 complex traits and diseases, the average individual-level PGS accuracy showed an extremely powerful negative correlation with GD, with a Pearson correlation coefficient of -0.95. This near-perfect correlation highlights that the individual’s genetic background relative to the training population is the primary determinant of score performance.\nUbiquity of Variation: This decreasing trend was observed in all populations considered, including within traditionally labeled ‘homogeneous’ genetic ancestry groups (e.g., European ancestry in UK Biobank). This shows that sub-ancestry variation within a single continent still results in a measurable loss of accuracy based on GD.\n\n\n\nDemonstrating Inequity and Systematic Bias\nThe study leveraged data from the UK Biobank (UKBB, training set, predominantly White British) and the diverse Los Angeles biobank (ATLAS, testing set) to quantify the transferability gap.\n\nIntra-European Penalty: When applying UKBB-trained models to individuals of European ancestry in ATLAS, those in the furthest GD decile experienced a significant 14% lower accuracy relative to those in the closest decile.\nCross-Ancestry Disparity: The results reveal a severe “distance penalty” for non-European groups. Individuals of Hispanic/Latino American ancestry who are genetically closest (lowest GD decile) to the training data showed similar PGS performance to the European-ancestry individuals who are furthest away (highest GD decile). For the most distant Hispanic/Latino individuals, accuracy was substantially lower, overlapping with that of African American participants.\nBias in Risk Estimates: Crucially, GD was found to be significantly correlated with the PGS estimates themselves for 82 of 84 traits. This means the systematic bias due to ancestry distance does not just affect the accuracy (\\(R^2\\)) but also the magnitude of the predicted risk, potentially leading to widespread miscalibration and inequitable risk stratification.\n\n\n\nConclusion and Call to Action\nThe authors conclude that relying on aggregate population-level metrics (\\(\\text{e.g., } R^2\\)) obscures this vital individual-level variation and hinders efforts toward health equity. They urge researchers to abandon the use of discrete genetic ancestry clusters in favor of continuous metrics (like GD) to better characterize and address performance disparities, ensuring more reliable and equitable application of PGSs in personalized medicine."
  },
  {
    "objectID": "pgs/mars_2022_35591975.html",
    "href": "pgs/mars_2022_35591975.html",
    "title": "Genome-wide risk prediction of common diseases across ancestries in one million people",
    "section": "",
    "text": "PubMed: 35591975\nDOI: 10.1016/j.xgen.2022.100118\nOverview generated by: Gemini 2.5 Flash, 26/11/2025"
  },
  {
    "objectID": "pgs/mars_2022_35591975.html#key-findings",
    "href": "pgs/mars_2022_35591975.html#key-findings",
    "title": "Genome-wide risk prediction of common diseases across ancestries in one million people",
    "section": "Key Findings",
    "text": "Key Findings\nThis study performed a large-scale, cross-ancestry evaluation of Polygenic Risk Scores (PRSs) for four major common diseases—Coronary Artery Disease (CAD), Type 2 Diabetes (T2D), Breast Cancer, and Prostate Cancer—using genome-wide genotype data from six biobanks across Europe, the United States, and Asia, encompassing over one million individuals. The principal finding is a striking disparity in PRS transferability and accuracy: the predictive ability of PRSs remains robust and highly similar across various European populations and local population substructures, suggesting utility in clinical settings for this group. However, the PRSs exhibited significantly poorer transferability and substantially lower effect sizes in individuals of African ancestry, and to a lesser extent, in South Asian and East Asian ancestries. This large-scale empirical evidence underscores the immediate challenge of ancestral bias in genomic data and highlights the potential for the clinical implementation of current PRSs to exacerbate existing health disparities."
  },
  {
    "objectID": "pgs/mars_2022_35591975.html#study-design-and-data",
    "href": "pgs/mars_2022_35591975.html#study-design-and-data",
    "title": "Genome-wide risk prediction of common diseases across ancestries in one million people",
    "section": "Study Design and Data",
    "text": "Study Design and Data\nThe study utilized a combined dataset of approximately one million individuals across six major biobanks: BioBank Japan, Estonian Biobank, FinnGen, HUNT, Mass General Brigham (MGB) Biobank, and UK Biobank. The ancestries evaluated included European, South Asian, East Asian, and African.\n\nPRS Calculation and Evaluation\n\nPRS Method: Genome-wide PRSs were calculated using LDpred, a method that accounts for linkage disequilibrium (LD) and uses a Bayesian approach to estimate SNP effect sizes, incorporating over 6 million variants for each disease.\nInput Data: The input weights were obtained from the largest publicly available, non-overlapping Genome-Wide Association Studies (GWASs) for each of the four diseases.\nTransferability Assessment: Transferability was assessed by comparing the Odds Ratios (OR) per standard deviation (SD) increase in PRS across different global ancestry groups, and also within European populations (including a population isolate, Finland)."
  },
  {
    "objectID": "pgs/mars_2022_35591975.html#key-results-on-transferability",
    "href": "pgs/mars_2022_35591975.html#key-results-on-transferability",
    "title": "Genome-wide risk prediction of common diseases across ancestries in one million people",
    "section": "Key Results on Transferability",
    "text": "Key Results on Transferability\n\nGlobal Ancestry Disparities\nA clear gradient of PRS accuracy was observed, directly correlated with the genetic distance from the primary European GWAS training cohorts:\n\nEuropean Ancestry: The PRSs showed consistently high and similar effect sizes (ORs) across various European populations and health-care systems, suggesting good utility for risk stratification.\nAsian Ancestry: Individuals of South Asian and East Asian ancestry exhibited similar or slightly lower effect sizes compared to Europeans.\nAfrican Ancestry: Individuals of African ancestry consistently had the lowest effect sizes and poorest prediction accuracy for all four diseases. For instance, in breast cancer, the association was not statistically significant in women of African ancestry in some cohorts.\n\n\n\nSubstructure and Polygenicity\n\nWithin-European Transferability: The PRSs transferred well even between highly structured European populations, such as various regional substructures within Finland, demonstrating robustness across recent population bottlenecks.\nGenome-wide vs. Sparse PRS: A crucial methodological finding was that the highly polygenic, genome-wide PRSs (using millions of variants) displayed higher effect sizes and better transferability across global ancestries than PRSs containing only a smaller, more stringently selected set of variants (sparse PRSs). This supports the notion that the polygenic nature of these traits is captured across different populations, even if the fine-mapping of causal variants differs."
  },
  {
    "objectID": "pgs/mars_2022_35591975.html#implications-for-clinical-utility",
    "href": "pgs/mars_2022_35591975.html#implications-for-clinical-utility",
    "title": "Genome-wide risk prediction of common diseases across ancestries in one million people",
    "section": "Implications for Clinical Utility",
    "text": "Implications for Clinical Utility\nThe findings provide strong evidence that the current state of PRS technology is not ready for equitable clinical deployment:\n\nClinical Utility: Current PRSs have demonstrated significant potential for clinical screening and prevention in individuals of European ancestry.\nHealth Equity Concern: The low predictive accuracy in individuals of African ancestry, South Asian, and East Asian ancestry—stemming from the lack of diversity in the original GWAS training data—poses a significant challenge to global health equity and personalized medicine. The study stresses the urgent necessity of investing in and executing large-scale GWAS in non-European populations to address this bias."
  },
  {
    "objectID": "index.html#proteomics",
    "href": "index.html#proteomics",
    "title": "",
    "section": "proteomics",
    "text": "proteomics"
  },
  {
    "objectID": "index.html#metabolomics",
    "href": "index.html#metabolomics",
    "title": "",
    "section": "metabolomics",
    "text": "metabolomics"
  },
  {
    "objectID": "index.html#multi-omics",
    "href": "index.html#multi-omics",
    "title": "",
    "section": "multi-omics",
    "text": "multi-omics"
  },
  {
    "objectID": "index.html#statistics",
    "href": "index.html#statistics",
    "title": "",
    "section": "statistics",
    "text": "statistics"
  },
  {
    "objectID": "index.html#mr",
    "href": "index.html#mr",
    "title": "",
    "section": "MR",
    "text": "MR"
  },
  {
    "objectID": "mr.html",
    "href": "mr.html",
    "title": "Proteomics",
    "section": "",
    "text": "test\ntest"
  },
  {
    "objectID": "genetics.html",
    "href": "genetics.html",
    "title": "genetics",
    "section": "",
    "text": "1/3 of pQTL signals (Olink/Somalogic) are epitope effects\n\n\nSpecificity, length and luck drive gene rankings in association studies\n\nSystematic comparison of GWAS and rare variant burden tests across 209 UK Biobank traits revealing they prioritize different genes through distinct mechanisms\nBurden tests favor trait-specific genes while GWAS capture both trait-specific genes and context-specific variants on pleiotropic genes\nGene length and genetic drift are major confounders affecting rankings in burden tests and GWAS respectively\n\n\n\nmetaGE: Investigating genotype x environment interactions through GWAS meta-analysis\n\nNovel meta-analysis approach for multi-environment trials (METs) that jointly analyzes GWAS summary statistics while accounting for inter-environment correlations\nControls Type I error effectively (FDR ≤0.05) where competing methods fail severely (METAL FDR &gt;0.84), with computational efficiency enabling analysis of 600K markers × 22 environments in ~2 minutes\nIdentified novel competition-responsive flowering QTLs in Arabidopsis and heat-stress yield QTLs in maize through contrast tests and meta-regression with environmental covariates\n\n\n\nUsing GWAS summary data to impute traits for genotyped individuals\n\nNovel nonparametric LS-imputation method recovers genetic components of traits from GWAS summary statistics and individual genotypes, enabling nonlinear association analyses impossible with summary data alone\nPerfectly recovers trait values when test genotypes match training genotypes (correlation &gt;0.999), capturing nonlinear SNP-trait information despite using only linear marginal associations\nOutperforms PRS-CS for association analyses in UK Biobank HDL data: successfully detects non-additive genetic effects, SNP-SNP interactions, and trains nonlinear prediction models (random forests) while PRS-CS shows severe false positive inflation\n\n\n\nAdjusting for Heritable Covariates Can Bias Effect Estimates in Genome-Wide Association Studies\n\nAdjusting a Genome-Wide Association Study (GWAS) for a heritable covariate (a correlated, genetically influenced trait) introduces an unintended collider bias, which distorts SNP effect estimates and can lead to false positive associations.\nThe bias is approximately proportional to the product of the genetic effect on the covariate and the phenotypic correlation between the traits, and was empirically confirmed by finding a significant enrichment of SNPs with opposite effects in the WHR adjusted for BMI GWAS (\\(p=0.005\\)).\nThe authors strongly caution against interpreting adjusted results as true direct genetic effects, recommending unadjusted GWAS for total effect discovery and bivariate methods for power gains without inducing collider bias.\n\n\n\nGenetic architecture: the shape of the genetic contribution to human traits and disease\n\nThis review defines genetic architecture by four components: the number of causal variants (polygenicity), the distribution of their effect sizes, their allele frequency spectrum, and the types of genetic and environmental interactions (dominance, epistasis, GxE).\nIt highlights that complex traits are highly polygenic and influenced by variants across the entire frequency spectrum, addressing “missing heritability” by pointing to the role of rare variants, non-additive effects, and Gene-by-Environment (GxE) interactions.\nThe authors emphasize that pleiotropy (one variant affecting multiple traits) is widespread among common variants, discussing how techniques like Mendelian Randomization (MR) are essential for distinguishing causation from pleiotropy in the complex genetic landscape.\n\n\n\nGenome-wide association scans for secondary traits using case-control samples\n\nThis statistical methodology paper examines the bias introduced when a case-control GWAS (designed for a primary disease \\(D\\)) is used to analyze a secondary quantitative trait (\\(T\\)).\nIt demonstrates that naïve analysis (ignoring case-control ascertainment) leads to biased effect estimates for the marker-secondary trait association (\\(G-T\\)) specifically when both the marker \\(G\\) and the trait \\(T\\) are independently associated with the primary disease \\(D\\).\nThe authors propose using Inverse-Probability-of-Sampling-Weighted (IPW) regression as the robust method, which provides unbiased estimates in all scenarios, though at the cost of reduced statistical power, recommending naïve analysis for markers not associated with the primary disease.\n\n\n\nJoint analysis of GWAS and multi-omics QTL summary statistics reveals a large fraction of GWAS signals shared with molecular phenotypes\n\nObjective: The new method, OPERA, was developed to integrate GWAS and multi-omics QTL (xQTL) summary statistics to quantify the proportion of complex trait genetic signals mediated by molecular phenotypes.\nKey Finding: The study found that approximately 50% of genetic signals identified in GWAS are shared with (and likely mediated by) at least one molecular phenotype, with eQTLs (gene expression QTLs) being the most dominant mediators.\nImpact: OPERA led to the discovery of 89 novel genes for 11 complex traits, confirming the approach’s ability to significantly enhance gene discovery and fine-mapping by linking genetic variants to their underlying molecular regulatory mechanisms.\n\n\n\nThe contribution of genetic determinants of blood gene expression and splicing to molecular phenotypes and health outcomes\n\nTopic: Investigating the gene-regulatory mechanisms (eQTLs and sQTLs) of nonprotein-coding genetic variants in blood and their causal contribution to 3,430 molecular phenotypes (proteins, metabolites, lipids) and health outcomes.\nMethod: Mapped eQTLs and sQTLs in 4,732 individuals and used colocalization and mediation analyses to link these regulatory variants to downstream molecular and disease traits.\nImpact: Identified 222 molecular phenotypes significantly mediated by gene expression or splicing, providing mechanistic insights into diseases (e.g., \\(WARS1\\) in hypertension) and offering a valuable public resource for human genetic etiology.\n\n\n\nExtensive co-regulation of neighboring genes complicates the use of eQTLs in target gene prioritization\n\nResearch Problem: Systematic benchmarking showed that using eQTL colocalization methods to prioritize causal genes for GWAS hits is often complicated by extensive co-regulation of neighboring genes and is less effective than simpler heuristics.\nKey Result: The simple strategy of assigning fine-mapped pQTLs to the closest protein coding gene significantly outperformed all tested Bayesian colocalization methods, achieving 76.9% recall and 71.9% precision.\nConclusion: Linking GWAS variants to target genes remains challenging using eQTL evidence alone, and robust gene prioritization requires the triangulation of evidence from multiple functional sources to improve confidence.\n\n\n\nGlobal quantification of mammalian gene expression control\n\nCore Discovery: This seminal study, using parallel metabolic pulse labeling and absolute quantification in mammalian cells, concluded that the cellular abundance of proteins is predominantly controlled at the level of translation, not transcription.\nQuantification: The study found a strong correlation between mRNA and protein abundance (\\(R \\approx 0.73\\)) but no correlation between the half-lives (turnover rates) of corresponding mRNA and proteins.\nMechanism: Protein synthesis rates (translation) were found to be the most variable component, serving as the primary determinant of protein steady-state levels, while degradation rates primarily determine the kinetics and response time of the system."
  },
  {
    "objectID": "metabolomics/amara_2022_35350714.html",
    "href": "metabolomics/amara_2022_35350714.html",
    "title": "Networks and Graphs Discovery in Metabolomics Data Analysis and Interpretation",
    "section": "",
    "text": "PubMed: 35350714 DOI: 10.3389/fmolb.2022.841373 Overview generated by: Gemini 2.5 Flash, 28/11/2025"
  },
  {
    "objectID": "metabolomics/amara_2022_35350714.html#key-focus-the-application-of-graph-theory-in-metabolomics",
    "href": "metabolomics/amara_2022_35350714.html#key-focus-the-application-of-graph-theory-in-metabolomics",
    "title": "Networks and Graphs Discovery in Metabolomics Data Analysis and Interpretation",
    "section": "Key Focus: The Application of Graph Theory in Metabolomics",
    "text": "Key Focus: The Application of Graph Theory in Metabolomics\nThis review article provides a comprehensive overview of how networks and graph theory are used as analytical and interpretive tools in metabolomics data analysis. It focuses on the shift from viewing the metabolome as a list of molecules to a structured system of interactions, which is crucial for making biological sense of complex high-throughput data.\n\nThe Role of Graph Theory\nIn metabolomics, graphs (or networks) are mathematical structures used to represent relationships between two entities: * Nodes (Vertices): Represent the metabolites, genes, proteins, samples, or analytical features (e.g., MS ions). * Edges (Links): Represent the connections or relationships between the nodes, which can be chemical similarity, biological correlation, co-occurrence, or a known reaction."
  },
  {
    "objectID": "metabolomics/amara_2022_35350714.html#network-types-and-their-applications",
    "href": "metabolomics/amara_2022_35350714.html#network-types-and-their-applications",
    "title": "Networks and Graphs Discovery in Metabolomics Data Analysis and Interpretation",
    "section": "Network Types and Their Applications",
    "text": "Network Types and Their Applications\nThe review categorizes network applications into two main areas based on the type of data they analyze:\n\n1. Analytical/Chemical Networks\nThese networks are derived directly from mass spectrometry (MS) data and are primarily used for compound identification and annotation.\n\nMolecular Networking (MN): This is the most prominent example. It connects MS/MS spectra based on the similarity of their fragmentation patterns. This allows researchers to group structurally related metabolites (e.g., compounds in the same chemical family) into clusters, enabling the identification of unknown members once one member of the cluster is known\n\n[Image of Molecular Networking Graph] . * Feature-Based Molecular Networking (FBMN): An extension that integrates chromatographic and quantitative data for more robust connections.\n\n\n2. Biological/Correlation Networks\nThese networks are derived from quantitative abundance data and are primarily used for biological interpretation and pathway discovery.\n\nMetabolite-Metabolite Correlation Networks: Nodes are metabolites, and edges represent a significant statistical correlation in their concentration changes across different samples or conditions. These correlations can indicate shared regulation, sequential reactions in a metabolic pathway, or common transporters.\nMetabolite-Pathway Networks: These map identified metabolites onto established biochemical pathways (e.g., KEGG, MetaCyc) to visualize which pathways are perturbed in a given experiment.\nIntegrated Omics Networks: Graphs are essential for multi-omics integration, connecting metabolites to other molecular entities like transcripts (mRNA) and proteins. The edges often represent genetic co-expression, shared regulation, or enzyme-substrate relationships."
  },
  {
    "objectID": "metabolomics/amara_2022_35350714.html#key-advantages",
    "href": "metabolomics/amara_2022_35350714.html#key-advantages",
    "title": "Networks and Graphs Discovery in Metabolomics Data Analysis and Interpretation",
    "section": "Key Advantages",
    "text": "Key Advantages\nThe application of graph theory offers significant advantages for metabolomics: * Discovery of Hidden Relationships: Networks can reveal complex, non-linear relationships that are missed by traditional univariate statistics. * Hypothesis Generation: Network hubs (highly connected nodes) often represent key regulatory or rate-limiting enzymes and metabolites, pointing to critical biological control points. * Visualization: They provide an intuitive and powerful way to visualize and communicate complex, high-dimensional data."
  },
  {
    "objectID": "metabolomics/amara_2022_35350714.html#conclusion",
    "href": "metabolomics/amara_2022_35350714.html#conclusion",
    "title": "Networks and Graphs Discovery in Metabolomics Data Analysis and Interpretation",
    "section": "Conclusion",
    "text": "Conclusion\nThe review concludes that network and graph-based approaches are fundamental to modern metabolomics, bridging the gap between raw analytical data and comprehensive biological understanding. They are crucial for moving the field forward, especially in the context of integrating data from multiple omics layers."
  },
  {
    "objectID": "metabolomics/sullivan_2016_27658530.html",
    "href": "metabolomics/sullivan_2016_27658530.html",
    "title": "Altered metabolite levels in cancer: implications for tumour biology and cancer therapy",
    "section": "",
    "text": "PubMed: 27658530 DOI: 10.1038/nrc.2016.85 Overview generated by: Gemini 2.5 Flash, 28/11/2025"
  },
  {
    "objectID": "metabolomics/sullivan_2016_27658530.html#key-focus-metabolite-concentration-as-a-driver-of-cancer-biology",
    "href": "metabolomics/sullivan_2016_27658530.html#key-focus-metabolite-concentration-as-a-driver-of-cancer-biology",
    "title": "Altered metabolite levels in cancer: implications for tumour biology and cancer therapy",
    "section": "Key Focus: Metabolite Concentration as a Driver of Cancer Biology",
    "text": "Key Focus: Metabolite Concentration as a Driver of Cancer Biology\nThis review explores how altered intracellular metabolite concentrations—a fundamental characteristic of cancer cells (metabolic reprogramming)—can actively promote tumor initiation, progression, and survival, rather than merely being a consequence of altered metabolism.\n\nThe Oncogenic Role of Metabolites\nThe core concept is that changes in the concentration of specific metabolites, often driven by genetic mutations or cancer-associated protein modifications, can act as oncometabolites or signaling molecules. These altered levels can then directly impact cell fate by modifying proteins, regulating gene expression, and altering redox balance."
  },
  {
    "objectID": "metabolomics/sullivan_2016_27658530.html#major-oncometabolites-and-their-mechanisms",
    "href": "metabolomics/sullivan_2016_27658530.html#major-oncometabolites-and-their-mechanisms",
    "title": "Altered metabolite levels in cancer: implications for tumour biology and cancer therapy",
    "section": "Major Oncometabolites and Their Mechanisms",
    "text": "Major Oncometabolites and Their Mechanisms\nThe review details several key metabolites whose altered levels have profound implications for cancer:\n\n1. 2-Hydroxyglutarate (2-HG)\n\nSource: Produced at high levels by mutations in Isocitrate Dehydrogenase 1 (IDH1) or IDH2.\nMechanism: 2-HG is an “oncometabolite” that functions as a potent competitive inhibitor of several \\(\\alpha\\)-ketoglutarate (\\(\\alpha\\)-KG)-dependent dioxygenases, including epigenetic regulators like TET DNA demethylases and histone demethylases.\nEffect: This inhibition leads to a hypermethylation phenotype and globally altered gene expression, promoting oncogenesis.\n\n\n\n2. Fumarate and Succinate\n\nSource: Accumulate due to mutations in the tricarboxylic acid (TCA) cycle enzymes Fumarate Hydratase (FH) and Succinate Dehydrogenase (SDH).\nMechanism: Similar to 2-HG, these two metabolites are also \\(\\alpha\\)-KG competitive inhibitors. They inhibit \\(\\alpha\\)-KG-dependent prolyl hydroxylase (PHD) enzymes.\nEffect: Inhibition of PHDs stabilizes the transcription factor Hypoxia-Inducible Factor 1\\(\\alpha\\) (HIF-\\(1\\alpha\\)). This leads to the activation of the Warburg effect and promotes cell proliferation and angiogenesis, even in normoxic conditions (pseudohypoxia).\n\n\n\n3. Aspartate\n\nSource: Can be limited in tumor environments, leading to reduced cell proliferation.\nEffect: Aspartate is a critical precursor for the synthesis of nucleotides (purines and pyrimidines). Its availability links the rate of mitochondrial metabolism (TCA cycle) directly to cell proliferation, acting as a metabolic checkpoint.\n\n\n\n4. Reactive Oxygen Species (ROS)\n\nSource: Increased ROS production due to altered mitochondrial function and high metabolic flux.\nDual Role: While high ROS levels can induce cell death, moderate, sustained increases in ROS can promote tumorigenesis by activating pro-survival signaling pathways and contributing to DNA damage."
  },
  {
    "objectID": "metabolomics/sullivan_2016_27658530.html#therapeutic-implications",
    "href": "metabolomics/sullivan_2016_27658530.html#therapeutic-implications",
    "title": "Altered metabolite levels in cancer: implications for tumour biology and cancer therapy",
    "section": "Therapeutic Implications",
    "text": "Therapeutic Implications\nUnderstanding the altered metabolome provides clear therapeutic vulnerabilities:\n\nTargeting Metabolite Effects: Drugs can be developed to counteract the downstream effects of oncometabolites (e.g., targeting the epigenetic readers or writers whose activity is modified by 2-HG).\nExploiting Dependencies: Cancer cells often become dependent on specific nutrients or pathways due to metabolic constraints (e.g., relying on external aspartate). Inhibiting the transport or synthesis of these essential metabolites could selectively kill tumor cells."
  },
  {
    "objectID": "metabolomics/sullivan_2016_27658530.html#conclusion",
    "href": "metabolomics/sullivan_2016_27658530.html#conclusion",
    "title": "Altered metabolite levels in cancer: implications for tumour biology and cancer therapy",
    "section": "Conclusion",
    "text": "Conclusion\nThe review concludes that changes in intracellular metabolite concentrations are a central feature of cancer cell biology, acting as effector molecules that dictate cancer phenotype. Metabolomics is thus vital for identifying new therapeutic targets and understanding the underlying mechanisms of malignancy."
  },
  {
    "objectID": "metabolomics/idkowiak_2025_41027880.html",
    "href": "metabolomics/idkowiak_2025_41027880.html",
    "title": "Best practices and tools in R and Python for statistical processing and visualization of lipidomics and metabolomics data",
    "section": "",
    "text": "PubMed: 41027880 DOI: 10.1038/s41467-025-63751-1 Overview generated by: Gemini 2.5 Flash, 28/11/2025"
  },
  {
    "objectID": "metabolomics/idkowiak_2025_41027880.html#key-focus-data-analysis-tools-and-best-practices-in-r-and-python",
    "href": "metabolomics/idkowiak_2025_41027880.html#key-focus-data-analysis-tools-and-best-practices-in-r-and-python",
    "title": "Best practices and tools in R and Python for statistical processing and visualization of lipidomics and metabolomics data",
    "section": "Key Focus: Data Analysis Tools and Best Practices in R and Python",
    "text": "Key Focus: Data Analysis Tools and Best Practices in R and Python\nThis review article serves as a comprehensive guide and compilation of best practices and freely accessible tools in R and Python for the statistical processing and visualization of mass spectrometry-based lipidomics and metabolomics data. The authors acknowledge that these “omics” generate extensive datasets that require specific data exploration skills to effectively identify and visualize statistically significant trends and biologically relevant differences.\n\nThe Need for Dedicated Tools\nMass spectrometry-based lipidomics and metabolomics workflows are characterized by high-dimensional data, complex normalization needs, and the necessity to integrate data with extensive metadata (such as clinical parameters). Standard spreadsheet software is insufficient for handling the volume and complexity of these datasets. The review addresses this gap by compiling and discussing computational resources tailored for this purpose.\n\n\nCore Areas Covered by the Review\nThe review focuses on the core stages of post-acquisition data analysis, primarily emphasizing exploratory data analysis (EDA) and visualization:\n\nStatistical Processing: Tools and packages for performing univariate and multivariate statistical analyses, including methods like Principal Component Analysis (PCA) and Partial Least Squares Discriminant Analysis (PLS-DA), which are standard for feature reduction and visualizing separation between groups.\nVisualization: Compilations of packages for generating high-quality graphical representations essential for biological interpretation, such as volcano plots, heatmaps, box plots, and specialized lipid/metabolite class distribution plots.\nBest Practices: Discussion of standardized workflows and best practices to ensure reproducibility and accurate results in data handling, which is critical given the inherent variability in mass spectrometry data.\nR and Python Focus: The article prioritizes tools within the R and Python ecosystems, which are the dominant platforms for modern biological data analysis due to their powerful statistical libraries and open-source nature."
  },
  {
    "objectID": "metabolomics/idkowiak_2025_41027880.html#conclusion-and-utility",
    "href": "metabolomics/idkowiak_2025_41027880.html#conclusion-and-utility",
    "title": "Best practices and tools in R and Python for statistical processing and visualization of lipidomics and metabolomics data",
    "section": "Conclusion and Utility",
    "text": "Conclusion and Utility\nThe review provides a valuable resource for researchers in the lipidomics and metabolomics fields, compiling the solid core of accessible tools required for transforming raw data into biologically meaningful insights. By focusing on R and Python, it guides users toward implementing robust, reproducible, and effective computational strategies for their high-dimensional data."
  },
  {
    "objectID": "multi-omics/adamer_2024_38940158.html",
    "href": "multi-omics/adamer_2024_38940158.html",
    "title": "Biomarker identification by interpretable maximum mean discrepancy",
    "section": "",
    "text": "PubMed: 38940158 DOI: 10.1093/bioinformatics/btae251 Overview generated by: Gemini 2.5 Flash, 27/11/2025"
  },
  {
    "objectID": "multi-omics/adamer_2024_38940158.html#background-and-objective",
    "href": "multi-omics/adamer_2024_38940158.html#background-and-objective",
    "title": "Biomarker identification by interpretable maximum mean discrepancy",
    "section": "Background and Objective",
    "text": "Background and Objective\nIn biomedical applications, researchers frequently deal with paired groups of samples (e.g., treated vs. control, or diseased vs. healthy) and aim to identify discriminating features, or biomarkers, based on high-dimensional omics data. This problem is fundamentally a two-sample problem, requiring a statistical test to establish a difference between the groups and a method to interpret which features cause that difference.\nWhile the multivariate Maximum Mean Discrepancy (MMD) test can quantify group-level differences, the identification of specific features (biomarkers) usually requires a separate, often less powerful, univariate feature selection step. The objective of this study was to introduce a novel method that combines two-sample testing and feature selection into a single, unified experiment."
  },
  {
    "objectID": "multi-omics/adamer_2024_38940158.html#methods-spinopt-mmd",
    "href": "multi-omics/adamer_2024_38940158.html#methods-spinopt-mmd",
    "title": "Biomarker identification by interpretable maximum mean discrepancy",
    "section": "Methods: SpInOpt-MMD",
    "text": "Methods: SpInOpt-MMD\nThe authors developed SpInOpt-MMD (Sparse, Interpretable, and Optimized MMD test), a novel statistical framework designed to simultaneously test for differences between two high-dimensional datasets and identify the most relevant features contributing to that difference.\n\nApproach: SpInOpt-MMD extends the standard MMD by incorporating sparse and interpretable optimization techniques. This allows the model to quantify the difference between two sample distributions (two-sample test) while simultaneously performing feature selection.\nFeature Selection: Unlike methods that rely on subsequent univariate analysis or complex post-hoc interpretation (like SHapley Additive exPlanations, or SHAP), SpInOpt-MMD directly outputs the set of statistically significant and distinguishing features (biomarkers) as part of the core testing process.\nVersatility: The method is versatile and was demonstrated on a variety of data types, including gene expression measurements, text data, and images."
  },
  {
    "objectID": "multi-omics/adamer_2024_38940158.html#key-results-and-conclusion",
    "href": "multi-omics/adamer_2024_38940158.html#key-results-and-conclusion",
    "title": "Biomarker identification by interpretable maximum mean discrepancy",
    "section": "Key Results and Conclusion",
    "text": "Key Results and Conclusion\nThe evaluation of SpInOpt-MMD highlighted its effectiveness, particularly in challenging scenarios:\n\nSuperior Performance: SpInOpt-MMD was shown to be highly effective at identifying relevant features, even in small sample sizes, and demonstrated superior performance compared to traditional feature selection methods such as SHAP and univariate association analysis in several experiments.\nUnified Analysis: The method provides a powerful, single-step solution for the two-sample testing and biomarker identification problem in high-dimensional settings, which is crucial for multi-omics research.\nOpen-Access Resource: The code and links to the public data are made available to promote reproducibility and widespread adoption of the new method."
  },
  {
    "objectID": "multi-omics/hsu_2020_32467615.html",
    "href": "multi-omics/hsu_2020_32467615.html",
    "title": "Integrating untargeted metabolomics, genetically informed causal inference, and pathway enrichment to define the obesity metabolome",
    "section": "",
    "text": "PubMed: 32467615 DOI: 10.1038/s41366-020-0603-x Overview generated by: Gemini 2.5 Flash, 27/11/2025"
  },
  {
    "objectID": "multi-omics/hsu_2020_32467615.html#background-and-objective",
    "href": "multi-omics/hsu_2020_32467615.html#background-and-objective",
    "title": "Integrating untargeted metabolomics, genetically informed causal inference, and pathway enrichment to define the obesity metabolome",
    "section": "Background and Objective",
    "text": "Background and Objective\nObesity is associated with significant metabolic disruption. Understanding the causal connections between obesity and changes in metabolite levels is crucial for identifying new intervention targets. Previous studies using Mendelian Randomization (MR) to infer causality between metabolites and obesity often ignored the majority of data generated by untargeted metabolomics—specifically, the signals from unknown or unidentified metabolites.\nThis study aimed to develop a comprehensive framework that integrates untargeted metabolomics, genetics, and pathway enrichment analysis to: 1. Identify a broad spectrum of metabolites causally related to Body Mass Index (BMI). 2. Characterize the biological pathways involved in the obesity metabolome, including those represented by unidentified metabolites."
  },
  {
    "objectID": "multi-omics/hsu_2020_32467615.html#study-methods",
    "href": "multi-omics/hsu_2020_32467615.html#study-methods",
    "title": "Integrating untargeted metabolomics, genetically informed causal inference, and pathway enrichment to define the obesity metabolome",
    "section": "Study Methods",
    "text": "Study Methods\nThe authors utilized a multi-stage approach across three large cohorts:\n\nMetabolomic Profiling: Untargeted metabolomic profiling was conducted on samples from the Framingham Heart Study (FHS), generating quantitative signals for both known (identified) and unknown (unidentified) metabolites.\nGenome-Wide Association Study (GWAS): A GWAS was performed to identify metabolite-QTLs (mQTLs)—genetic variants associated with the levels of both known and unknown metabolites. These mQTLs served as the instrumental variables (IVs) for the subsequent MR analysis.\nTwo-Sample Mendelian Randomization (MR):\n\nThe mQTLs identified from the FHS metabolomics GWAS were used as IVs.\nThe exposure was each metabolite (known and unknown).\nThe outcome was BMI, using summary statistics from a large published BMI GWAS.\nMultivariable MR was applied to test for independence among groups of putatively co-regulated metabolites.\n\nPathway Enrichment Analysis: To interpret the role of the unknown metabolites, a novel Pathway Enrichment Analysis method was developed. This method uses the shared genetic architecture (i.e., common mQTLs) between unknown and known metabolites to infer the metabolic pathway of the unknown metabolites."
  },
  {
    "objectID": "multi-omics/hsu_2020_32467615.html#key-results-and-findings",
    "href": "multi-omics/hsu_2020_32467615.html#key-results-and-findings",
    "title": "Integrating untargeted metabolomics, genetically informed causal inference, and pathway enrichment to define the obesity metabolome",
    "section": "Key Results and Findings",
    "text": "Key Results and Findings\n\nCausal Metabolites\n\nThe study identified 23 metabolites (15 known and 8 unknown) that were causally associated with BMI.\nPro-Obesity Metabolites: Metabolites whose higher levels were causally linked to higher BMI included certain amino acid catabolites, medium-chain acylcarnitines, and metabolites related to the carnitine cycle and fatty acid oxidation.\nAnti-Obesity Metabolites: Metabolites whose higher levels were causally linked to lower BMI included glycine, acetylated/formylated amino acids, and several glycerophospholipids.\n\n\n\nDefining the Obesity Metabolome\nThe integrated analysis defined the obesity metabolome as being characterized by a metabolic shift involving: 1. Impaired Amino Acid Catabolism: Specifically, branched-chain amino acid (BCAA) and aromatic amino acid catabolites showed a strong causal link with higher BMI. 2. Dysfunctional Lipid Metabolism: The enrichment analysis linked BMI to pathways of lipid metabolism, particularly those involving glycerophospholipids and the carnitine shuttle.\n\n\nImportance of Unknown Metabolites\nThe inclusion of 8 causally-associated unknown metabolites expanded the definition of the obesity metabolome. The novel pathway enrichment approach successfully mapped these unknown metabolites to relevant metabolic pathways (e.g., lipid and amino acid metabolism) based on their shared genetic control with known metabolites."
  },
  {
    "objectID": "multi-omics/hsu_2020_32467615.html#conclusions-and-significance",
    "href": "multi-omics/hsu_2020_32467615.html#conclusions-and-significance",
    "title": "Integrating untargeted metabolomics, genetically informed causal inference, and pathway enrichment to define the obesity metabolome",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nThe comprehensive framework successfully leveraged the power of untargeted metabolomics in combination with genetically informed causal inference (MR) and a novel pathway enrichment tool. This approach provides a more complete, systems-level view of the metabolic disturbances associated with obesity.\nThe identified causal metabolites and pathways offer promising avenues for targeted therapeutic and diagnostic interventions aimed at treating obesity and its related metabolic diseases."
  },
  {
    "objectID": "multi-omics/mo_2018_30657866.html",
    "href": "multi-omics/mo_2018_30657866.html",
    "title": "A fully Bayesian latent variable model for integrative clustering analysis of multi-type omics data",
    "section": "",
    "text": "PubMed: 28541380 DOI: 10.1093/biostatistics/kxx017 Overview generated by: Gemini 2.5 Flash, 27/11/2025"
  },
  {
    "objectID": "multi-omics/mo_2018_30657866.html#background-and-objective",
    "href": "multi-omics/mo_2018_30657866.html#background-and-objective",
    "title": "A fully Bayesian latent variable model for integrative clustering analysis of multi-type omics data",
    "section": "Background and Objective",
    "text": "Background and Objective\nThe identification of clinically relevant disease subtypes and their corresponding molecular signatures is a central goal in precision medicine, particularly in cancer research. Large-scale projects like The Cancer Genome Atlas (TCGA) generate vast amounts of heterogeneous multi-omics data (e.g., gene expression, DNA methylation, copy number variation). However, standard clustering methods applied to individual omics layers often yield conflicting or unstable results, failing to capture the comprehensive biological signal.\nThis paper introduces a novel statistical framework based on a fully Bayesian latent variable model for integrative clustering analysis of multi-type omics data. The objective is to simultaneously: 1. Identify robust, shared disease subtypes across all omics platforms. 2. Identify the key molecular features (signatures) from each omics platform that characterize these subtypes."
  },
  {
    "objectID": "multi-omics/mo_2018_30657866.html#methods-the-bayesian-integrative-clustering-icluster-framework",
    "href": "multi-omics/mo_2018_30657866.html#methods-the-bayesian-integrative-clustering-icluster-framework",
    "title": "A fully Bayesian latent variable model for integrative clustering analysis of multi-type omics data",
    "section": "Methods: The Bayesian Integrative Clustering (iCluster) Framework",
    "text": "Methods: The Bayesian Integrative Clustering (iCluster) Framework\n\nCore Algorithm: Latent Variable Model\nThe method is an advancement of the iCluster and iClusterPlus frameworks. It assumes that the variation in each omics dataset (\\(\\mathbf{X}_k\\)) for the \\(k\\)-th data type can be explained by a set of shared, hidden (latent) variables (\\(\\mathbf{Z}\\)), plus noise:\n\\[\\mathbf{X}_k = \\mathbf{Z} \\mathbf{W}_k + \\mathbf{E}_k\\]\nwhere \\(\\mathbf{Z}\\) represents the common latent factors (which define the patient clusters) and \\(\\mathbf{W}_k\\) are the omics-specific loading matrices (which define the molecular features).\n\n\nBayesian Implementation and Innovation\nThe key methodological innovations are the implementation of the model using a fully Bayesian approach (via Gibbs sampling) and the incorporation of sparsity priors:\n\nIntegrative Clustering: The latent variables \\(\\mathbf{Z}\\) are clustered using a Dirichlet process prior, which automatically determines the optimal number of clusters (\\(K\\)) that best explains the integrated data structure.\nFeature Selection (Sparsity): Adaptive shrinkage priors are placed on the loading matrices (\\(\\mathbf{W}_k\\)). This is crucial because it drives many of the loadings to zero, effectively performing automatic variable selection and identifying a sparse set of molecular features (genes, loci, etc.) that are most relevant to the cluster assignments. This simultaneously resolves the high-dimensionality problem and provides the subtype-specific molecular signatures.\nHandling Multi-type Data: The Bayesian framework naturally accommodates the different distributional properties of multi-omics data (e.g., continuous expression, count data, binary mutation status)."
  },
  {
    "objectID": "multi-omics/mo_2018_30657866.html#key-results-and-application",
    "href": "multi-omics/mo_2018_30657866.html#key-results-and-application",
    "title": "A fully Bayesian latent variable model for integrative clustering analysis of multi-type omics data",
    "section": "Key Results and Application",
    "text": "Key Results and Application\n\nApplication to Cancer Data\nThe method was applied to three publicly available cancer datasets (Glioblastoma Multiforme, Lung Squamous Cell Carcinoma, and Endometrial Carcinoma) from TCGA, integrating data types such as: * mRNA expression * DNA methylation * Copy number variation (CNV)\n\n\nRobust Subtype Identification\nThe Bayesian integrative clustering approach demonstrated superior performance in identifying biologically and clinically relevant subtypes compared to methods applied to single omics layers or less sophisticated integrative methods. The inferred clusters showed strong agreement with established cancer subtyping schemes and often refined them.\n\n\nSignature Discovery\nThe sparse loading matrices (\\(\\mathbf{W}_k\\)) successfully identified the specific, highly characteristic molecular signatures for each subtype across the different omics layers, providing a clear biological interpretation of the patient groupings."
  },
  {
    "objectID": "multi-omics/mo_2018_30657866.html#conclusions-and-significance",
    "href": "multi-omics/mo_2018_30657866.html#conclusions-and-significance",
    "title": "A fully Bayesian latent variable model for integrative clustering analysis of multi-type omics data",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nThis fully Bayesian latent variable model offers a statistically rigorous and powerful solution for the integrative clustering of multi-omics data . The method’s ability to automatically determine the number of clusters and simultaneously perform sparse feature selection is a major advance. By providing robust disease subtypes and their corresponding minimal molecular signatures, this framework is critical for advancing precision medicine and translational research in complex diseases."
  },
  {
    "objectID": "multi-omics/cantini_2021_33402734.html",
    "href": "multi-omics/cantini_2021_33402734.html",
    "title": "Benchmarking joint multi-omics dimensionality reduction approaches for the study of cancer",
    "section": "",
    "text": "PubMed: 33402734 DOI: 10.1038/s41467-020-20430-7 Overview generated by: Gemini 2.5 Flash, 27/11/2025"
  },
  {
    "objectID": "multi-omics/cantini_2021_33402734.html#background-and-objective",
    "href": "multi-omics/cantini_2021_33402734.html#background-and-objective",
    "title": "Benchmarking joint multi-omics dimensionality reduction approaches for the study of cancer",
    "section": "Background and Objective",
    "text": "Background and Objective\nHigh-dimensional multi-omics data is now standard for understanding complex biological systems like cancer. Joint Dimensionality Reduction (jDR) methods are crucial for the effective integration of these heterogeneous datasets. However, the large number of available jDR methods necessitates a systematic evaluation to provide researchers with reliable guidance on which method to choose for their specific research question.\nThis paper presents a comprehensive benchmark of nine representative joint multi-omics dimensionality reduction approaches to offer practical guidelines for their application, particularly in the study of cancer."
  },
  {
    "objectID": "multi-omics/cantini_2021_33402734.html#methods-systematic-evaluation",
    "href": "multi-omics/cantini_2021_33402734.html#methods-systematic-evaluation",
    "title": "Benchmarking joint multi-omics dimensionality reduction approaches for the study of cancer",
    "section": "Methods: Systematic Evaluation",
    "text": "Methods: Systematic Evaluation\nThe study systematically evaluated nine representative jDR methods (including MCIA, iCluster, and intNMF) across three complementary benchmark scenarios:\n\nGround-Truth Clustering: Assessing the methods’ ability to retrieve known sample clustering patterns from simulated multi-omics datasets.\nClinical Relevance (TCGA): Using The Cancer Genome Atlas (TCGA) cancer data to evaluate how well the methods’ reduced dimensions predict patient survival, clinical annotations, and enrich for known pathways/biological processes.\nSingle-Cell Classification: Assessing performance in the classification of multi-omics single-cell data.\n\nThe authors also created a reproducible code platform named momix (multi-omics mix), implementing the code developed for this benchmark to support users and future comparative studies."
  },
  {
    "objectID": "multi-omics/cantini_2021_33402734.html#key-results-and-conclusions",
    "href": "multi-omics/cantini_2021_33402734.html#key-results-and-conclusions",
    "title": "Benchmarking joint multi-omics dimensionality reduction approaches for the study of cancer",
    "section": "Key Results and Conclusions",
    "text": "Key Results and Conclusions\nThe in-depth comparisons provided clear performance distinctions among the nine methods:\n\nBest Clustering Performer: The intNMF (integrated Non-negative Matrix Factorization) method demonstrated the best performance in retrieving ground-truth clustering from simulated data.\nBest All-Rounder: MCIA (Multiple Co-Inertia Analysis) offered effective and consistent behavior across many different contexts and benchmark criteria, suggesting it is a robust general-purpose tool for integration.\nSignificance: The benchmarking study is a critical resource for the multi-omics community, offering data-driven recommendations for selecting appropriate jDR tools based on the specific research question (e.g., whether the goal is clustering or prediction)."
  },
  {
    "objectID": "multi-omics/ryan_2024_39177104.html",
    "href": "multi-omics/ryan_2024_39177104.html",
    "title": "Multi-Omic Graph Diagnosis (MOGDx): a data integration tool to perform classification tasks for heterogeneous diseases",
    "section": "",
    "text": "PubMed: 39177104 DOI: 10.1093/bioinformatics/btae523 Overview generated by: Gemini 2.5 Flash, 27/11/2025"
  },
  {
    "objectID": "multi-omics/ryan_2024_39177104.html#background-and-objective",
    "href": "multi-omics/ryan_2024_39177104.html#background-and-objective",
    "title": "Multi-Omic Graph Diagnosis (MOGDx): a data integration tool to perform classification tasks for heterogeneous diseases",
    "section": "Background and Objective",
    "text": "Background and Objective\nThe complexity and heterogeneity of human diseases (e.g., in cancer or psychiatric disorders) make precise diagnosis and treatment challenging. Multi-omics data offers the opportunity to redefine these diseases at a more granular, molecular level. However, existing integrative machine learning methods often face limitations in scalability, oversimplification of biological relationships, and effectively handling missing data.\nThis paper introduces Multi-Omic Graph Diagnosis (MOGDx), a flexible data integration tool that leverages Graph Neural Networks (GNNs) to perform robust classification tasks for heterogeneous diseases by capturing complex, non-linear relationships across multiple omics layers."
  },
  {
    "objectID": "multi-omics/ryan_2024_39177104.html#methods-the-mogdx-framework",
    "href": "multi-omics/ryan_2024_39177104.html#methods-the-mogdx-framework",
    "title": "Multi-Omic Graph Diagnosis (MOGDx): a data integration tool to perform classification tasks for heterogeneous diseases",
    "section": "Methods: The MOGDx Framework",
    "text": "Methods: The MOGDx Framework\nMOGDx is a supervised learning framework that integrates diverse omics data types (like gene expression, DNA methylation) by modeling the study cohort as a graph.\n\nGraph Neural Network Integration\n\nNodes and Features: Each patient or sample is represented as a node in the graph. The various omics data (e.g., expression levels) are used as the initial feature vectors for each patient node.\nEdges (Similarity): The edges (connections) between patient nodes are determined by calculating a measure of molecular similarity across all omics data types (similar to Similarity Network Fusion).\nGraph Convolutional Network (GCN): The core of MOGDx is a GCN. This GNN propagates information across the patient-similarity graph, enabling the model to learn complex, non-linear dependencies both within and between the omics layers. This process captures subtle patterns of heterogeneity shared across modalities, which is then used for the classification task (e.g., predicting disease subtype).\n\n\n\nData Robustness\nThe GNN architecture allows MOGDx to be robust in handling missing data, which is a critical feature for real-world multi-omics cohorts where not all measurements are available for every patient."
  },
  {
    "objectID": "multi-omics/ryan_2024_39177104.html#key-results-and-findings",
    "href": "multi-omics/ryan_2024_39177104.html#key-results-and-findings",
    "title": "Multi-Omic Graph Diagnosis (MOGDx): a data integration tool to perform classification tasks for heterogeneous diseases",
    "section": "Key Results and Findings",
    "text": "Key Results and Findings\nMOGDx was applied to several public cancer cohorts (including TCGA data for Glioblastoma, Lung Adenocarcinoma, and Lung Squamous Cell Carcinoma).\n\nSuperior Classification: MOGDx demonstrated superior classification accuracy compared to leading non-GNN multi-omics integration methods and single-omics models, validating the strength of the graph-based approach in learning complex patient relationships.\nBiomarker Identification: The framework facilitates the identification of the specific molecular features (biomarkers) that are most critical in distinguishing the disease classes, by analyzing the feature weights learned in the GNN layers.\nMissing Data Performance: The model maintained high classification performance even when substantial portions of the omics data were missing, confirming its robustness for real-world applications."
  },
  {
    "objectID": "multi-omics/ryan_2024_39177104.html#conclusions-and-significance",
    "href": "multi-omics/ryan_2024_39177104.html#conclusions-and-significance",
    "title": "Multi-Omic Graph Diagnosis (MOGDx): a data integration tool to perform classification tasks for heterogeneous diseases",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nMOGDx offers a significant methodological advancement for multi-omics data integration using a flexible and robust Graph Neural Network approach. By effectively modeling patient relationships and learning complex cross-omics patterns, MOGDx provides a powerful tool for improving the classification accuracy of heterogeneous diseases and accelerating the discovery of precision biomarkers."
  },
  {
    "objectID": "multi-omics/singh_2019_30657866.html",
    "href": "multi-omics/singh_2019_30657866.html",
    "title": "DIABLO: an integrative approach for identifying key molecular drivers from multi-omics assays",
    "section": "",
    "text": "PubMed: 30657866 DOI: 10.1093/bioinformatics/bty1054 Overview generated by: Gemini 2.5 Flash, 27/11/2025"
  },
  {
    "objectID": "multi-omics/singh_2019_30657866.html#background-and-objective",
    "href": "multi-omics/singh_2019_30657866.html#background-and-objective",
    "title": "DIABLO: an integrative approach for identifying key molecular drivers from multi-omics assays",
    "section": "Background and Objective",
    "text": "Background and Objective\nThe challenge in analyzing modern multi-omics data is the high dimensionality of individual datasets and the difficulty in integrating these diverse data types (e.g., transcriptomics, proteomics, metabolomics) while accounting for the shared biological signal across them. Current methods often integrate data post-analysis, neglecting the opportunity to jointly identify features that drive biological differences.\nThis paper introduces DIABLO (Data Integration Analysis for Biomarker discovery using Latent variable approaches for Omics datasets), a novel computational method designed to: 1. Perform supervised integration of multiple heterogeneous omics datasets. 2. Simultaneously identify a minimal set of key molecular features (biomarkers) that are highly correlated across omics types and maximally associated with a specific outcome variable (e.g., disease status, clinical subtype)."
  },
  {
    "objectID": "multi-omics/singh_2019_30657866.html#methods-the-diablo-framework",
    "href": "multi-omics/singh_2019_30657866.html#methods-the-diablo-framework",
    "title": "DIABLO: an integrative approach for identifying key molecular drivers from multi-omics assays",
    "section": "Methods: The DIABLO Framework",
    "text": "Methods: The DIABLO Framework\n\nCore Algorithm\nDIABLO is based on Partial Least Squares (PLS), a method for simultaneous dimension reduction and feature selection. Specifically, it uses a generalized form of PLS called multi-block PLS (MB-PLS) or Generalised Canonical Correlation Analysis (GCCA).\n\n\nSupervised Integration\nUnlike unsupervised integration methods, DIABLO is supervised—it incorporates an outcome variable (e.g., healthy vs. disease) into the model. The method identifies latent components (similar to principal components) that maximize the covariance between: 1. The multi-omics datasets. 2. The multi-omics datasets and the outcome variable.\n\n\nFeature Selection and Biomarker Discovery\nDIABLO employs a sparse penalty (using an \\(L_1\\) penalty, similar to LASSO regression) within its iterative algorithm. This forces the latent components to be linear combinations of only a small number of features. This feature selection step is critical for: * Dimension Reduction: Reducing the number of irrelevant features. * Biomarker Identification: Pinpointing the most important “key molecular drivers” that explain the variation in the biological outcome across the different omics layers."
  },
  {
    "objectID": "multi-omics/singh_2019_30657866.html#key-results-and-application",
    "href": "multi-omics/singh_2019_30657866.html#key-results-and-application",
    "title": "DIABLO: an integrative approach for identifying key molecular drivers from multi-omics assays",
    "section": "Key Results and Application",
    "text": "Key Results and Application\n\nPerformance\nThe authors demonstrated that DIABLO outperformed several state-of-the-art multi-omics integration and classification methods (e.g., iClusterPlus, multi-kernel learning) in terms of: * Classification Accuracy: Achieving higher predictive performance for distinguishing between sample groups. * Biological Relevance: Identifying smaller, more biologically coherent subsets of features that were consistently selected across different omics blocks.\n\n\nCase Study: Breast Cancer\nDIABLO was applied to a multi-omics breast cancer dataset (transcriptomics, metabolomics, miRNA) to distinguish between clinical subtypes. * Integrated Signatures: DIABLO identified a signature of features that were highly correlated across the omics layers, including specific genes (mRNA), microRNAs, and metabolites. * Key Drivers: The method pinpointed known and novel molecular drivers (e.g., genes and pathways related to cell cycle and proliferation) whose concerted changes across the different omics layers were responsible for the differences between cancer subtypes."
  },
  {
    "objectID": "multi-omics/singh_2019_30657866.html#conclusions-and-significance",
    "href": "multi-omics/singh_2019_30657866.html#conclusions-and-significance",
    "title": "DIABLO: an integrative approach for identifying key molecular drivers from multi-omics assays",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nDIABLO is a powerful, flexible, and scalable tool for supervised integration of multi-omics data. Its ability to simultaneously perform dimension reduction, feature selection, and association with a clinical outcome makes it particularly well-suited for biomarker discovery.\nBy identifying small, highly relevant, and correlated sets of features across different molecular layers, DIABLO provides valuable insights into the key molecular drivers underlying complex biological states or disease phenotypes, aiding in the transition toward personalized medicine."
  },
  {
    "objectID": "multi-omics/dugourd_2021_33502086.html",
    "href": "multi-omics/dugourd_2021_33502086.html",
    "title": "Causal integration of multi-omics data with prior knowledge to generate mechanistic hypotheses",
    "section": "",
    "text": "PubMed: 33502086 DOI: 10.15252/msb.20209703 Overview generated by: Gemini 2.5 Flash, 27/11/2025"
  },
  {
    "objectID": "multi-omics/dugourd_2021_33502086.html#background-and-objective",
    "href": "multi-omics/dugourd_2021_33502086.html#background-and-objective",
    "title": "Causal integration of multi-omics data with prior knowledge to generate mechanistic hypotheses",
    "section": "Background and Objective",
    "text": "Background and Objective\nThe rise of multi-omics technologies provides vast datasets that capture different layers of molecular information (e.g., phosphorylation, transcription, metabolism). However, methods for integrating these diverse data types to systematically extract mechanistic hypotheses—specifically, how a change in one molecular layer causally affects another—are limited.\nThis paper introduces COSMOS (Causal Oriented Search of Multi-Omics Space), a novel computational method designed to: 1. Integrate quantitative data from three omics layers: phosphoproteomics, transcriptomics, and metabolomics. 2. Combine this data with extensive prior knowledge contained within signaling, metabolic, and gene regulatory networks. 3. Infer the causal relationships between molecular activities across these layers to generate mechanistic hypotheses."
  },
  {
    "objectID": "multi-omics/dugourd_2021_33502086.html#methods-the-cosmos-framework",
    "href": "multi-omics/dugourd_2021_33502086.html#methods-the-cosmos-framework",
    "title": "Causal integration of multi-omics data with prior knowledge to generate mechanistic hypotheses",
    "section": "Methods: The COSMOS Framework",
    "text": "Methods: The COSMOS Framework\n\nCausal Integration Principle\nCOSMOS’s core strength lies in its use of a large, curated prior knowledge network (PKN) that encompasses relationships between various molecular entities, including transcription factors, kinases, genes, and metabolites. The method works by performing two key causal steps:\n\nActivity Inference: COSMOS first calculates the estimated activity of key molecular regulators from the measured omics data:\n\nKinase Activity (from phosphoproteomics)\nTranscription Factor (TF) Activity (from transcriptomics)\nEnzyme/Pathway Activity (from metabolomics)\n\nCausal Scoring and Hypothesis Generation: The inferred activity changes are then mapped onto the PKN. COSMOS calculates a causal score for every potential regulatory link between a regulator (e.g., a kinase) and its target (e.g., a TF or a metabolite) by evaluating whether the measured change in the regulator’s activity is consistent with the measured change in its target’s activity, considering the known network topology.\n\n\n\nApplication: Renal Cell Carcinoma (RCC)\nCOSMOS was applied to multi-omics data from a drug screen on Clear Cell Renal Cell Carcinoma (ccRCC) cells treated with various anti-cancer compounds. This allowed the authors to investigate how drug perturbations causally rewire the signaling, gene regulation, and metabolic networks of cancer cells."
  },
  {
    "objectID": "multi-omics/dugourd_2021_33502086.html#key-results-and-mechanistic-hypotheses",
    "href": "multi-omics/dugourd_2021_33502086.html#key-results-and-mechanistic-hypotheses",
    "title": "Causal integration of multi-omics data with prior knowledge to generate mechanistic hypotheses",
    "section": "Key Results and Mechanistic Hypotheses",
    "text": "Key Results and Mechanistic Hypotheses\n\nNetwork Rewiring in ccRCC\nCOSMOS successfully inferred activity changes in known regulatory pathways, demonstrating the effect of drug perturbations on the signaling, transcriptional, and metabolic machinery of the cancer cells.\n\n\nCausal Hypotheses Generated\nThe method generated specific, testable mechanistic hypotheses that connect the omics layers: 1. Signaling to Transcription (Kinase → TF): COSMOS identified the TSSK4 kinase as a causal regulator of the ZHX2 transcription factor following treatment with a CDK inhibitor. This suggested a specific kinase-TF cascade that could be important for therapeutic response. 2. Transcription to Metabolism (TF → Metabolite): The method also revealed a causal link from the ZHX2 TF to the regulation of GAPDH enzyme activity (a key player in glycolysis), which in turn regulated specific metabolites like pyruvate. This closed the loop, linking signaling through transcription to metabolic output. 3. Discovery of Novel Kinases: Through its integration, COSMOS implicated several previously uncharacterized kinases (e.g., CDK11) as potential drivers of the transcriptional response to drug treatment, suggesting novel targets for further investigation."
  },
  {
    "objectID": "multi-omics/dugourd_2021_33502086.html#conclusions-and-significance",
    "href": "multi-omics/dugourd_2021_33502086.html#conclusions-and-significance",
    "title": "Causal integration of multi-omics data with prior knowledge to generate mechanistic hypotheses",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nCOSMOS represents a significant advance in systems biology by offering a robust, transparent, and biologically constrained approach to causal integration of multi-omics data.\nBy leveraging extensive prior knowledge, it moves beyond simple association to generate specific, directional, and testable mechanistic hypotheses that bridge the gap between different omics layers. This capability is critical for understanding complex diseases like cancer and for accelerating the discovery of novel therapeutic targets and biomarkers."
  },
  {
    "objectID": "statistics/hernan_2025_39494894.html",
    "href": "statistics/hernan_2025_39494894.html",
    "title": "A Structural Description of Biases That Generate Immortal Time",
    "section": "",
    "text": "PubMed: 39494894 DOI: 10.1097/EDE.0000000000001808 Overview generated by: Gemini 2.5 Flash, 26/11/2025"
  },
  {
    "objectID": "statistics/hernan_2025_39494894.html#key-concepts-and-goal",
    "href": "statistics/hernan_2025_39494894.html#key-concepts-and-goal",
    "title": "A Structural Description of Biases That Generate Immortal Time",
    "section": "Key Concepts and Goal",
    "text": "Key Concepts and Goal\nImmortal time is defined as an event-free period included in a survival analysis during which a person, by definition, cannot experience the event of interest. The authors argue that the term “immortal time bias” is misleading because the bias is not generated by the time itself, but by the underlying selection or misclassification that creates the immortal time. The primary goal of the paper is to review the two mechanisms that produce immortal time and to propose causal diagrams to represent them.\nThe ultimate prevention strategy is Target Trial Emulation, which explicitly specifies eligibility and assignment to the treatment strategies and synchronizes them at the start of follow-up. This alignment prevents the selection and misclassification that lead to immortal time."
  },
  {
    "objectID": "statistics/hernan_2025_39494894.html#mechanism-1-immortal-time-due-to-selection",
    "href": "statistics/hernan_2025_39494894.html#mechanism-1-immortal-time-due-to-selection",
    "title": "A Structural Description of Biases That Generate Immortal Time",
    "section": "Mechanism 1: Immortal Time Due to Selection",
    "text": "Mechanism 1: Immortal Time Due to Selection\nThis mechanism arises when an eligibility criterion is (incorrectly) applied after the start of follow-up (i.e., after treatment assignment).\n\nStudy Design Failure\nThe issue is created when researchers restrict the analysis to individuals who survived or completed a certain period of follow-up after the initial treatment assignment. For example, if follow-up data for the first 3 months are accidentally deleted, or if an observational analysis is restricted to individuals who have completed 3 months of follow-up, the resulting dataset only contains survivors, creating an “immortal” period where all included individuals survived.\n\n\nResulting Bias\n\nThe selection of surviving individuals results in selection bias due to a differential exclusion of the individuals most susceptible to the outcome in each treatment group.\nIf the follow-up is started at the time of treatment assignment but the immortal period is included, the resulting selection bias is often called “immortal time bias.”\nIf the follow-up is started after the immortal period (e.g., using a landmark approach), the resulting bias is sometimes referred to as “prevalent user bias.”\n\n\n\nSolution\nTo prevent this bias, researchers must ensure that all eligibility criteria are defined at time zero so that no selection occurs after treatment assignment. This is naturally achieved by explicitly emulating a target trial based on data for all eligible individuals from the time of treatment assignment."
  },
  {
    "objectID": "statistics/hernan_2025_39494894.html#mechanism-2-immortal-time-due-to-misclassification",
    "href": "statistics/hernan_2025_39494894.html#mechanism-2-immortal-time-due-to-misclassification",
    "title": "A Structural Description of Biases That Generate Immortal Time",
    "section": "Mechanism 2: Immortal Time Due to Misclassification",
    "text": "Mechanism 2: Immortal Time Due to Misclassification\nThis mechanism occurs when individuals are misclassified into a treatment group that differs from the one they were assigned to, typically because the treatment strategies under study are not distinguishable at time zero.\n\nStudy Design Failure\nThis arises when a treatment strategy includes a grace period or a waiting period (e.g., “start treatment within 3 months” vs. “never start treatment”). If the assignment indicator (\\(Z\\)) is deleted (or unknown in observational data), researchers might reconstruct an assignment (\\(Z^*\\)) based on whether the individual actually received treatment during the grace period. This forces individuals who were assigned to treatment but died before starting it to be classified into the “no treatment” group.\n\n\nResulting Bias\n\nThis reconstruction uses information on the outcome (survival until treatment) to define the assignment variable \\(Z^*\\), which is a violation of the rule that assignment at time zero must not depend on future outcome values.\nThe resulting error is outcome-dependent misclassification, which makes the treated group look artificially “immortal” during the grace period (as anyone who died before treatment is excluded from the \\(Z^*\\) group).\n\n\n\nSolutions\n\nChange the Causal Question: Re-define the comparison to strategies that are distinguishable at time zero (e.g., comparing immediate treatment to no treatment today).\nCloning Followed by Censoring: Create multiple “clones” of each individual for every treatment strategy compatible with their data at baseline. Each clone is censored if they deviate from the assigned strategy. This approach requires inverse-probability weighting to adjust for the induced selection bias.\nPlug-in G-Formula: A complex estimation approach requiring the modeling of the joint distribution or iterated conditional expectation of time-varying treatment, outcome, and confounders."
  },
  {
    "objectID": "statistics/hernan_2025_39494894.html#cautionary-note-on-alternative-methods",
    "href": "statistics/hernan_2025_39494894.html#cautionary-note-on-alternative-methods",
    "title": "A Structural Description of Biases That Generate Immortal Time",
    "section": "Cautionary Note on Alternative Methods",
    "text": "Cautionary Note on Alternative Methods\nWhile landmark analysis and person-time analysis can avoid immortal time, they do not explicitly specify the target trial and do not eliminate the fundamental misalignment of eligibility and assignment. These methods can still be susceptible to bias (like selection bias for landmark analysis) or rely on implausible assumptions (like the constant hazard ratio assumption for person-time analysis)."
  },
  {
    "objectID": "statistics/debertin_2024_38860706.html",
    "href": "statistics/debertin_2024_38860706.html",
    "title": "Synthesizing Subject-matter Expertise for Variable Selection in Causal Effect Estimation: A Case Study",
    "section": "",
    "text": "PubMed: 38860706 DOI: 10.1097/EDE.0000000000001758 Overview generated by: Gemini 2.5 Flash, 26/11/2025"
  },
  {
    "objectID": "statistics/debertin_2024_38860706.html#background-and-purpose",
    "href": "statistics/debertin_2024_38860706.html#background-and-purpose",
    "title": "Synthesizing Subject-matter Expertise for Variable Selection in Causal Effect Estimation: A Case Study",
    "section": "Background and Purpose",
    "text": "Background and Purpose\nDirected Acyclic Graphs (DAGs) are a crucial theoretical tool for covariate selection in causal effect estimation, as they allow researchers to identify minimal adjustment sets that control for confounding. However, there is limited empirical research on the practical creation of these graphs. This paper assesses different approaches to DAG construction using data from the Coronary Drug Project (CDP) trial. The focus is on estimating the effect of placebo adherence on mortality, a relationship where the true causal effect is assumed to be zero (as a placebo cannot cause mortality), providing a robust benchmark for comparing methods."
  },
  {
    "objectID": "statistics/debertin_2024_38860706.html#study-methods-and-design",
    "href": "statistics/debertin_2024_38860706.html#study-methods-and-design",
    "title": "Synthesizing Subject-matter Expertise for Variable Selection in Causal Effect Estimation: A Case Study",
    "section": "Study Methods and Design",
    "text": "Study Methods and Design\nThe authors created multiple DAGs based on various strategies for identifying and linking variables. For each DAG, the corresponding minimal adjustment sets were derived to control for confounding variables. These adjustment sets were then applied to the CDP data under two primary modeling strategies:\n\nBaseline-only Adjustment: Estimating the cumulative effect of adherence on mortality by adjusting only for baseline covariate values in a standard regression.\nTime-Varying Adjustment: Estimating the effect by adjusting for time-varying covariates of adherence using Inverse Probability Weighting (IPW)."
  },
  {
    "objectID": "statistics/debertin_2024_38860706.html#empirical-results",
    "href": "statistics/debertin_2024_38860706.html#empirical-results",
    "title": "Synthesizing Subject-matter Expertise for Variable Selection in Causal Effect Estimation: A Case Study",
    "section": "Empirical Results",
    "text": "Empirical Results\n\nEffect of Nonconfounding Prognostic Factors\nWhen estimating the cumulative effect using only baseline covariates, the results showed that the specific choice of covariates had minimal effect on the (expectedly biased) point estimates. However, including nonconfounding prognostic factors (variables that predict the outcome but not the exposure) led to smaller variance estimates. This finding provides empirical support for the theoretical advice that including prognostic factors increases the efficiency of the causal estimate without introducing bias.\n\n\nEffect of Exposure Predictors\nConversely, when using IPW to adjust for time-varying covariates, adjustment sets that included exposure predictors that were not prognostic factors were shown to result in less bias control.\n\n\nPerformance of DAG Creation Strategies\nOverall, the DAGs that were explicitly created by focusing subject-matter expertise on the identification of potential outcome prognostic factors performed best, particularly in the more complex time-varying covariate scenario using IPW."
  },
  {
    "objectID": "statistics/debertin_2024_38860706.html#conclusions-and-recommendations",
    "href": "statistics/debertin_2024_38860706.html#conclusions-and-recommendations",
    "title": "Synthesizing Subject-matter Expertise for Variable Selection in Causal Effect Estimation: A Case Study",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\n\nConfirmation of Theory\nThe study empirically confirms key theoretical advice regarding causal variable selection:\n\nInclude Prognostic Factors: Identifying and including covariates that are strong predictors of the outcome (prognostic factors) but not predictors of the exposure is highly recommended to reduce variance and increase statistical power.\nCaution with Exposure Predictors: Covariates that are strong predictors of the exposure but not the outcome may interfere with bias control and should be considered with caution.\n\n\n\nPractical Recommendation\nThe paper recommends that researchers and subject-matter experts begin the hand-creation of DAGs with a systematic effort to identify and include all potential outcome prognostic factors, as this strategy proved most effective in constructing a robust adjustment set for causal effect estimation."
  },
  {
    "objectID": "statistics/smith_2018_dyw314.html",
    "href": "statistics/smith_2018_dyw314.html",
    "title": "Step away from stepwise",
    "section": "",
    "text": "PubMed: Not Indexed (Journal of Big Data) DOI: 10.1186/s40537-018-0143-6 Overview generated by: Gemini 2.5 Flash, 26/11/2025"
  },
  {
    "objectID": "statistics/smith_2018_dyw314.html#key-findings-the-dangers-of-stepwise-regression-in-the-era-of-big-data",
    "href": "statistics/smith_2018_dyw314.html#key-findings-the-dangers-of-stepwise-regression-in-the-era-of-big-data",
    "title": "Step away from stepwise",
    "section": "Key Findings: The Dangers of Stepwise Regression in the Era of Big Data",
    "text": "Key Findings: The Dangers of Stepwise Regression in the Era of Big Data\nThis short report by Gary Smith critiques the continued use of stepwise regression, a popular but flawed variable selection method, especially in the context of Big Data. The central argument is that stepwise procedures are fundamentally unable to distinguish between genuine explanatory variables and nuisance variables (spurious correlations), a problem that is dramatically exacerbated when the pool of potential predictors is large.\n\nThe Fundamental Flaws of Stepwise Regression\nStepwise regression (which includes forward selection, backward elimination, and bidirectional methods) uses an arbitrary threshold of statistical significance (p-value) to automate the inclusion or exclusion of variables in a multiple-regression model. The author identifies three major, interconnected problems with this approach:\n\nSelection of Nuisance Variables: By relying solely on statistical significance, stepwise procedures frequently select nuisance variables that happen to be coincidentally significant in the in-sample data. Since these variables have no true causal effect, they are useless for prediction with fresh data (out-of-sample).\nExclusion of True Explanatory Variables: Conversely, genuine explanatory variables with causal effects may be incorrectly excluded because they happen not to be statistically significant in the particular sample analyzed.\nSevere Out-of-Sample Failure: The resulting model, while often providing an excellent fit to the estimation data (high \\(R^2\\) due to including significant noise variables), does poorly out-of-sample. The selection of irrelevant variables provides a false confidence in the estimated model because of the high t-values and the boost to the in-sample \\(R^2\\). ### Big Data Exacerbates the Problem\n\nThe article specifically addresses the belief held by some “Big-Data researchers” that the larger the number of possible explanatory variables, the more useful stepwise regression becomes.\n\nIncreased Chance of Spurious Correlation: In reality, the efficacy of stepwise regression is less effective the larger the number of potential explanatory variables. The sheer number of variables in Big Data increases the probability of finding highly significant but spurious correlations purely by chance.\nWorsening Out-of-Sample Fit: As the number of candidate variables increases, the in-sample fit improves, but the out-of-sample fit deteriorates, causing the ratio of the out-of-sample errors to the in-sample errors to “balloon”.\n\n\n\nConclusion\nThe paper concludes that stepwise regression does not offer a solution to the challenge of too many explanatory variables in the Big Data era; rather, Big Data exacerbates the failings of stepwise regression. The focus should instead be on methods that prioritize predictive accuracy and robust out-of-sample validation."
  },
  {
    "objectID": "statistics/sterne_2001_11159626.html",
    "href": "statistics/sterne_2001_11159626.html",
    "title": "Sifting the evidence—what’s wrong with significance tests?",
    "section": "",
    "text": "PubMed: 11159626 DOI: 10.1136/bmj.322.7280.226 Overview generated by: Gemini 2.5 Flash, 26/11/2025"
  },
  {
    "objectID": "statistics/sterne_2001_11159626.html#key-findings-the-misuse-and-limitations-of-statistical-significance",
    "href": "statistics/sterne_2001_11159626.html#key-findings-the-misuse-and-limitations-of-statistical-significance",
    "title": "Sifting the evidence—what’s wrong with significance tests?",
    "section": "Key Findings: The Misuse and Limitations of Statistical Significance",
    "text": "Key Findings: The Misuse and Limitations of Statistical Significance\nThis highly influential article critiques the over-reliance on statistical significance testing (p-values) in medical and epidemiological research, arguing that this practice fundamentally distorts the scientific literature and leads to public skepticism.\n\nThe Central Problem: Publication Bias\nThe primary issue is the medical literature’s strong tendency to accentuate the positive, leading to publication bias (also known as the “file drawer effect”):\n\nSkewed Reporting: Studies with positive outcomes (those achieving statistical significance, typically P&lt;0.05) are far more likely to be published than those with null results (non-significant findings).\nIncreased Chance Findings: This creates a system where a host of purely chance findings are published and subsequently mistaken for real biological or clinical effects. The authors note that, by conventional reasoning, examining 20 associations will produce one “significant at P=0.05” result by chance alone.\nErosion of Trust: The proliferation of inconsistent, purely chance findings contributes significantly to scepticism about medical research and epidemiological studies among the public and practitioners.\n\n\n\nShifting Focus: From P-value to Effect Magnitude\nThe authors argue that focusing solely on whether a P-value crosses the 0.05 threshold misses the point of much medical research, particularly in observational and interventional studies:\n\nNull Hypothesis Relevance: In many epidemiological studies and randomized controlled trials, there is often little reason to expect the true effect to be exactly null. The key issue is not consistency with a strict null hypothesis.\nThe Real Questions: Instead of asking if an effect exists (P-value), researchers should focus on:\n\nWhether the direction of the effect has been reasonably and firmly established.\nWhether the magnitude of the effect is such that it is of public health or clinical importance. A small, statistically significant effect may be clinically irrelevant. A large, non-significant effect may warrant further study.\n\n\n\n\nHistorical Context\nThe critique is contextualized by referencing the founder of statistical significance testing, Ronald Fisher, and the early criticism he received from his colleague F. Yates, who noted that Fisher’s emphasis on significance testing was problematic. The paper implicitly advocates for the use of Confidence Intervals (CIs), which convey both the direction and the magnitude of the effect along with the precision of the estimate."
  },
  {
    "objectID": "statistics/altman_2006_16675816.html",
    "href": "statistics/altman_2006_16675816.html",
    "title": "The cost of dichotomising continuous variables",
    "section": "",
    "text": "PubMed: 16675816 DOI: 10.1136/bmj.332.7549.1080 Overview generated by: Gemini 2.5 Flash, 26/11/2025"
  },
  {
    "objectID": "statistics/altman_2006_16675816.html#key-finding-dichotomization-of-continuous-variables-is-statistically-detrimental",
    "href": "statistics/altman_2006_16675816.html#key-finding-dichotomization-of-continuous-variables-is-statistically-detrimental",
    "title": "The cost of dichotomising continuous variables",
    "section": "Key Finding: Dichotomization of Continuous Variables is Statistically Detrimental",
    "text": "Key Finding: Dichotomization of Continuous Variables is Statistically Detrimental\nThis article critiques the common practice in clinical research of converting continuous variables (e.g., blood pressure, weight, cholesterol) into binary categories (dichotomization, e.g., “hypertensive” or “not hypertensive”). The authors argue that while this practice is useful for clinical decision-making and data presentation, it is unnecessary for statistical analysis and introduces several serious, avoidable drawbacks."
  },
  {
    "objectID": "statistics/altman_2006_16675816.html#study-design-and-methods",
    "href": "statistics/altman_2006_16675816.html#study-design-and-methods",
    "title": "The cost of dichotomising continuous variables",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\nThis paper is a Statistics Note (a commentary/review) that uses theoretical arguments and a review of existing statistical literature to demonstrate the flaws of dichotomization.\nThe core argument is based on quantifying the statistical and inferential costs associated with replacing precise continuous data with a simple binary indicator (0 or 1). The analysis focuses on the detrimental impact on statistical power and Type I error control in subsequent analyses, particularly in regression modeling."
  },
  {
    "objectID": "statistics/altman_2006_16675816.html#results-and-major-drawbacks",
    "href": "statistics/altman_2006_16675816.html#results-and-major-drawbacks",
    "title": "The cost of dichotomising continuous variables",
    "section": "Results and Major Drawbacks",
    "text": "Results and Major Drawbacks\nThe authors identify four main statistical costs associated with dichotomization:\n\n1. Substantial Loss of Statistical Power\nThe primary statistical consequence is a reduction in power to detect a true association or effect. By converting a continuous measure into a binary one, researchers discard a significant amount of information (i.e., the magnitude of a value relative to others). This loss of information is equivalent to conducting a study with a much smaller sample size, making it harder to achieve statistical significance for a genuine effect.\n\n\n2. Inflation of the Type I Error Rate\nWhen a confounding variable is dichotomized in a multivariable model (e.g., logistic regression), it often fails to adequately control for the confounding effect across the full range of the variable. This residual confounding can lead to a substantial inflation of the Type I error rate for other variables in the model, increasing the risk of false-positive findings.\n\n\n3. Arbitrary and Unstable Cut-points\nThe choice of the cut-point used to divide the continuous variable is frequently arbitrary and lacks strong biological justification. Furthermore, attempts to find an “optimal” cut-point based on the observed data are statistically dangerous, as this practice can lead to biased effect estimates (overestimation) and a loss of validity when generalizing results to new data.\n\n\n4. Misleading Effect Estimates\nDichotomization assumes a simple step-function relationship between the variable and the outcome. This ignores the detailed variation within each group and can entirely misrepresent a true biological relationship, especially if that relationship is non-linear across the continuous scale. The resulting effect estimate only represents the difference between the mean values of the two resulting groups, masking the true dose-response curve."
  },
  {
    "objectID": "statistics/altman_2006_16675816.html#conclusion-and-recommendations",
    "href": "statistics/altman_2006_16675816.html#conclusion-and-recommendations",
    "title": "The cost of dichotomising continuous variables",
    "section": "Conclusion and Recommendations",
    "text": "Conclusion and Recommendations\nThe authors conclude that dichotomization sacrifices statistical rigor for an unnecessary simplicity in the analysis stage.\nThey strongly recommend that continuous variables should be analyzed on their original continuous scale in statistical models (e.g., as continuous predictors in regression models). If the relationship is suspected to be non-linear, more appropriate methods such as fractional polynomials or splines should be used instead of categorization."
  },
  {
    "objectID": "statistics/dewalsche_2025_40918066.html",
    "href": "statistics/dewalsche_2025_40918066.html",
    "title": "Large-scale composite hypothesis testing procedure for omics data analyses",
    "section": "",
    "text": "PubMed: 40918066\nDOI: 10.1093/nargab/lqaf118\nOverview generated by: Claude Sonnet 4.5, 25/11/2025"
  },
  {
    "objectID": "statistics/dewalsche_2025_40918066.html#key-findings",
    "href": "statistics/dewalsche_2025_40918066.html#key-findings",
    "title": "Large-scale composite hypothesis testing procedure for omics data analyses",
    "section": "Key Findings",
    "text": "Key Findings\nThis study introduces qch_copula, a novel composite hypothesis testing (CHT) method for analyzing multiple traits or omics levels simultaneously, addressing key limitations in existing approaches for large-scale genomic studies.\n\nMain Discoveries\n\nNovel P-value derivation: First method to provide rigorously defined P-values directly from mixture model approaches for composite hypothesis testing\nSuperior performance: qch_copula effectively controls Type I error rates while maintaining higher detection power compared to eight state-of-the-art methods (DACT, HDMT, PLACO, adaFilter, IMIX, c-csmGmm, Primo, qch)\nScalability breakthrough: Memory-efficient EM algorithm reduces storage from O(n × 2^Q) to O(n + 2^Q), enabling analysis of up to 20 traits and 105-106 markers\nDependency modeling: Explicitly accounts for correlations between traits/omics levels through copula functions, improving false positive control"
  },
  {
    "objectID": "statistics/dewalsche_2025_40918066.html#study-design",
    "href": "statistics/dewalsche_2025_40918066.html#study-design",
    "title": "Large-scale composite hypothesis testing procedure for omics data analyses",
    "section": "Study Design",
    "text": "Study Design\n\nMethodological Framework\nThe method addresses testing composite null hypotheses of the form H₀: “a marker/gene has effects in at most q̃ - 1 conditions” versus H₁: “effects in at least q̃ conditions.”\nKey components: - Mixture model with 2^Q components (one per configuration) - Gaussian copula to capture dependencies between conditions - Nonparametric estimation of alternative distributions - Two-step inference: marginal distributions, then proportions and copula parameters\n\n\nModel Specification\nFor Q conditions, z-scores (negative probit transforms of P-values) follow:\n\\[Z_i \\sim \\sum_{c \\in C} w_c \\psi_c\\]\nwhere each component ψ_c combines: - Univariate marginal distributions F^q_0 (null) and F^q_1 (alternative) - Copula function C_θ describing dependencies\nPosterior-based P-value:\n\\[\\text{pval}(z) = \\frac{1}{n\\hat{W}_0} \\sum_{j=1}^n \\mathbb{1}_{\\{\\hat{\\tau}_j &gt; \\hat{\\tau}_i\\}} (1 - \\hat{\\tau}_j)\\]\nwhere τ represents the posterior probability of belonging to alternative configurations."
  },
  {
    "objectID": "statistics/dewalsche_2025_40918066.html#major-results",
    "href": "statistics/dewalsche_2025_40918066.html#major-results",
    "title": "Large-scale composite hypothesis testing procedure for omics data analyses",
    "section": "Major Results",
    "text": "Major Results\n\nSimulation Study Design\nEvaluated across 24 settings varying: - Number of conditions: Q = 2, 8, 16 - Correlation levels: ρ = 0, 0.3, 0.5, 0.7 - Scenarios: sparse (≥90% null) vs. dense (50% alternative) - Sample size: n = 10^5 markers\n\n\nType I Error Control (Q = 2)\nIndependent traits (ρ = 0): - Most methods controlled FDR at nominal 5% level - DACT-JC showed substantial inflation (~0.25) - Minor deviations for PLACO and c-csmGmm (~0.08)\nCorrelated traits (ρ = 0.3): - Only DACT_Efron and qch_copula maintained proper FDR control - qch showed severe inflation (FDR = 0.256 sparse, 0.145 dense) - HDMT, PLACO, IMIX, c-csmGmm all exceeded nominal level\nHigher correlations (ρ = 0.5, 0.7): - qch_copula consistently controlled FDR near 0.05 - Other methods (except DACT_Efron) showed increased inflation\n\n\nType I Error Control (Q = 8, 16)\nQ = 8 with ρ = 0.3: - All three scalable methods (Primo, adaFilter, qch_copula) controlled FDR - Primo and adaFilter were conservative (FDR &lt; 0.02 in most cases) - qch_copula maintained FDR close to nominal level\nQ = 16 with ρ = 0.3: - qch_copula: slight inflation in sparse scenario (FDR ≤ 0.08) - adaFilter: comparable performance - Primo: computational failure (&gt;24h runtime or memory exhaustion)\nSpatial dependence (ξ = 0.3): - Improved FDR control for qch_copula and Primo - No impact on adaFilter - qch_copula reduced FDR from ~0.11 to &lt;0.065 in challenging scenarios\n\n\nDetection Power\nQ = 2: - DACT_Efron: zero power (too conservative) - qch_copula: 0.03-0.124 depending on scenario\nQ = 8, ρ = 0.3:\n\n\n\nTesting hypothesis\nPrimo Power\nadaFilter Power\nqch_copula Power\n\n\n\n\n≥2 traits (dense)\n0.143\n0.317\n0.609\n\n\n≥4 traits (dense)\n0.126\n0.148\n0.534\n\n\n≥8 traits (dense)\n0.125\n0.033\n0.186\n\n\n\nQ = 16, ρ = 0.3:\n\n\n\nTesting hypothesis\nadaFilter Power\nqch_copula Power\n\n\n\n\n≥2 traits (dense)\n0.278\n0.646\n\n\n≥4 traits (dense)\n0.166\n0.678\n\n\n≥8 traits (sparse)\n0.097\n0.568\n\n\n\nqch_copula showed 6× higher power than adaFilter for detecting associations with ≥8 traits in sparse scenarios."
  },
  {
    "objectID": "statistics/dewalsche_2025_40918066.html#computational-efficiency",
    "href": "statistics/dewalsche_2025_40918066.html#computational-efficiency",
    "title": "Large-scale composite hypothesis testing procedure for omics data analyses",
    "section": "Computational Efficiency",
    "text": "Computational Efficiency\n\nMemory-Efficient EM Algorithm\nClassical approach: Stores full posterior matrix T = (τ_ic) requiring 26 GB for n=10^5, Q=15\nqch_copula approach: - Computes posteriors on-the-fly during M-step - Stores only summary statistics S^(t)_i - Reduces memory from 26 GB to 1 MB (same example)\nRuntime (Q=16, n=10^5): - Model fitting: 78 minutes - Per hypothesis test: ~1 minute - Platform: Single thread, 3.2 GB RAM"
  },
  {
    "objectID": "statistics/dewalsche_2025_40918066.html#application-i-psychiatric-disorders",
    "href": "statistics/dewalsche_2025_40918066.html#application-i-psychiatric-disorders",
    "title": "Large-scale composite hypothesis testing procedure for omics data analyses",
    "section": "Application I: Psychiatric Disorders",
    "text": "Application I: Psychiatric Disorders\n\nDataset\n\n14 psychiatric disorders from Psychiatric Genomics Consortium\n5,172,884 common SNPs\nObjective: Identify pleiotropic regions associated with ≥8 disorders\n\n\n\nComparison with PLACO\nOriginal analysis (PLACO): - Aggregated SNPs to 26,024 genes using MAGMA - Performed 91 pairwise analyses (all combinations of 2 disorders) - Identified 38 candidate genes\nqch_copula analysis: - Direct SNP-level analysis (no aggregation) - Single joint test across all 14 disorders - Identified 1,608 SNPs in 28 distinct regions\n\n\nNovel Findings\n35/38 PLACO genes confirmed plus 8 new regions:\n\n\n\nRegion\nChr\nPosition (Mb)\n# SNPs\nTop SNP P-value\n\n\n\n\nNovel 1\n5\n103.6-104.0\n338\n5.16×10^-12\n\n\nNovel 2\n1\n73.8-73.9\n156\n1.01×10^-7\n\n\nNovel 3\n3\n52.6-53.1\n90\n7.27×10^-8\n\n\n\nChromosome 5 region (top finding): - 338 SNPs detected - 25 SNPs associated with 11 disorders - Overlaps with RP11-6N13.1 gene - Previously reported for ADHD, ASD, BIP, MDD, SCZ, TS\nThree PLACO-only genes (NEGR1, TMX2, C11orf31): - Had only 3-7 P-values &lt;0.01 out of 14 (insufficient for ≥8 disorders) - Likely false positives from pairwise approach"
  },
  {
    "objectID": "statistics/dewalsche_2025_40918066.html#application-ii-cucumber-virus-resistance",
    "href": "statistics/dewalsche_2025_40918066.html#application-ii-cucumber-virus-resistance",
    "title": "Large-scale composite hypothesis testing procedure for omics data analyses",
    "section": "Application II: Cucumber Virus Resistance",
    "text": "Application II: Cucumber Virus Resistance\n\nDataset\n\n289 cucumber lines (elite, landraces, hybrids)\n6 viruses: CGMMV, CMV, CVYV, PRSV, WMV, ZYMV\n339,804 common SNPs (after QC)\nObjective: Detect QTLs for multi-virus resistance\n\n\n\nResults by Pleiotropy Level\n\n\n\nNumber of viruses\n# SNPs detected\n# Regions\n\n\n\n\n≥2\n1,845\n5\n\n\n≥3\n164\n1\n\n\n≥4\n15\n1\n\n\n\n\n\nIdentified Hotspot Regions\nFive regions associated with ≥2 viruses:\n\n\n\nRegion\nViruses\nOriginal study\nExternal validation\n\n\n\n\nChr 5: 6.3-8.8 Mb\nWMV, CGMMV, CVYV, CMV\nReported\n-\n\n\nChr 6: 6.8-14.7 Mb\nPRSV, ZYMV\nReported\n-\n\n\nChr 1: 9.1-10.1 Mb\n-\nNovel\n-\n\n\nChr 2: 1.3 Mb\nPRSV, ZYMV\nNovel\nCMV, CABYV QTLs\n\n\nChr 6: 22.8-26.4 Mb\nPRSV, ZYMV\nNovel\nWMV, CABYV QTLs\n\n\n\nThree novel regions not reported in original study: - Two validated by independent studies showing shared resistance mechanisms - Demonstrates enhanced power of joint analysis over individual GWAS"
  },
  {
    "objectID": "statistics/dewalsche_2025_40918066.html#methodological-insights",
    "href": "statistics/dewalsche_2025_40918066.html#methodological-insights",
    "title": "Large-scale composite hypothesis testing procedure for omics data analyses",
    "section": "Methodological Insights",
    "text": "Methodological Insights\n\nAdvantages over Existing Methods\nMixture model approaches (IMIX, c-csmGmm, Primo): - Fully parametric: constrain alternative distributions to Gaussian - qch_copula: nonparametric estimation with copula dependencies\nPairwise methods (PLACO, HDMT): - Limited to Q=2 - Multiple pairwise tests lack statistical guarantees for joint inference - Can produce inconsistent results\nFiltering methods (adaFilter): - More conservative - Lower power for stringent hypotheses\n\n\nP-values vs. Posteriors\nqch_copula establishes theoretical equivalence between: - Adaptive Benjamini-Hochberg FDR control on derived P-values - Local FDR control on posteriors\nAdvantages of P-values: - Compatible with any multiple testing procedure - Enable diagnostic tools (QQ-plots, histograms) - Allow Volcano/Manhattan plots - Facilitate method comparison\n\n\nCopula Modeling Strategy\nSingle correlation matrix across all components: - Balances model flexibility and computational efficiency - Avoids poor estimation from under-represented components - Captures essential dependency structure\nAlternative approaches: - Component-specific matrices (IMIX): computationally prohibitive - No dependencies (qch): severe Type I error inflation\n\n\nRobustness to Dependencies\nWithin-series correlation (ξ = 0.3): - Minimal impact on most methods - Actually improved FDR control for qch_copula - Particularly beneficial in challenging scenarios (Q=16)\nFurther options: - Combine with dependency-aware multiple testing procedures - Apply local score techniques for spatial clustering"
  },
  {
    "objectID": "statistics/dewalsche_2025_40918066.html#practical-recommendations",
    "href": "statistics/dewalsche_2025_40918066.html#practical-recommendations",
    "title": "Large-scale composite hypothesis testing procedure for omics data analyses",
    "section": "Practical Recommendations",
    "text": "Practical Recommendations\n\nChoosing q̃ (Minimum Effect Threshold)\nMultiple strategies: 1. Hypothesis-driven: Based on research question (e.g., pleiotropy → q̃=2) 2. Prior evidence: From previous analyses (e.g., q̃=8 from literature) 3. Cost-benefit: Economic considerations (breeding value of multi-trait resistance) 4. Exploratory: Test multiple q̃ values for ranking\n\n\nMethod Selection\nUse qch_copula when: - Q &gt; 2 traits/conditions - Correlations between traits exist - Need rigorously defined P-values - Large-scale data (105-106 markers, Q ≤ 20) - Testing various composite hypotheses\nConsider alternatives when: - Q = 2 and no dependencies: simpler methods may suffice - Extremely high correlations (ρ &gt; 0.7): expect minor FDR inflation - Need for component-specific direction effects: extensions required\n\n\nImplementation Details\nAvailable in R package qch on CRAN\nKey functions: - Model fitting with copula dependencies - P-value computation for any composite hypothesis - Multiple hypothesis testing without re-estimation"
  },
  {
    "objectID": "statistics/dewalsche_2025_40918066.html#limitations-and-extensions",
    "href": "statistics/dewalsche_2025_40918066.html#limitations-and-extensions",
    "title": "Large-scale composite hypothesis testing procedure for omics data analyses",
    "section": "Limitations and Extensions",
    "text": "Limitations and Extensions\n\nCurrent Limitations\n\nCorrelation levels: Slight FDR inflation at ρ ≥ 0.5 for large Q (16+)\nIndependence assumption: Items assumed independent within series\nDirection agnostic: Does not account for effect signs\nPost-hoc inference: Testing multiple q̃ values raises multiple comparisons issues\n\n\n\nOngoing/Future Work\nEffect direction: - Extension accounting for effect signs available in qch package (independent case) - Copula + direction effects: under development\nWithin-series dependencies: - Compatible with dependency-aware multiple testing - Integration with local score techniques\nModel selection: - Post-hoc inference procedures for q̃ selection"
  },
  {
    "objectID": "statistics/dewalsche_2025_40918066.html#related-concepts",
    "href": "statistics/dewalsche_2025_40918066.html#related-concepts",
    "title": "Large-scale composite hypothesis testing procedure for omics data analyses",
    "section": "Related Concepts",
    "text": "Related Concepts\n\nComposite hypothesis: Union of multiple elementary hypotheses (e.g., H₀¹ ∪ H₀²)\nConfiguration: Vector c = (c₁, …, c_Q) indicating null/alternative status across conditions\nCopula function: Describes dependency structure between marginal distributions\nFlattening: Memory-efficient EM update using on-the-fly computation\nLocal FDR: Posterior probability of belonging to null configuration\nPleiotropy: Single locus affecting multiple traits/phenotypes"
  },
  {
    "objectID": "statistics/mcgowan_2024_2276446.html",
    "href": "statistics/mcgowan_2024_2276446.html",
    "title": "Causal Inference Is Not Just a Statistics Problem",
    "section": "",
    "text": "PubMed: Not Indexed (Journal of Statistics and Data Science Education) DOI: 10.1080/26939169.2023.2276446 Overview generated by: Gemini 2.5 Flash, 26/11/2025"
  },
  {
    "objectID": "statistics/mcgowan_2024_2276446.html#key-findings",
    "href": "statistics/mcgowan_2024_2276446.html#key-findings",
    "title": "Causal Inference Is Not Just a Statistics Problem",
    "section": "Key Findings",
    "text": "Key Findings\nThis article argues that causal inference—the process of determining whether an exposure causes an outcome—is a challenge rooted primarily in study design and conceptual modeling, not just statistical analysis. While statistical methods are necessary for quantifying effects, they cannot salvage a poorly designed study or validate a flawed causal hypothesis.\n\nCausal Inference is Not Statistical Regression\nThe authors emphasize the crucial distinction between prediction/association (a purely statistical exercise) and causal inference (a scientific exercise):\n\nCausality Requires Assumptions: Unlike association, causality requires making strong, often untestable assumptions about the data-generating process. These assumptions include positivity (everyone had a chance to receive the treatment), consistency (the treatment is well-defined), and exchangeability (the treated and untreated groups are comparable, usually requiring control for all confounders).\nStatistics Quantifies, Design Ensures Validity: Statistical methods (like regression, matching, or propensity scores) can only adjust for observed confounding variables under the assumption that all necessary confounders have been identified and measured without error. If a critical confounder is unmeasured (unmeasured confounding), the resulting causal estimate is likely biased, regardless of the sophistication of the statistics used.\n\n\n\nThe Primacy of Study Design\nThe article strongly aligns with the philosophy that “Design Trumps Analysis” (a concept attributed to Donald Rubin).\n\nNeed for Domain Knowledge: The process of identifying the correct causal model and the necessary variables to control (confounders) is a non-statistical process that relies entirely on domain-specific scientific knowledge (e.g., biology, epidemiology, medicine).\nThe “Which Variables to Control?” Problem: The decision of which covariates to include in a regression model is a causal question, not a statistical one. Including a variable that is actually a collider or a mediator can introduce bias where none existed, an error statisticians cannot prevent without external scientific guidance.\nRandomized Controlled Trials (RCTs): RCTs are the gold standard for causal inference precisely because they use a design (randomization) to satisfy the assumption of exchangeability (balance all confounders, measured and unmeasured), thereby circumventing the statistical problem of controlling for confounders.\n\n\n\nConclusion and Education Focus\nThe conclusion stresses that teaching causal inference must go beyond simply running statistical models. Students must be trained to: * Formulate a Causal Question first. * Diagram the Causal Structure using tools like Directed Acyclic Graphs (DAGs). * Identify the Sources of Bias (confounding, selection bias, measurement error) based on their domain knowledge. * Choose a Design (experimental or observational) that minimizes these biases. * Use Statistics only to quantify the effect size within the context of the chosen design and stated causal assumptions."
  },
  {
    "objectID": "statistics/greenland_2016_27209009.html",
    "href": "statistics/greenland_2016_27209009.html",
    "title": "Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations",
    "section": "",
    "text": "PubMed: 27209009 DOI: 10.1007/s10654-016-0149-3 Overview generated by: Gemini 2.5 Flash, 26/11/2025"
  },
  {
    "objectID": "statistics/greenland_2016_27209009.html#key-findings-correcting-the-misinterpretations-of-statistical-inference",
    "href": "statistics/greenland_2016_27209009.html#key-findings-correcting-the-misinterpretations-of-statistical-inference",
    "title": "Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations",
    "section": "Key Findings: Correcting the Misinterpretations of Statistical Inference",
    "text": "Key Findings: Correcting the Misinterpretations of Statistical Inference\nThis essential essay, authored by a collective of prominent statisticians and epidemiologists, provides a detailed guide to the widespread misinterpretations and abuse of basic statistical concepts: P-values, confidence intervals (CIs), and statistical power. It serves as a strong complement to the American Statistical Association’s (ASA) 2016 statement on p-values, emphasizing that these misuses are rampant and lead to profoundly flawed scientific conclusions.\n\nMisinterpretations of the P-value\nThe authors outline 12 common misconceptions about the P-value, which is defined correctly as the probability of observing a test statistic as extreme as, or more extreme than, the one calculated from the data, assuming the null hypothesis (\\(H_0\\)) is true.\nThe paper explicitly states that the P-value is NOT: * The probability that the study hypothesis is true. * The probability that the null hypothesis is true. * The probability that a result is due to chance. * A measure of the magnitude or importance of an effect.\nThey emphasize that a common and disastrous error is using the P-value threshold (e.g., P&lt;0.05) to draw a dichotomous conclusion (i.e., ‘significant’ or ‘non-significant’), which falsely suggests the conclusion is certain or that two studies with slightly different P-values (e.g., P=0.04 and P=0.06) have fundamentally different results.\n\n\nMisinterpretations of Confidence Intervals (CIs)\nThe paper clarifies that a Confidence Interval (CI), commonly 95% CI, is defined by its long-run performance. If one were to repeat the study an infinite number of times, 95% of the CIs constructed would contain the true value of the parameter.\nThe paper stresses that a CI is NOT a probability statement about the parameter in the specific study at hand. Misinterpretations include: * Assuming there is a 95% probability that the true effect lies within the observed interval. * Assuming that values outside the interval are refuted or implausible.\nThe main value of CIs is their ability to convey the precision of the estimate and the range of effect magnitudes that are compatible with the data, encouraging researchers to focus on effect size rather than just statistical significance.\n\n\nMisinterpretations of Statistical Power\nStatistical power is the probability of obtaining a statistically significant result, given a specific assumed effect size and the study’s design.\nThe authors note the main misuses: * Treating power as a continuous measure of study quality; it is highly dependent on the hypothesized effect size. * Misinterpreting low power: A non-significant result from a low-power study does not imply that the true effect is small or non-existent, only that the study was incapable of detecting the hypothesized effect.\n\n\nConclusion\nThe essay’s ultimate conclusion is that statistical inference tools, including P-values, are just one component of scientific reasoning. Their correct use requires attention to design, measurement quality, data integrity, and background knowledge. They recommend using CIs to emphasize effect magnitude and avoiding the common practice of dichotomizing results based on arbitrary P-value thresholds."
  },
  {
    "objectID": "statistics/gururaghavendran_2025_39218433.html",
    "href": "statistics/gururaghavendran_2025_39218433.html",
    "title": "Can algorithms replace expert knowledge for causal inference? A case study on novice use of causal discovery",
    "section": "",
    "text": "PubMed: 39218433 DOI: 10.1093/aje/kwae338 Overview generated by: Gemini 2.5 Flash, 26/11/2025"
  },
  {
    "objectID": "statistics/gururaghavendran_2025_39218433.html#background-and-purpose",
    "href": "statistics/gururaghavendran_2025_39218433.html#background-and-purpose",
    "title": "Can algorithms replace expert knowledge for causal inference? A case study on novice use of causal discovery",
    "section": "Background and Purpose",
    "text": "Background and Purpose\nThis paper addresses the increasing discussion within epidemiology regarding the use of causal discovery algorithms (a form of machine learning) to automate the construction of Causal Directed Acyclic Graphs (DAGs) for covariate selection. The study’s objective was to assess the performance of these data-driven methods, when applied by a novice user, against a known confounded effect: the relationship between placebo adherence and mortality in the Coronary Drug Project (CDP) trial. This relationship is widely accepted to have a true causal effect of null, providing a strong benchmark for evaluating the ability of the algorithms to correctly control for confounding."
  },
  {
    "objectID": "statistics/gururaghavendran_2025_39218433.html#study-design-and-methods",
    "href": "statistics/gururaghavendran_2025_39218433.html#study-design-and-methods",
    "title": "Can algorithms replace expert knowledge for causal inference? A case study on novice use of causal discovery",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\n\nCausal Discovery Implementation\nThe authors tested 4 common causal discovery algorithms: Peter-Clark (PC), Fast Causal Inference (FCI), Fast Greedy Causal Search (FGES), and Greedy Relaxed Sparsest Permutation (GRaSP). These algorithms were run on the CDP placebo arm data using 39 baseline covariates and the adherence/mortality outcome.\n\n\nParameter Variation\nTo simulate novice use and assess robustness, the authors varied several inputs: 1. Statistical Thresholds: For test-based algorithms (PC, FCI, GRaSP), the \\(\\chi^2\\) alpha level (\\(\\alpha\\)) was varied from 0.001 to 0.20. 2. Prior Knowledge: Models were run with no prior knowledge, a 3-tier time-ordering (covariates \\(\\rightarrow\\) adherence \\(\\rightarrow\\) death), and a 4-tier time-ordering (age/race \\(\\rightarrow\\) other covariates \\(\\rightarrow\\) adherence \\(\\rightarrow\\) death).\n\n\nAdjustment Set Selection\nFrom 17 model parameterizations (including 100 bootstrap samples per ensemble), 15 adjustment sets were identified. Because the bootstrapped results often produced cyclic graphs that could not be resolved into minimally sufficient adjustment sets, the authors adopted a simplification strategy: selecting all covariates identified as potential causes of either mortality or adherence in at least one bootstrap sample."
  },
  {
    "objectID": "statistics/gururaghavendran_2025_39218433.html#key-findings-residual-bias-and-subjectivity",
    "href": "statistics/gururaghavendran_2025_39218433.html#key-findings-residual-bias-and-subjectivity",
    "title": "Can algorithms replace expert knowledge for causal inference? A case study on novice use of causal discovery",
    "section": "Key Findings: Residual Bias and Subjectivity",
    "text": "Key Findings: Residual Bias and Subjectivity\n\nPerformance\n\nBaseline-only Adjustment: The adjustment sets identified by the algorithms, when used to adjust for only baseline covariates, performed similarly to prior published results by achieving a roughly 36% reduction of bias from the unadjusted relationship.\nTime-Varying Adjustment (IPW): When more complex methods were used (Inverse Probability Weighting) to adjust for time-varying confounding, the adjustment sets from the causal discovery algorithms resulted in more residual bias compared to the adjustment sets selected by the original CDP expert team.\n\n\n\nChallenges and Subjectivity\n\nInconsistent Results: Varying the input parameters and algorithm type resulted in a wide range of unique adjustment sets. Only about half of the resulting analyses showed compatibility with the known null effect.\nExpert Knowledge Value: The use of time-ordering (prior knowledge) was essential for improving the interpretability of the resulting graphs, particularly concerning temporal relationships like age and race.\nMethodological Difficulty: The use of bootstrap samples frequently led to cyclic graphs, requiring subjective decisions on simplification and adjustment set selection, which introduces substantial subjectivity into the process."
  },
  {
    "objectID": "statistics/gururaghavendran_2025_39218433.html#conclusions-and-recommendations",
    "href": "statistics/gururaghavendran_2025_39218433.html#conclusions-and-recommendations",
    "title": "Can algorithms replace expert knowledge for causal inference? A case study on novice use of causal discovery",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study concludes that while causal discovery algorithms can partially replicate expert findings, their use is not straightforward and introduces a significant degree of subjectivity through the selection of algorithms, input parameters, and interpretation of non-acyclic graphs.\nThe authors strongly recommend that researchers without detailed knowledge of causal discovery algorithms not attempt to use these tools without the aid of an expert in the field. Absent this expert support, the use of traditional subject matter experts to generate causal graphs provides greater transparency about the assumptions made and, in this case study, yielded the best estimate of the true causal effect."
  },
  {
    "objectID": "statistics/hoenig_2001.html",
    "href": "statistics/hoenig_2001.html",
    "title": "The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis",
    "section": "",
    "text": "PubMed: Not Indexed (The American Statistician) DOI: 10.1080/00031305.2001.10473582 Overview generated by: Gemini 2.5 Flash, 26/11/2025"
  },
  {
    "objectID": "statistics/hoenig_2001.html#key-finding-the-fallacy-of-retrospective-power-analysis",
    "href": "statistics/hoenig_2001.html#key-finding-the-fallacy-of-retrospective-power-analysis",
    "title": "The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis",
    "section": "Key Finding: The Fallacy of Retrospective Power Analysis",
    "text": "Key Finding: The Fallacy of Retrospective Power Analysis\nThis article by Hoenig and Heisey critically examines and rejects the common practice of performing post-experiment or retrospective power calculations (also called “observed power”) to interpret a statistically non-significant result (i.e., a failure to reject the null hypothesis, \\(H_0\\)).\n\nThe Flawed Logic of Retrospective Power\nThe authors demonstrate that the use of retrospective power as an aid to interpretation is fundamentally flawed because it is a simple monotonic transformation of the p-value.\n\nDefinition: Retrospective power (or observed power) is typically calculated as the statistical power to detect the observed effect size using the observed sample size and the observed variance.\nThe Circularity Problem: Because the observed effect size is used as the hypothetical “true” effect, the retrospective power calculation is nearly equivalent to the p-value:\n\nA small p-value (significant result) will always lead to a high retrospective power.\nA large p-value (non-significant result) will always lead to a low retrospective power.\n\nNo New Information: Retrospective power provides no additional information beyond what is already contained in the p-value and the confidence interval. Stating that a non-significant result had low power is merely restating the finding that the confidence interval around the point estimate is wide enough to include the null hypothesis.\n\n\n\nWhy the Flaw is Pervasive\nThe practice of retrospective power analysis stems from a misunderstanding of the dilemma of the nonrejected null hypothesis: when we fail to reject \\(H_0\\), we want to know if it’s because the true effect is small (or zero), or because the study lacked power to detect an important effect.\n\nMisleading Interpretation: Advocates of retrospective power claim that a low observed power, combined with a non-significant test, suggests the result is “inconclusive” and that a “Type II error” (failing to reject a false \\(H_0\\)) is likely.\nThe Correct Interpretation: A non-significant result means that the data are consistent with the null hypothesis (\\(H_0\\) being true). The only way to address the dilemma is by looking at the confidence interval to see if it excludes effect sizes that are considered biologically or clinically important.\n\n\n\nRecommendation\nThe authors recommend that statistical power should be used only for planning an experiment (prospective analysis). To interpret the results of a completed study, especially a non-significant finding, researchers should focus on:\n\nThe p-value.\nThe point estimate (observed effect size).\nThe confidence interval (which indicates the range of true effects consistent with the data).\n\nThe confidence interval is the superior tool for interpreting non-significant results because it shows whether important effect sizes have been reasonably ruled out, which is the actual goal of most post-hoc power discussions."
  },
  {
    "objectID": "statistics/lawlor_2017_28108528.html",
    "href": "statistics/lawlor_2017_28108528.html",
    "title": "Triangulation in aetiological epidemiology: Approaches to causal inference",
    "section": "",
    "text": "PubMed: 28108528 DOI: 10.1093/ije/dyw314 Overview generated by: Gemini 2.5 Flash, 26/11/2025"
  },
  {
    "objectID": "statistics/lawlor_2017_28108528.html#key-findings-enhancing-causal-inference-through-triangulation",
    "href": "statistics/lawlor_2017_28108528.html#key-findings-enhancing-causal-inference-through-triangulation",
    "title": "Triangulation in aetiological epidemiology: Approaches to causal inference",
    "section": "Key Findings: Enhancing Causal Inference Through Triangulation",
    "text": "Key Findings: Enhancing Causal Inference Through Triangulation\nThis foundational article in aetiological epidemiology advocates for the systematic use of Triangulation—the practice of integrating results from multiple distinct research approaches—to strengthen causal inference in the study of disease aetiology. The authors argue that relying on single methods or studies is inherently problematic due to the limitations and biases specific to that approach.\n\nThe Principle of Triangulation\nTriangulation requires the simultaneous use of several approaches where:\n\nDifferent Bias Sources: Each approach must possess different key sources of potential bias that are uncorrelated with the biases of the other approaches. This ensures that any consistent finding is less likely to be an artefact of a single, shared flaw.\nIncreased Confidence: When the findings from different, methodologically distinct approaches all point to the same conclusion regarding a causal relationship (e.g., exposure \\(A\\) causes outcome \\(B\\)), confidence in that causal finding is significantly increased.\n\n\n\nIdentifying and Addressing Bias\nThe power of triangulation is particularly evident when methodological biases are explicitly considered:\n\nBias Prediction: The approach is strongest when the key sources of bias of some methods (e.g., unmeasured confounding in observational studies, reverse causation in cross-sectional studies) would predict findings that point in opposite directions if those biases were solely responsible for the observed association. Consistency despite these opposing biases provides strong evidence for causality.\nInconsistency as a Guide: When inconsistencies or contradictions arise between the results of different approaches, the triangulation framework provides a mechanism to identify and dissect the key sources of bias inherent in each method. This process then guides researchers to design further, more robust studies to address the causal question.\n\n\n\nApplication in Aetiological Epidemiology\nThe paper illustrates the application of triangulation by combining evidence from a variety of sources to address epidemiological causal questions, including:\n\nConventional Epidemiology: Standard cohort or case-control studies (prone to confounding and reverse causation).\nMendelian Randomization (MR): Genetic association studies that use genetic variants as instrumental variables (less prone to confounding and reverse causation).\nQuasi-experimental designs: Such as sibling comparisons or natural experiments.\nRandomized Controlled Trials (RCTs): The gold standard for causality, often unfeasible or unethical for long-term aetiological questions.\n\nBy integrating evidence across these different methods, triangulation provides a robust framework for overcoming the inherent limitations of any single study design in defining causality."
  },
  {
    "objectID": "statistics/thoresen_2019_30732587.html",
    "href": "statistics/thoresen_2019_30732587.html",
    "title": "Spurious interaction as a result of categorization",
    "section": "",
    "text": "PubMed: 30732587 DOI: 10.1186/s12874-019-0667-2 Overview generated by: Gemini 2.5 Flash, 26/11/2025"
  },
  {
    "objectID": "statistics/thoresen_2019_30732587.html#key-findings-the-generation-of-spurious-interaction",
    "href": "statistics/thoresen_2019_30732587.html#key-findings-the-generation-of-spurious-interaction",
    "title": "Spurious interaction as a result of categorization",
    "section": "Key Findings: The Generation of Spurious Interaction",
    "text": "Key Findings: The Generation of Spurious Interaction\nThis paper presents an additional argument against the common practice in epidemiological and clinical research of converting continuous exposure variables into categorical variables. It demonstrates that such categorization can lead to spurious interaction effects in multiple regression models, even when no true interaction exists between the continuous variables.\nThe spurious interaction problem is fundamentally linked to other well-known issues caused by categorization, including loss of information and statistical power and an increased risk of Type I error if continuous confounder variables are also categorized."
  },
  {
    "objectID": "statistics/thoresen_2019_30732587.html#study-design-and-methods",
    "href": "statistics/thoresen_2019_30732587.html#study-design-and-methods",
    "title": "Spurious interaction as a result of categorization",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\nThe investigation used a combination of analytical and simulation-based methods:\n\nAnalytical Development\nThe authors derived precise analytical expressions for the linear regression model with two bivariate normally distributed exposure variables (\\(X_1\\) and \\(X_2\\)) and a continuous outcome (\\(Y\\)). Crucially, the true model assumed no interaction between the continuous exposure variables. The analysis then examined the conditions under which an interaction term would appear in a model using the categorized versions (\\(\\tilde{X}_1\\) and \\(\\tilde{X}_2\\)).\n\n\nInterpretation\nThe authors interpret the spurious interaction in two related ways: 1. Measurement Error: Categorization is viewed as an extreme form of differential measurement error. Because the reliability (as measured by the point-biserial correlation) of the categorized variable \\(\\tilde{X}_i\\) varies with the level of the other variable \\(X_j\\), the measurement error is differential, which is known to induce interaction. 2. Residual Confounding: Categorization of a continuous variable leaves residual confounding. Differences in this residual confounding across strata defined by the other exposure variable may lead to the observed spurious interaction."
  },
  {
    "objectID": "statistics/thoresen_2019_30732587.html#results",
    "href": "statistics/thoresen_2019_30732587.html#results",
    "title": "Spurious interaction as a result of categorization",
    "section": "Results",
    "text": "Results\n\nAnalytical Result\nFor two correlated, normally distributed exposure variables, both categorized at the same cut point (\\(c\\)), a spurious interaction term (\\(\\tilde{\\beta}_3\\)) will be induced unless one of two conditions is met: * The two variables are uncorrelated (\\(\\rho=0\\)). * The variables are categorized precisely at the median (\\(c=0\\) in the standardized case).\n\n\nEmpirical Illustrations and Simulation Findings\n\nReal Data Examples: The paper provides two practical examples (one linear model for lung function and one logistic model for myocardial infarction mortality) showing that categorization can change the interpretation of data by generating a statistically significant interaction where the original continuous model showed a non-significant or practically insignificant effect.\nMagnitude: Simulations demonstrated that the magnitude of the induced interaction term (relative to the main effects) increases substantially as the chosen cut point becomes more extreme (further from the median) and as the correlation (\\(\\rho\\)) between the variables increases.\nGeneralizability: Simulations using different distributions (Normal, Uniform, and Chi-square) confirmed that the general effect of spurious interaction due to categorization is present across various distributional shapes."
  },
  {
    "objectID": "statistics/thoresen_2019_30732587.html#conclusions-and-recommendations",
    "href": "statistics/thoresen_2019_30732587.html#conclusions-and-recommendations",
    "title": "Spurious interaction as a result of categorization",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe primary conclusion is a strong recommendation that the categorization of continuous variables in regression modeling should be avoided.\nThe practice introduces a number of problems, including biased estimates, loss of power, and inflated Type-I error rates, with the generation of spurious interaction being another critical drawback.\nIf an interaction effect is found in an analysis using categorized explanatory variables, the researcher must consider the categorization method itself as a potential and likely explanation for the finding.\nAs alternatives, the authors suggest: * Using non-parametric regression methods if the relationship cannot be easily modeled by classical parametric models. * If one chooses to categorize despite the warnings, it is preferable to categorize into more than two groups to minimize the resulting information loss."
  },
  {
    "objectID": "statistics/shaw_2023_37338987.html",
    "href": "statistics/shaw_2023_37338987.html",
    "title": "Comparison of Imputation Strategies for Incomplete Longitudinal Data in Life-Course Epidemiology",
    "section": "",
    "text": "PubMed: 37338987 DOI: 10.1093/aje/kwad139 Overview generated by: Gemini 2.5 Flash, 27/11/2025"
  },
  {
    "objectID": "statistics/shaw_2023_37338987.html#background-and-objective",
    "href": "statistics/shaw_2023_37338987.html#background-and-objective",
    "title": "Comparison of Imputation Strategies for Incomplete Longitudinal Data in Life-Course Epidemiology",
    "section": "Background and Objective",
    "text": "Background and Objective\nIncomplete longitudinal data—data with values missing at different time points for the same individuals—is a pervasive challenge in life-course epidemiology. If not handled correctly, this missingness can introduce bias and lead to incorrect statistical inference. Multiple imputation (MI) is the increasingly preferred method for addressing this, but real-world performance comparisons between different MI techniques are limited.\nThe objective of this study was to conduct a direct comparison of three Multiple Imputation (MI) methods using real-world longitudinal data to assess their performance, feasibility, and impact on causal effect estimates across various missing-data scenarios."
  },
  {
    "objectID": "statistics/shaw_2023_37338987.html#methods-benchmarking-on-hrs-data",
    "href": "statistics/shaw_2023_37338987.html#methods-benchmarking-on-hrs-data",
    "title": "Comparison of Imputation Strategies for Incomplete Longitudinal Data in Life-Course Epidemiology",
    "section": "Methods: Benchmarking on HRS Data",
    "text": "Methods: Benchmarking on HRS Data\nThe researchers compared three Multiple Imputation (MI) methods using data from the Health and Retirement Study (HRS):\n\nNormal Linear Regression (Multiple Imputation by Chained Equations - MICE)\nPredictive Mean Matching (PMM) (a non-parametric MICE method)\nVariable-Tailored Specification\n\nThe comparison was conducted under nine missing-data scenarios that represented combinations of: * Missingness Rate: 10%, 20%, and 30% missingness. * Missingness Mechanism: Missing Completely at Random (MCAR), Missing at Random (MAR), and Missing Not at Random (MNAR).\nThe study introduced record-level missingness to a complete sample of HRS participants and then used the imputed datasets to fit Cox proportional hazards models. The outcome of interest was the effect of four different operationalizations of longitudinal depressive symptoms on subsequent mortality."
  },
  {
    "objectID": "statistics/shaw_2023_37338987.html#key-results-and-conclusion",
    "href": "statistics/shaw_2023_37338987.html#key-results-and-conclusion",
    "title": "Comparison of Imputation Strategies for Incomplete Longitudinal Data in Life-Course Epidemiology",
    "section": "Key Results and Conclusion",
    "text": "Key Results and Conclusion\nThe comparison focused on three performance metrics: bias in hazard ratios, root mean square error (RMSE), and computation time.\n\nBias: Bias in the estimated hazard ratios was found to be similar across all three MI methods.\nConsistency: The results were consistent across the four different ways the longitudinal exposure (depressive symptoms) was defined.\nPerformance: The authors suggest that Predictive Mean Matching (PMM) may be the most appealing strategy for imputing life-course exposure data. PMM consistently showed the lowest root mean square error (RMSE)—indicating greater precision—and maintained competitive computation times with few implementation challenges.\n\nThis study provides valuable, empirically-based guidance for researchers in life-course epidemiology regarding the choice of MI method for handling incomplete longitudinal exposure data."
  },
  {
    "objectID": "statistics/senn_1994_7997705.html",
    "href": "statistics/senn_1994_7997705.html",
    "title": "TESTING FOR BASELINE BALANCE IN CLINICAL TRIALS",
    "section": "",
    "text": "PubMed: 7997705 DOI: 10.1002/sim.4780131610 Overview generated by: Gemini 2.5 Flash, 26/11/2025"
  },
  {
    "objectID": "statistics/senn_1994_7997705.html#key-findings-the-misguided-practice-of-testing-baseline-balance",
    "href": "statistics/senn_1994_7997705.html#key-findings-the-misguided-practice-of-testing-baseline-balance",
    "title": "TESTING FOR BASELINE BALANCE IN CLINICAL TRIALS",
    "section": "Key Findings: The Misguided Practice of Testing Baseline Balance",
    "text": "Key Findings: The Misguided Practice of Testing Baseline Balance\nThis influential paper by Stephen Senn critiques the common, but statistically unsound, practice in randomized controlled trials (RCTs) of performing “tests of baseline homogeneity” (i.e., p-value tests comparing baseline characteristics between treatment arms) before proceeding to analyze the treatment effect on the outcome. Senn argues that this practice is both philosophically flawed and practically misleading.\n\nThe Problem with Testing for Balance\nThe core issue lies in the interpretation of randomization and the nature of the null hypothesis in a properly conducted RCT:\n\nPhilosophically Unsound: The goal of randomization is to ensure that, in expectation, the treatment groups are comparable. Any observed differences at baseline are due purely to chance, and no statistical test is required to confirm this. Performing a test is equivalent to testing the effectiveness of a coin toss—a pointless exercise.\nNo Practical Value: A statistically significant difference at baseline (e.g., \\(p &lt; 0.05\\) for age difference) does not indicate a flaw in the randomization process; it is merely a low-probability chance event that is expected to occur in 5% of all covariates tested. Such a finding offers no useful guidance on how to analyze the treatment effect.\nPotentially Misleading:\n\nNon-significant difference (\\(p &gt; 0.05\\)): This does not prove the groups are “balanced” or comparable. It only means the study lacked the power to detect a difference, or that the observed difference was not large enough to cross the arbitrary significance threshold. The non-significant result might wrongly convince the investigator that no adjustment is needed, even if the difference is clinically important.\nSignificant difference (\\(p &lt; 0.05\\)): This may wrongly prompt an investigator to use an ad hoc adjustment (like Analysis of Covariance, ANCOVA) only for that specific variable, introducing subjectivity into the analysis plan and potentially invalidating the comparison of treatment effects.\n\n\n\n\nThe Recommended Practice: Analysis of Covariance (ANCOVA)\nSenn strongly recommends replacing baseline balance testing with a principled approach to analysis:\n\nPre-specify Prognostic Variables: The study protocol should identify and list all known or suspected prognostic covariates (variables predictive of the outcome), regardless of their baseline distribution.\nRoutine ANCOVA: These prognostic variables should be included in the primary analysis model using Analysis of Covariance (ANCOVA), irrespective of their p-value from the baseline comparison.\nStatistical Advantages: Adjusting for important prognostic variables using ANCOVA increases the precision and statistical power of the treatment effect estimate, leading to narrower confidence intervals. Crucially, the validity of ANCOVA in an RCT relies on the fact that randomization ensures the baseline covariates are unrelated to the treatment assignment, not on their baseline p-value.\n\n\n\nConclusion\nThe paper concludes that the practice of baseline testing is a confusion of philosophy and statistics. Researchers should focus on design (randomization) to ensure validity and statistical efficiency (ANCOVA) to maximize power, rather than using post-randomization tests that are incapable of fulfilling the purpose for which they are intended."
  },
  {
    "objectID": "statistics/pividori_2024_39243756.html",
    "href": "statistics/pividori_2024_39243756.html",
    "title": "An efficient, not-only-linear correlation coefficient based on clustering",
    "section": "",
    "text": "PubMed: 39243756 DOI: 10.1016/j.cels.2024.08.005 Overview generated by: Gemini 2.5 Flash, 28/11/2025"
  },
  {
    "objectID": "statistics/pividori_2024_39243756.html#key-findings-the-clustering-based-correlation-coefficient-ccc",
    "href": "statistics/pividori_2024_39243756.html#key-findings-the-clustering-based-correlation-coefficient-ccc",
    "title": "An efficient, not-only-linear correlation coefficient based on clustering",
    "section": "Key Findings: The Clustering-based Correlation Coefficient (CCC)",
    "text": "Key Findings: The Clustering-based Correlation Coefficient (CCC)\nThis paper introduces the Clustering-based Correlation Coefficient (CCC), an efficient and easy-to-use statistical measure designed to identify both linear and non-linear relationships between variables. The core motivation is to overcome the limitation of standard coefficients, such as Pearson’s \\(r\\), which are fundamentally restricted to measuring linear associations and therefore miss complex patterns prevalent in high-dimensional biological data."
  },
  {
    "objectID": "statistics/pividori_2024_39243756.html#study-design-and-methods-introducing-ccc",
    "href": "statistics/pividori_2024_39243756.html#study-design-and-methods-introducing-ccc",
    "title": "An efficient, not-only-linear correlation coefficient based on clustering",
    "section": "Study Design and Methods: Introducing CCC",
    "text": "Study Design and Methods: Introducing CCC\n\nThe Nature of CCC\nThe CCC is a not-only-linear correlation coefficient that operates by leveraging a clustering-based statistic. Instead of relying on linear regression assumptions, it assesses the strength of the relationship by quantifying how well the marginal information of each variable separately aligns with the clusters formed by the joint distribution of the two variables.\n\n\nMethodology\n\nJoint Clustering: A clustering algorithm is applied to the data points based on the values of both variables simultaneously.\nMarginal Assessment: The CCC then measures the extent to which the values of the first variable alone can explain the clusters identified in step one, and the same is done for the second variable.\nFinal Score: The resulting coefficient reflects the consistency between the joint clustering and the clustering explained by the individual variables.\n\nThe authors show that CCC is a statistically sound measure that can effectively capture a wide array of patterns, including linear, non-linear (e.g., parabolic, or U-shaped), and non-monotonic dependencies."
  },
  {
    "objectID": "statistics/pividori_2024_39243756.html#results-and-empirical-application",
    "href": "statistics/pividori_2024_39243756.html#results-and-empirical-application",
    "title": "An efficient, not-only-linear correlation coefficient based on clustering",
    "section": "Results and Empirical Application",
    "text": "Results and Empirical Application\n\nSimulation and Comparison\nThrough simulated data, the authors demonstrate that the CCC successfully detects relationships where standard measures like Pearson’s \\(r\\), Spearman’s \\(\\rho\\), and even other non-linear coefficients like the Maximal Information Coefficient (MIC), exhibit reduced performance. The CCC is also shown to be computationally efficient, making it scalable for large datasets.\n\n\nApplication to Genome-Scale Data\nThe CCC was applied to human gene expression data (a common source of complex, non-linear biological associations) to identify correlated gene pairs.\n\nDetection of Biological Patterns: When applied to gene expression data, CCC successfully identified non-linear patterns that were not captured by linear-only coefficients.\nSex Differences: The identified non-linear associations were often explained by sex differences, where the relationship between two genes varied significantly across male and female subgroups. For instance, the expression of certain gene pairs might be highly correlated within sexes but exhibit a non-linear overall pattern when sexes are combined, which CCC could detect while linear methods could not."
  },
  {
    "objectID": "statistics/pividori_2024_39243756.html#conclusions-and-recommendations",
    "href": "statistics/pividori_2024_39243756.html#conclusions-and-recommendations",
    "title": "An efficient, not-only-linear correlation coefficient based on clustering",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe Clustering-based Correlation Coefficient (CCC) is a valuable and robust tool for pattern identification in complex datasets. It provides a computationally efficient, simple, and not-only-linear alternative to standard correlation measures. The demonstrated ability of CCC to reveal biologically meaningful non-linear patterns—such as those linked to sex differences in gene expression—highlights its utility for high-throughput analyses in genomics and other fields where relationships are rarely simple or purely linear."
  },
  {
    "objectID": "statistics/ciolino_2013_24138438.html",
    "href": "statistics/ciolino_2013_24138438.html",
    "title": "Covariate Imbalance and Adjustment for Logistic Regression Analysis of Clinical Trial Data",
    "section": "",
    "text": "PubMed: 24138438 DOI: 10.1080/10543406.2013.834912 Overview generated by: Gemini 2.5 Flash, 26/11/2025"
  },
  {
    "objectID": "statistics/ciolino_2013_24138438.html#key-findings-the-necessity-of-covariate-adjustment-in-logistic-regression",
    "href": "statistics/ciolino_2013_24138438.html#key-findings-the-necessity-of-covariate-adjustment-in-logistic-regression",
    "title": "Covariate Imbalance and Adjustment for Logistic Regression Analysis of Clinical Trial Data",
    "section": "Key Findings: The Necessity of Covariate Adjustment in Logistic Regression",
    "text": "Key Findings: The Necessity of Covariate Adjustment in Logistic Regression\nThis paper uses simulation to quantify the benefits of covariate adjustment in the analysis of randomized controlled trials (RCTs) with a binary outcome, specifically focusing on models using logistic regression. The findings highlight that, unlike linear regression, unadjusted and adjusted treatment effect estimates in logistic regression are generally not equivalent, making adjustment a necessary step for precise inference.\n\nInequivalence of Adjusted vs. Unadjusted Estimates\nThe central statistical problem addressed is the non-collapsibility of the Odds Ratio (OR) in logistic regression:\n\nUnadjusted Bias: In the presence of influential covariates, the unadjusted OR (which estimates the marginal effect) is typically a biased estimate of the conditional OR (the effect after controlling for covariates), even if the groups are perfectly balanced due to randomization.\nImpact of Imbalance: While randomization ensures that any imbalance is due to chance, the presence of an influential, imbalanced covariate can further exacerbate the difference between the marginal (unadjusted) and conditional (adjusted) treatment effects.\n\n\n\nQuantifying the Benefit of Adjustment\nThe simulation study quantified the statistical benefits of using adjusted analysis:\n\nIncreased Power: Adjusting for important prognostic covariates significantly increases statistical power to detect a true treatment effect. Simulations demonstrated power benefits of up to 17.5% for log-normally distributed covariates and up to 9.4% for normally distributed covariates when the covariate effect was strong.\nReduced Bias: Adjustment helps reduce the bias of the treatment effect estimate with respect to the conditional treatment effect (the effect being targeted in the adjusted model).\n\n\n\nRecommendations for Clinical Trial Analysis\nThe authors reinforce established guidelines and provide practical advice for analysis:\n\nPre-specification: Following International Conference on Harmonization (ICH) guidelines, covariate adjustment should be pre-specified in the study protocol. Unplanned adjustments should be considered secondary analyses.\nBalance is Not Enough: If adjustment is not possible or unplanned, achieving strong baseline balance in continuous covariates can mitigate some of the shortcomings (lower power, greater potential for bias) of unadjusted analyses, but it cannot fully eliminate the inherent difference between the marginal and conditional ORs due to the non-collapsibility of the logistic model.\nFocus on Efficiency: The primary reason for adjustment is not to correct a failure of randomization, but to increase the efficiency (power) of the analysis by accounting for known sources of variation in the outcome."
  },
  {
    "objectID": "statistics/wasserstein_2019_1583913.html",
    "href": "statistics/wasserstein_2019_1583913.html",
    "title": "Moving to a World Beyond \\(p < 0.05\\)",
    "section": "",
    "text": "PubMed: Not Indexed (The American Statistician) DOI: 10.1080/00031305.2019.1583913 Overview generated by: Gemini 2.5 Flash, 26/11/2025"
  },
  {
    "objectID": "statistics/wasserstein_2019_1583913.html#key-findings-the-case-against-dichotomous-statistical-significance",
    "href": "statistics/wasserstein_2019_1583913.html#key-findings-the-case-against-dichotomous-statistical-significance",
    "title": "Moving to a World Beyond \\(p < 0.05\\)",
    "section": "Key Findings: The Case Against Dichotomous Statistical Significance",
    "text": "Key Findings: The Case Against Dichotomous Statistical Significance\nThis highly influential editorial, which introduced a special issue of The American Statistician containing 43 papers on the topic, marks a critical step in the statistical community’s effort to reform scientific practice. It explicitly calls for an end to the culture of dichotomous thinking based solely on the threshold of \\(p &lt; 0.05\\). The authors argue that declaring a result “statistically significant” based on an arbitrary cutoff is counterproductive and has fueled the reproducibility crisis.\n\nThe Problem with Dichotomization\nThe fundamental issue lies in the over-simplification of complex statistical results into a binary “significant/non-significant” outcome:\n\nArbitrary Cutoff: The \\(p &lt; 0.05\\) threshold is arbitrary. Treating a \\(p=0.049\\) result as fundamentally different from a \\(p=0.051\\) result leads to illogical conclusions and flawed decision-making.\nExaggerated Confidence: Declaring a result “statistically significant” implies a certainty or importance that the p-value alone does not justify. It often leads researchers and the public to mistake statistical significance for scientific or clinical importance.\nFueling Bias: The dichotomous framework is the root cause of poor research practices like P-hacking (data-dependent manipulation to cross the threshold) and publication bias (selective reporting of results that pass the threshold). These biases inflate the true rate of false positives in the literature.\n\n\n\nThe Solution: Embracing a Continuous View of Evidence\nThe authors propose a simple, yet profound, shift in reporting and thinking:\n\nAbandon “Statistical Significance”: The term “statistically significant” and its binary cousin “non-significant” should be retired from scientific discourse.\nFocus on Compatibility: Researchers should instead report the p-value as a measure of incompatibility between the data and the assumed statistical model (usually the null hypothesis). They must interpret this value continuously, considering the p-value’s proximity to zero as a gradient of evidence against the null.\nEmphasize Magnitude and Precision: The core focus of reporting should be on the effect size and its uncertainty, which is best communicated via Confidence Intervals (CIs) or Credible Intervals (from Bayesian analysis). CIs show the range of plausible effects compatible with the data.\nIntegrate Context: Conclusions must be based on the entirety of the evidence, including the context of the research, the quality of the study design, external knowledge, and the costs/benefits of potential actions, not just the p-value.\n\n\n\nThe Role of Prediction Intervals and False Discovery Rates\nThe article encourages the use of various other tools that provide a more complete picture of the evidence, such as:\n\nPrediction Intervals: Show the range of values expected for a future observation.\nFalse Discovery Rates (FDR) and False Positive Risks (FPR): Help quantify the probability that a “significant” finding is actually false, which is often much higher than the p-value suggests.\n\nThe statement concludes by asserting that the scientific community needs to move toward a “post p &lt; 0.05 era” where thoughtful interpretation replaces rigid decision rules."
  },
  {
    "objectID": "statistics/wasserstein_2016_26820252.html",
    "href": "statistics/wasserstein_2016_26820252.html",
    "title": "The ASA’s Statement on p-Values: Context, Process, and Purpose",
    "section": "",
    "text": "DOI: 10.1080/00031305.2016.1154108 Overview generated by: Gemini 2.5 Flash, 26/11/2025"
  },
  {
    "objectID": "statistics/wasserstein_2016_26820252.html#key-findings-principles-for-the-proper-use-of-the-p-value",
    "href": "statistics/wasserstein_2016_26820252.html#key-findings-principles-for-the-proper-use-of-the-p-value",
    "title": "The ASA’s Statement on p-Values: Context, Process, and Purpose",
    "section": "Key Findings: Principles for the Proper Use of the p-Value",
    "text": "Key Findings: Principles for the Proper Use of the p-Value\nThis article presents and explains the American Statistical Association’s (ASA) Statement on p-Values and Statistical Significance, the first-ever policy statement from the ASA specifically addressing a foundational issue of statistical inference. The statement was prompted by the widespread misuse and misinterpretation of the p-value across all fields of science, which contributes to the crisis of irreproducible research.\n\nSix Core Principles for p-Values\nThe statement outlines six key principles, aiming to promote better practice and curb common errors:\n\nP-values can indicate how incompatible the data are with a specified statistical model.\n\nThe p-value measures the incompatibility between the data and the assumed statistical model (usually the null hypothesis). A small p-value indicates that the data are unlikely under the null model.\n\nP-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.\n\nThis directly addresses the most common and disastrous misinterpretation: that a low p-value (e.g., P=0.01) means the null hypothesis has only a 1% chance of being true. P-values relate to data given the model, not the probability of the model given the data.\n\nScientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\n\nThe ASA strongly discourages the practice of dichotomizing results into “statistically significant” and “not statistically significant.” This threshold thinking can lead to flawed decisions, ignoring the importance of effect size, context, and other evidence.\n\nProper inference requires full reporting and transparency.\n\nThe selective reporting of only “significant” results (known as P-hacking or publication bias) renders the reported p-values meaningless. All analyses, assumptions, and findings, regardless of the p-value, must be disclosed.\n\nA p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\n\nStatistical significance is often confused with substantive (clinical, practical, or scientific) importance. A very large study can produce a statistically significant p-value for a trivial effect, while a small study might fail to find significance for a massive, important effect. The focus should be on effect magnitude.\n\nBy itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.\n\nScientific reasoning requires more than just a p-value. It demands contextual knowledge, common sense, study design quality, data reliability, and integration with external evidence. Other statistical measures, particularly Confidence Intervals (CIs) and Bayesian methods, should be used to provide a richer understanding of the evidence.\n\n\n\n\nPurpose of the Statement\nThe statement is not intended to be a simple replacement for null hypothesis significance testing, but rather a step toward a “post p&lt;0.05 era” where statistical methods are used more thoughtfully. It calls for a move toward better scientific practice characterized by open communication, transparent methodology, and a focus on effect size, precision (CIs), and context."
  },
  {
    "objectID": "multi-omics/okamoto_2025_39901160.html",
    "href": "multi-omics/okamoto_2025_39901160.html",
    "title": "Multi-INTACT: integrative analysis of the genome, transcriptome, and proteome identifies causal mechanisms of complex traits",
    "section": "",
    "text": "PubMed: 39901160 DOI: 10.1186/s13059-025-03480-2 Overview generated by: Gemini 2.5 Flash, 27/11/2025"
  },
  {
    "objectID": "multi-omics/okamoto_2025_39901160.html#background-and-objective",
    "href": "multi-omics/okamoto_2025_39901160.html#background-and-objective",
    "title": "Multi-INTACT: integrative analysis of the genome, transcriptome, and proteome identifies causal mechanisms of complex traits",
    "section": "Background and Objective",
    "text": "Background and Objective\nGenome-Wide Association Studies (GWAS) have localized thousands of single nucleotide polymorphisms (SNPs) associated with complex traits. However, most of these variants reside in non-coding regions, making it challenging to identify the effector genes and the complete molecular cascade by which they influence disease. It is widely hypothesized that genetic variants primarily act by perturbing molecular intermediate traits, such as gene expression (transcriptomics) and protein levels (proteomics).\nThis paper introduces Multi-INTACT (Integrative Analysis of Causal Transcriptome and Proteome), a novel statistical framework designed to: 1. Jointly analyze GWAS, eQTL (expression quantitative trait loci), and pQTL (protein quantitative trait loci) summary statistics. 2. Systematically identify the causal chain from genetic variant to gene expression to protein level, and finally to the complex trait outcome. 3. Quantify the proportion of GWAS heritability mediated by both transcriptional and proteomic regulation."
  },
  {
    "objectID": "multi-omics/okamoto_2025_39901160.html#methods-the-multi-intact-framework",
    "href": "multi-omics/okamoto_2025_39901160.html#methods-the-multi-intact-framework",
    "title": "Multi-INTACT: integrative analysis of the genome, transcriptome, and proteome identifies causal mechanisms of complex traits",
    "section": "Methods: The Multi-INTACT Framework",
    "text": "Methods: The Multi-INTACT Framework\n\nData Integration and Causal Modeling\nMulti-INTACT extends the functionality of the existing INTACT method by incorporating three molecular layers: genome, transcriptome, and proteome.\n\nSNP-to-Gene Expression (eQTL) and SNP-to-Protein (pQTL) Mapping: The method simultaneously estimates the joint effects of all SNPs in a locus on both gene expression and protein abundance.\nCausal Chain Analysis: Multi-INTACT employs a multi-mediator causal model based on the Inverse-Variance Weighted (IVW) method from Mendelian Randomization (MR). It models the complex trait (outcome) as being causally influenced by protein levels (direct mediator) and gene expression (indirect mediator). This allows the tool to distinguish between genetic effects that flow through expression, through protein, or through a combination.\nHeritability Partitioning: The framework also quantifies the proportion of the GWAS heritability explained by the genetic effects on gene expression (\\(\\text{h}^2_E\\)) and on protein levels (\\(\\text{h}^2_P\\)).\n\n\n\nApplication\nThe method was applied to publicly available summary statistics for seven complex traits (e.g., triglycerides, HDL cholesterol) and integrated with large-scale multi-omics datasets from various tissues (e.g., adipose, muscle, liver)."
  },
  {
    "objectID": "multi-omics/okamoto_2025_39901160.html#key-results-and-findings",
    "href": "multi-omics/okamoto_2025_39901160.html#key-results-and-findings",
    "title": "Multi-INTACT: integrative analysis of the genome, transcriptome, and proteome identifies causal mechanisms of complex traits",
    "section": "Key Results and Findings",
    "text": "Key Results and Findings\n\nCausal Genes and Mediation\n\nSignificant Causal Proteins: Multi-INTACT identified numerous proteins whose plasma levels were found to be causally associated with the complex traits studied.\nTranscriptional and Proteomic Mediation: The framework successfully partitioned the heritability, demonstrating that many GWAS signals are indeed mediated by changes in molecular phenotypes. For example, for lipids, the method identified regulatory effects flowing from genetic variants, through gene expression, and subsequently through the regulation of Apolipoproteins (proteins) to affect the final trait.\n\n\n\nNovel Regulatory Mechanisms\nThe joint modeling allowed the authors to uncover specific regulatory mechanisms that would be missed by single-omics or simpler MR approaches:\n\nProtein-Mediated Effects: The analysis confirmed the involvement of genes known to be related to the traits (e.g., LPL and APOE for lipids) and refined the causal path, often showing that the effects are primarily mediated by the protein level, not just the mRNA level.\nNovel Loci: Multi-INTACT uncovered several novel gene-trait associations and provided mechanistic evidence that these associations are driven by molecular regulation."
  },
  {
    "objectID": "multi-omics/okamoto_2025_39901160.html#conclusions-and-significance",
    "href": "multi-omics/okamoto_2025_39901160.html#conclusions-and-significance",
    "title": "Multi-INTACT: integrative analysis of the genome, transcriptome, and proteome identifies causal mechanisms of complex traits",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nMulti-INTACT provides a sophisticated and statistically rigorous framework for performing causal integration of tri-omics data (genome, transcriptome, proteome). It successfully moves beyond simple association to illuminate the molecular mechanism underlying GWAS hits.\nThe ability to jointly model and partition heritability across the transcriptional and proteomic layers is crucial for pinpointing the exact effector genes and the most proximal molecular target for therapeutic intervention, accelerating the translation of GWAS findings into clinical insights."
  },
  {
    "objectID": "multi-omics/argelaguet_2018_29925568.html",
    "href": "multi-omics/argelaguet_2018_29925568.html",
    "title": "Multi-Omics Factor Analysis a framework for unsupervised integration of multi-omics data sets",
    "section": "",
    "text": "PubMed: 29925568 DOI: 10.15252/msb.20178124 Overview generated by: Gemini 2.5 Flash, 27/11/2025"
  },
  {
    "objectID": "multi-omics/argelaguet_2018_29925568.html#background-and-objective",
    "href": "multi-omics/argelaguet_2018_29925568.html#background-and-objective",
    "title": "Multi-Omics Factor Analysis a framework for unsupervised integration of multi-omics data sets",
    "section": "Background and Objective",
    "text": "Background and Objective\nThe complexity and heterogeneity of multi-omics data (e.g., combining genomics, transcriptomics, and epigenomics) require sophisticated statistical methods for integration. Most existing methods are either limited to two data types or are “supervised,” meaning they require a known outcome variable (like disease status) for analysis. There was a critical need for unsupervised integration methods that can systematically discover the principal sources of variation in multi-omics datasets without prior knowledge of the relevant biological axes.\nThis paper introduces MOFA (Multi-Omics Factor Analysis), a computational framework designed to: 1. Unsupervisedly integrate multiple heterogeneous multi-omics data sets. 2. Infer a set of latent factors (hidden variables) that capture the major axes of biological and technical variability. 3. Disentangle shared heterogeneity (variation common to multiple omics layers) from modality-specific heterogeneity (variation unique to a single omics layer)."
  },
  {
    "objectID": "multi-omics/argelaguet_2018_29925568.html#methods-the-mofa-framework",
    "href": "multi-omics/argelaguet_2018_29925568.html#methods-the-mofa-framework",
    "title": "Multi-Omics Factor Analysis a framework for unsupervised integration of multi-omics data sets",
    "section": "Methods: The MOFA Framework",
    "text": "Methods: The MOFA Framework\n\nCore Algorithm\nMOFA is based on a general form of Factor Analysis. It models each observed omics data matrix (e.g., RNA expression, DNA methylation) as a linear combination of a small number of shared, hidden latent factors.\n\nShared Factors: These factors simultaneously explain the variability across all data modalities. The model automatically learns which factors are relevant for which omics layer, thus mapping the factors to the data modalities they affect.\nSparsity: The model employs a sparse prior on the factor loadings. This ensures that each factor is defined by only a small, interpretable subset of molecular features (genes, CpGs, etc.), making the resulting factors easier to interpret biologically.\nProbabilistic Framework: Being a fully probabilistic model, MOFA can naturally handle missing values (data imputation), batch effects, and different data distributions (e.g., binary for mutations, continuous for expression).\n\n\n\nApplication: Chronic Lymphocytic Leukemia (CLL)\nMOFA was applied to a large cohort of Chronic Lymphocytic Leukemia (CLL) patient samples, profiled for four molecular modalities: somatic mutations, RNA expression, DNA methylation, and ex vivo drug responses."
  },
  {
    "objectID": "multi-omics/argelaguet_2018_29925568.html#key-results-and-unsupervised-disentanglement",
    "href": "multi-omics/argelaguet_2018_29925568.html#key-results-and-unsupervised-disentanglement",
    "title": "Multi-Omics Factor Analysis a framework for unsupervised integration of multi-omics data sets",
    "section": "Key Results and Unsupervised Disentanglement",
    "text": "Key Results and Unsupervised Disentanglement\n\nDissecting CLL Heterogeneity\nMOFA successfully identified 12 latent factors that captured the major dimensions of disease heterogeneity in CLL. Crucially, the factors could be grouped:\n\nShared Factors: Factors shared across RNA expression, DNA methylation, and drug responses were strongly associated with known clinical drivers, such as the immunoglobulin heavy chain variable (IGHV) mutation status, a major prognostic marker in CLL.\nModality-Specific Factors: Other factors were specific to a single omics layer, such as a factor that only loaded on somatic mutations and distinguished known mutations like SF3B1.\n\n\n\nDownstream Applications\nThe inferred latent factors enabled several downstream analyses:\n\nSample Clustering: The factors were used to robustly identify subgroups of patients that were characterized by distinct multi-omics profiles.\nFeature Set Identification: The sparse loadings immediately identified the minimal set of key molecular features (genes, CpGs, etc.) that defined each factor and its associated biological axis.\nPrediction and Imputation: The factors could be used for data imputation of missing values and for improved outcome prediction compared to models built on raw data."
  },
  {
    "objectID": "multi-omics/argelaguet_2018_29925568.html#conclusions-and-significance",
    "href": "multi-omics/argelaguet_2018_29925568.html#conclusions-and-significance",
    "title": "Multi-Omics Factor Analysis a framework for unsupervised integration of multi-omics data sets",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nMOFA is a highly effective, flexible, and scalable statistical framework for the unsupervised integration of multi-omics data sets .\nIts ability to disentangle shared and specific axes of variation is paramount for discovering and interpreting the underlying biological phenomena in complex datasets. By distilling high-dimensional omics data into a small, interpretable set of latent factors, MOFA provides a powerful foundation for precision medicine, subtyping diseases, and gaining mechanistic insights."
  },
  {
    "objectID": "multi-omics/velten_2022_35027765.html",
    "href": "multi-omics/velten_2022_35027765.html",
    "title": "Identifying temporal and spatial patterns of variation from multimodal data using MEFISTO",
    "section": "",
    "text": "PubMed: 35027765 DOI: 10.1038/s41592-021-01343-9 Overview generated by: Gemini 2.5 Flash, 27/11/2025"
  },
  {
    "objectID": "multi-omics/velten_2022_35027765.html#background-and-objective",
    "href": "multi-omics/velten_2022_35027765.html#background-and-objective",
    "title": "Identifying temporal and spatial patterns of variation from multimodal data using MEFISTO",
    "section": "Background and Objective",
    "text": "Background and Objective\nFactor analysis, as implemented in tools like MOFA (Multi-Omics Factor Analysis), is widely used for dimensionality reduction and multi-omics integration. However, these models traditionally assume that samples are independent of one another. This assumption fails in modern high-resolution biological studies that profile molecular data with temporal (time-series) or spatial dependencies (e.g., spatial transcriptomics, longitudinal cohorts, developmental atlases).\nThis paper introduces MEFISTO (Multi-omics Factor Analysis Informed by Spatial and Temporal Omics), an extension of the MOFA framework, designed to: 1. Perform factor analysis while explicitly accounting for spatial or temporal dependencies between samples. 2. Perform spatio-temporally informed dimensionality reduction and imputation on high-dimensional multi-omics data. 3. Separate smooth (e.g., developmental trajectory) from non-smooth (e.g., technical noise) patterns of variation."
  },
  {
    "objectID": "multi-omics/velten_2022_35027765.html#methods-the-mefisto-framework",
    "href": "multi-omics/velten_2022_35027765.html#methods-the-mefisto-framework",
    "title": "Identifying temporal and spatial patterns of variation from multimodal data using MEFISTO",
    "section": "Methods: The MEFISTO Framework",
    "text": "Methods: The MEFISTO Framework\n\nCore Algorithm and Innovation\nMEFISTO maintains the core latent factor model of MOFA, where data from different omics layers is modeled as a linear combination of shared latent factors. The key innovation is how it models these latent factors:\n\nSpatio-Temporal Prior: MEFISTO incorporates a Gaussian Process (GP) prior on the factor values. This GP prior allows the factors to be smooth functions of the observed spatial or temporal coordinates (i.e., time points, spatial locations).\nModeling Dependencies: By using the GP prior, MEFISTO learns latent factors that are constrained to change gradually over space or time, which aligns with biological reality (e.g., developmental progression, cellular diffusion). Factors not relevant to the spatial/temporal axes are modeled with a non-smooth prior.\nJoint Integration: Like MOFA, MEFISTO can integrate data from multiple omics modalities measured on the same samples and simultaneously identify shared and modality-specific sources of variation.\n\n\n\nApplications\nMEFISTO was validated across diverse datasets with structured dependencies: 1. Spatial Transcriptomics: Applied to mouse organ data to reconstruct the spatial organization of gene expression. 2. Longitudinal Microbiome Study: Used to capture smooth temporal changes in the gut microbiome. 3. Single-Cell Multi-Omics: Applied to a mouse gastrulation atlas to align complex single-cell transcriptomic and epigenetic data along a developmental pseudotime axis."
  },
  {
    "objectID": "multi-omics/velten_2022_35027765.html#key-results-and-capabilities",
    "href": "multi-omics/velten_2022_35027765.html#key-results-and-capabilities",
    "title": "Identifying temporal and spatial patterns of variation from multimodal data using MEFISTO",
    "section": "Key Results and Capabilities",
    "text": "Key Results and Capabilities\n\nInformed Dimensionality Reduction\nIn all applications, MEFISTO successfully found latent factors that were biologically relevant and showed a smooth progression across the given time points or spatial coordinates.\n\n\nInterpolation and Imputation\nThe GP prior allows MEFISTO to perform robust interpolation—predicting factor values and corresponding omics data for unobserved time points or spatial locations. This is crucial for filling gaps in time-series experiments.\n\n\nData Alignment\nMEFISTO’s ability to identify underlying common factors makes it excellent for aligning multiple related datasets. For instance, it can align single-cell data from different samples or studies onto a common developmental trajectory, identifying the shared underlying factors of biological variation."
  },
  {
    "objectID": "multi-omics/velten_2022_35027765.html#conclusions-and-significance",
    "href": "multi-omics/velten_2022_35027765.html#conclusions-and-significance",
    "title": "Identifying temporal and spatial patterns of variation from multimodal data using MEFISTO",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nMEFISTO is a versatile and essential tool for the analysis of modern biological data that contains temporal or spatial structure. By incorporating Gaussian Process priors into the factor analysis framework, it overcomes the independence assumption of classical methods.\nMEFISTO’s capabilities in spatio-temporally informed dimensionality reduction, interpolation, and multi-dataset alignment make it a powerful method for extracting meaningful, smooth biological patterns from complex single-cell and multi-omics atlases."
  },
  {
    "objectID": "multi-omics/tenenhaus_2014_24550197.html",
    "href": "multi-omics/tenenhaus_2014_24550197.html",
    "title": "Variable selection for generalized canonical correlation analysis",
    "section": "",
    "text": "PubMed: 24550197 DOI: 10.1093/biostatistics/kxu001 Overview generated by: Gemini 2.5 Flash, 27/11/2025"
  },
  {
    "objectID": "multi-omics/tenenhaus_2014_24550197.html#background-and-objective",
    "href": "multi-omics/tenenhaus_2014_24550197.html#background-and-objective",
    "title": "Variable selection for generalized canonical correlation analysis",
    "section": "Background and Objective",
    "text": "Background and Objective\nThe rise of multi-omics studies, which generate three or more heterogeneous datasets (e.g., transcriptomics, proteomics, clinical data), necessitates statistical methods that can effectively integrate these data blocks. Canonical Correlation Analysis (CCA) is a classic method for finding shared variance between two datasets, and Generalized Canonical Correlation Analysis (GCCA) is its generalization to three or more data blocks.\nThis paper addresses a major limitation of standard GCCA: the resulting components are linear combinations of all input variables, making biological interpretation difficult due to high dimensionality. The primary objective was to introduce a method to perform variable selection within the GCCA framework, simultaneously reducing dimensionality and identifying the most relevant features."
  },
  {
    "objectID": "multi-omics/tenenhaus_2014_24550197.html#methods-sparse-generalized-canonical-correlation-analysis-sgcca",
    "href": "multi-omics/tenenhaus_2014_24550197.html#methods-sparse-generalized-canonical-correlation-analysis-sgcca",
    "title": "Variable selection for generalized canonical correlation analysis",
    "section": "Methods: Sparse Generalized Canonical Correlation Analysis (SGCCA)",
    "text": "Methods: Sparse Generalized Canonical Correlation Analysis (SGCCA)\n\nRegularized GCCA (RGCCA)\nThe authors first utilize the Regularized Generalized Canonical Correlation Analysis (RGCCA) framework, a flexible method for integrating \\(K \\ge 2\\) data blocks by defining different objectives (e.g., maximizing the sum of pairwise correlations) and connections (a block design matrix) between the data blocks.\n\n\nThe Innovation: SGCCA\nThe key methodological contribution is the introduction of a sparse penalty (an \\(L_1\\) penalty, similar to LASSO) into the RGCCA objective function. This novel method is termed Sparse Generalized Canonical Correlation Analysis (SGCCA).\n\nFunction: SGCCA performs dimension reduction (by finding latent components) and variable selection simultaneously.\nMechanism: The sparse penalty forces the loading vectors (which define the components) to contain many zero values. This means that the latent components are computed using only a small, relevant subset of the original features from each omics block.\nGoal: By selecting only a few, highly contributing variables, SGCCA facilitates the interpretability of the integrated results and identifies a minimal set of molecular features that drive the shared correlation structure across all omics datasets."
  },
  {
    "objectID": "multi-omics/tenenhaus_2014_24550197.html#conclusions-and-significance",
    "href": "multi-omics/tenenhaus_2014_24550197.html#conclusions-and-significance",
    "title": "Variable selection for generalized canonical correlation analysis",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nThe introduction of SGCCA provides a powerful and flexible multivariate statistical tool for the unsupervised integration of multi-omics data.\nBy seamlessly incorporating variable selection into the generalized CCA framework, the method addresses the high dimensionality inherent in omics data. SGCCA enables researchers to distill the complex relationships across multiple omics layers into a coherent, biologically meaningful set of key molecular features that are responsible for the shared variation across the integrated datasets.\nThe method is foundational for subsequent multi-omics integration tools, providing a basis for correlation-based feature selection in systems biology."
  },
  {
    "objectID": "multi-omics/chalise_2023_36929074.html",
    "href": "multi-omics/chalise_2023_36929074.html",
    "title": "Statistical Methods for Integrative Clustering of Multi-omics Data",
    "section": "",
    "text": "PubMed: 36929074 DOI: 10.1007/978-1-0716-2986-4_5 Overview generated by: Gemini 2.5 Flash, 27/11/2025"
  },
  {
    "objectID": "multi-omics/chalise_2023_36929074.html#background-and-objective",
    "href": "multi-omics/chalise_2023_36929074.html#background-and-objective",
    "title": "Statistical Methods for Integrative Clustering of Multi-omics Data",
    "section": "Background and Objective",
    "text": "Background and Objective\nThe heterogeneity of cancers, driven by complex alterations across multiple molecular levels (genomics, epigenomics, transcriptomics), necessitates advanced statistical methods. Identifying robust molecular subtypes of cancer is a crucial step for personalized medicine. Traditional clustering methods applied to single omics layers often yield unstable results.\nThis paper provides an overview and practical guide to integrative clustering—an unsupervised learning approach that uses multi-omics data to simultaneously identify shared disease subtypes and their associated molecular signatures. The chapter primarily focuses on model-based statistical approaches."
  },
  {
    "objectID": "multi-omics/chalise_2023_36929074.html#methods-integrative-clustering-taxonomy",
    "href": "multi-omics/chalise_2023_36929074.html#methods-integrative-clustering-taxonomy",
    "title": "Statistical Methods for Integrative Clustering of Multi-omics Data",
    "section": "Methods: Integrative Clustering Taxonomy",
    "text": "Methods: Integrative Clustering Taxonomy\nThe chapter classifies and describes the prominent statistical methods used for integrative clustering of multi-omics data:\n\n1. Model-Based Approaches\nThese methods assume that the variation across different omics datasets can be explained by a set of shared, hidden (latent) variables or factors.\n\niCluster (Integrative Clustering): A latent variable model that uses a joint likelihood function to integrate multiple omics data types. It incorporates penalized regression to enforce sparsity and perform feature selection, identifying the key molecular features that define the clusters.\niClusterPlus (Bayesian Extensions): Enhancements that use Bayesian methods and Dirichlet process priors to improve robustness, handle diverse data distributions (e.g., count, binary), and automatically estimate the optimal number of clusters (\\(K\\)).\n\n\n\n2. Nonparametric Approaches\nThese methods do not rely on specific distributional assumptions.\n\nSimilarity Network Fusion (SNF): This method constructs a similarity network for each individual omics dataset, then iteratively fuses them into a single, comprehensive patient similarity network, which is then clustered to define the final disease subtypes."
  },
  {
    "objectID": "multi-omics/chalise_2023_36929074.html#conclusions-and-significance",
    "href": "multi-omics/chalise_2023_36929074.html#conclusions-and-significance",
    "title": "Statistical Methods for Integrative Clustering of Multi-omics Data",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nIntegrative clustering is essential for analyzing heterogeneous multi-omics data. By combining information across multiple molecular layers, these methods produce stable and biologically meaningful disease subtypes and their corresponding molecular signatures. The guide provides the necessary foundation and practical steps for researchers to apply these techniques in cancer biology and precision medicine, including data preprocessing, model selection, and biological interpretation (e.g., using Kaplan-Meier curves to assess survival differences between subtypes)."
  },
  {
    "objectID": "multi-omics/lin_2020_32691502.html",
    "href": "multi-omics/lin_2020_32691502.html",
    "title": "A General Framework for Integrative Analysis of Incomplete Multi-omics Data",
    "section": "",
    "text": "PubMed: 32691502 DOI: 10.1002/gepi.22328 Overview generated by: Gemini 2.5 Flash, 27/11/2025"
  },
  {
    "objectID": "multi-omics/lin_2020_32691502.html#background-and-objective",
    "href": "multi-omics/lin_2020_32691502.html#background-and-objective",
    "title": "A General Framework for Integrative Analysis of Incomplete Multi-omics Data",
    "section": "Background and Objective",
    "text": "Background and Objective\nThe analysis of modern multi-omics studies is complicated by two major statistical challenges: 1. Missing Data: Not all omics data types (e.g., protein expression, metabolomics) are measured on every subject, often due to cost constraints, leading to partial or incomplete datasets. 2. Detection Limits (Censoring): Quantitative omics measurements frequently involve values that fall below (or above) the detection limit of the instrument, leading to left- (or right-) censoring of the data.\nFailing to properly account for these issues can lead to biased parameter estimates and reduced statistical power in integrative analyses. This paper proposes a general statistical framework to rigorously and powerfully handle missing values and detection limits in the integrative analysis of multi-omics data."
  },
  {
    "objectID": "multi-omics/lin_2020_32691502.html#methods-the-integrative-analysis-framework",
    "href": "multi-omics/lin_2020_32691502.html#methods-the-integrative-analysis-framework",
    "title": "A General Framework for Integrative Analysis of Incomplete Multi-omics Data",
    "section": "Methods: The Integrative Analysis Framework",
    "text": "Methods: The Integrative Analysis Framework\n\nModeling Incomplete Data\nThe framework addresses two main analytic goals common in multi-omics studies: 1. Omics-to-Genetics (xQTL): Relating quantitative omics features (e.g., protein or metabolite levels) to genetic variants (SNPs) and covariates using linear regression models. 2. Omics-to-Phenotype: Relating phenotypes (e.g., disease status) to quantitative omics features and covariates using generalized linear models (GLMs).\n\n\nKey Innovation: Joint Likelihood and EM Algorithm\nThe core innovation is the derivation of a joint likelihood function that formally accounts for the incomplete data structure. This joint likelihood allows for:\n\nArbitrary Missingness: The model is valid even when the pattern of missing omics data is complex (i.e., when data is missing at random).\nCensoring: It directly models the values that are below or above detection limits as censored observations, avoiding the need for simple (and often biased) imputation methods like replacing censored values with \\(\\frac{1}{2}\\) the detection limit.\n\nThe authors use an Expectation-Maximization (EM) algorithm to obtain maximum likelihood estimates of all model parameters, efficiently handling the latent (unobserved) censored and missing values.\n\n\nApplication: Emphysema and Blood Biomarkers\nThe method was applied to data from the SPIROMICS cohort, integrating genetic variants (SNPs), circulating blood biomarkers (proteins), and the phenotype emphysema status."
  },
  {
    "objectID": "multi-omics/lin_2020_32691502.html#key-results-and-findings",
    "href": "multi-omics/lin_2020_32691502.html#key-results-and-findings",
    "title": "A General Framework for Integrative Analysis of Incomplete Multi-omics Data",
    "section": "Key Results and Findings",
    "text": "Key Results and Findings\n\nSuperior Performance Over Imputation\nThe proposed method consistently demonstrated superior performance compared to standard practice methods that use ad-hoc imputation (e.g., replacing values below the detection limit with \\(\\frac{1}{2}\\) the limit) or methods that simply remove incomplete samples:\n\nReduced Bias: The likelihood-based approach yielded less biased estimates for the effects of biomarkers on emphysema.\nIncreased Power: The framework substantially increased the statistical power to detect associations, particularly when identifying protein quantitative trait loci (pQTLs), where many variants were discovered that were missed by imputation-based approaches.\nRobustness: The method proved robust in handling high rates of missingness and censoring, which are common in proteomic and metabolomic datasets."
  },
  {
    "objectID": "multi-omics/lin_2020_32691502.html#conclusions-and-significance",
    "href": "multi-omics/lin_2020_32691502.html#conclusions-and-significance",
    "title": "A General Framework for Integrative Analysis of Incomplete Multi-omics Data",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nThis paper presents a rigorous and powerful statistical solution for overcoming the challenges of missing data and detection limits in the integrative analysis of multi-omics datasets .\nBy developing a formal joint likelihood framework and using the EM algorithm, the method accurately models the relationships between genetic variants, omics features, and clinical phenotypes. This advancement is critical for improving the quality and interpretability of findings in large-scale multi-omics studies and accelerating the discovery of genetic determinants and molecular mediators of complex diseases."
  },
  {
    "objectID": "multi-omics/garg_2024_39261665.html",
    "href": "multi-omics/garg_2024_39261665.html",
    "title": "Disease prediction with multi-omics and biomarkers empowers case-control genetic discoveries in the UK Biobank",
    "section": "",
    "text": "PubMed: 39261665 DOI: 10.1038/s41588-024-01898-1 Overview generated by: Gemini 2.5 Flash, 27/11/2025"
  },
  {
    "objectID": "multi-omics/garg_2024_39261665.html#background-and-objective",
    "href": "multi-omics/garg_2024_39261665.html#background-and-objective",
    "title": "Disease prediction with multi-omics and biomarkers empowers case-control genetic discoveries in the UK Biobank",
    "section": "Background and Objective",
    "text": "Background and Objective\nBiobank-level datasets, such as the UK Biobank, offer unprecedented opportunities to discover novel biomarkers and develop powerful predictive algorithms for human disease. The challenge lies in effectively integrating diverse, multi-level data (genomics, proteomics, clinical records) to simultaneously improve disease prediction and the statistical power of genetic discovery.\nThis study introduces MILTON (Machine Learning with Phenotype Associations), an ensemble machine-learning framework designed to predict a wide range of diseases using multi-omics and clinical biomarkers. The main objective is to demonstrate how these accurate, biomarker-based predictions can augment case-control genetic association studies."
  },
  {
    "objectID": "multi-omics/garg_2024_39261665.html#methods-the-milton-framework",
    "href": "multi-omics/garg_2024_39261665.html#methods-the-milton-framework",
    "title": "Disease prediction with multi-omics and biomarkers empowers case-control genetic discoveries in the UK Biobank",
    "section": "Methods: The MILTON Framework",
    "text": "Methods: The MILTON Framework\nMILTON is an ensemble machine-learning framework that integrates diverse data types to predict disease status.\n\nData Integration: The framework was developed using the UK Biobank, integrating matched plasma proteomics data (from 46,327 samples) and other biomarkers with genetic data (from 484,230 genome-sequenced samples).\nPrediction Task: MILTON was trained to predict 3,213 incident disease cases—cases that were undiagnosed at the time of recruitment—by leveraging the UK Biobank’s longitudinal health record data.\nAugmenting Genetics: The highly accurate disease predictions from MILTON were then used to refine the case and control definitions in a phenome-wide association study (PheWAS). By improving disease classification through biomarker-based prediction, MILTON effectively enhances the statistical power of the genetic association analyses."
  },
  {
    "objectID": "multi-omics/garg_2024_39261665.html#key-results-and-significance",
    "href": "multi-omics/garg_2024_39261665.html#key-results-and-significance",
    "title": "Disease prediction with multi-omics and biomarkers empowers case-control genetic discoveries in the UK Biobank",
    "section": "Key Results and Significance",
    "text": "Key Results and Significance\nMILTON demonstrated substantial efficacy in both prediction and genetic discovery:\n\nSuperior Prediction: MILTON significantly outperformed available polygenic risk scores (PRSs) in predicting incident disease cases, especially for diseases with strong molecular links.\nEmpowered Genetic Discovery: When applied to the PheWAS, the framework successfully augmented genetic association analyses. This resulted in improved signals for 88 known genetic associations and led to the discovery of 14 novel genetic associations.\nTargeted Improvement: The framework showed the largest improvement in genetic discovery for diseases characterized by lower PRS prediction accuracy but higher biomarker prediction accuracy."
  },
  {
    "objectID": "multi-omics/garg_2024_39261665.html#conclusion",
    "href": "multi-omics/garg_2024_39261665.html#conclusion",
    "title": "Disease prediction with multi-omics and biomarkers empowers case-control genetic discoveries in the UK Biobank",
    "section": "Conclusion",
    "text": "Conclusion\nThe MILTON framework provides a powerful and practical approach to leverage deep molecular phenotyping, including multi-omics data, for disease prediction. Crucially, it demonstrates a successful strategy to empower case-control genetic discoveries by refining disease classification, which will accelerate the understanding of the underlying mechanisms of human diseases."
  },
  {
    "objectID": "metabolomics/do_2018_30830398.html",
    "href": "metabolomics/do_2018_30830398.html",
    "title": "Characterization of missing values in untargeted MS-based metabolomics data and evaluation of missing data handling strategies",
    "section": "",
    "text": "PubMed: 30830398 DOI: 10.1007/s11306-018-1420-2 Overview generated by: Gemini 2.5 Flash, 28/11/2025"
  },
  {
    "objectID": "metabolomics/do_2018_30830398.html#key-focus-the-problem-of-missing-values-in-untargeted-metabolomics",
    "href": "metabolomics/do_2018_30830398.html#key-focus-the-problem-of-missing-values-in-untargeted-metabolomics",
    "title": "Characterization of missing values in untargeted MS-based metabolomics data and evaluation of missing data handling strategies",
    "section": "Key Focus: The Problem of Missing Values in Untargeted Metabolomics",
    "text": "Key Focus: The Problem of Missing Values in Untargeted Metabolomics\nThis study provides a systematic characterization of missing values (MVs) in untargeted mass spectrometry (MS)-based metabolomics data and evaluates various strategies for handling these MVs. Missing data is a common issue that can severely reduce statistical power and introduce bias in downstream biomedical studies.\n\nCharacterization of Missing Values\nThe study distinguished two primary types of MVs based on their origin:\n\nMissing Due to Limits of Detection (LOD): This is the most prevalent form and is often systematic. It occurs when a compound’s concentration is below the instrument’s detection threshold. This systematic pattern can be further influenced by run day-dependent effects (e.g., changes in instrument performance over time).\nMissing at Random (MAR) or Completely at Random (MCAR): These MVs occur less frequently and are typically a consequence of random technical errors during sample preparation or measurement (e.g., ionization suppression, inconsistent retention time).\n\n\n\nEvaluation of Imputation Strategies\nThe study evaluated several common imputation strategies, including:\n\nFixed-Value Imputation: Replacing MVs with a fixed constant, such as zero, the mean, or a value derived from the limit of detection (LOD) (e.g., half the minimum detected value).\nData-Driven Imputation: Using statistical or machine learning methods based on the observed data, such as Probabilistic Principal Component Analysis (PPCA) or \\(k\\)-Nearest Neighbors (kNN).\n\n\n\nBest Performing Strategy\nThe key finding regarding MV handling was that the best strategy depends on the nature of the missingness:\n\nFor LOD-related MVs (Systematic Missingness): Simple methods like half-of-the-minimum-observed-value imputation performed surprisingly well and often outperformed more complex data-driven methods. This is because LOD-MVs are not truly random but represent a biological value that is near zero.\nFor MAR/MCAR MVs (Random Missingness): Data-driven methods like PPCA and kNN generally performed better than fixed-value methods. However, given that LOD-MVs dominate untargeted metabolomics, the overall benefit of complex methods was limited."
  },
  {
    "objectID": "metabolomics/do_2018_30830398.html#conclusions-and-recommendations",
    "href": "metabolomics/do_2018_30830398.html#conclusions-and-recommendations",
    "title": "Characterization of missing values in untargeted MS-based metabolomics data and evaluation of missing data handling strategies",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study emphasizes that the high proportion of systematic, LOD-related missingness is a characteristic feature of untargeted MS-based metabolomics.\nResearchers should: 1. Characterize Missingness: Visually inspect data to distinguish systematic LOD-MVs from random MVs. 2. Apply Targeted Imputation: For the high number of LOD-MVs, use a simple, robust method like imputation with a small value (e.g., half the minimum). 3. Future Development: The authors call for the development of new, tailored imputation methods that can explicitly and simultaneously model both the systematic (LOD) and random (MAR/MCAR) components of missingness in metabolomics data."
  },
  {
    "objectID": "metabolomics/makinen_2023_36823293.html",
    "href": "metabolomics/makinen_2023_36823293.html",
    "title": "Longitudinal metabolomics of increasing body-mass index and waist-hip ratio reveals two dynamic patterns of obesity",
    "section": "",
    "text": "PubMed: 36823293 DOI: 10.1038/s41366-023-01281-w Overview generated by: Gemini 2.5 Flash, 28/11/2025"
  },
  {
    "objectID": "metabolomics/makinen_2023_36823293.html#key-findings-two-distinct-metabolic-patterns-of-obesity",
    "href": "metabolomics/makinen_2023_36823293.html#key-findings-two-distinct-metabolic-patterns-of-obesity",
    "title": "Longitudinal metabolomics of increasing body-mass index and waist-hip ratio reveals two dynamic patterns of obesity",
    "section": "Key Findings: Two Distinct Metabolic Patterns of Obesity",
    "text": "Key Findings: Two Distinct Metabolic Patterns of Obesity\nThis longitudinal metabolomics study dissected the complex temporal associations between Body-Mass Index (BMI), Waist-Hip Ratio (WHR), and the circulating metabolome. The core finding is that there are two dynamically different metabolic patterns associated with increasing obesity over time, which are strongly influenced by the specific obesity metric used (BMI vs. WHR).\n\n1. BMI Trajectories: Metabolic Health vs. Risk\nThe longitudinal analysis (using the NFBC1966 cohort with data spanning 15 years) identified two main metabolic patterns related to BMI:\n\n“Metabolically Healthy” BMI: An increase in BMI that was not accompanied by an increase in fatty acid (FA) saturation (the degree to which FAs are saturated) was associated with a favorable lipid profile, better glucose metabolism, and lower inflammatory markers.\n“High-Risk” BMI: An increase in BMI that was accompanied by an increase in FA saturation was linked to an adverse metabolic profile, including elevated very-low-density lipoprotein (VLDL) and low-density lipoprotein (LDL) cholesterol, increased insulin resistance, and higher inflammation markers.\n\n\n\n2. WHR Trajectories: Highly Adverse Profile\nIn contrast to BMI, an increase in Waist-Hip Ratio (WHR) over time was uniformly associated with a single, highly adverse metabolic profile, regardless of the accompanying changes in FA saturation. WHR, a measure of central adiposity, was linked to: * Significantly worse glucose metabolism (insulin resistance). * Higher levels of inflammatory markers. * Higher concentrations of amino acids (specifically branched-chain amino acids, BCAA), which are known markers of insulin resistance."
  },
  {
    "objectID": "metabolomics/makinen_2023_36823293.html#methods-and-study-design",
    "href": "metabolomics/makinen_2023_36823293.html#methods-and-study-design",
    "title": "Longitudinal metabolomics of increasing body-mass index and waist-hip ratio reveals two dynamic patterns of obesity",
    "section": "Methods and Study Design",
    "text": "Methods and Study Design\n\nCohorts and Data\n\nLongitudinal Cohorts: Northern Finland Birth Cohort (NFBC1966, n=3,117) with two time points (age 31 and 46), allowing for the calculation of BMI and WHR trajectories (changes over time).\nCross-sectional Cohort: FINRISK (n=9,708) for initial data-driven subgrouping.\nMetabolomics: Quantification of 174 circulating metabolic measures (including lipids, fatty acids, and amino acids) using proton nuclear magnetic resonance (\\(^1\\)H-NMR) spectroscopy.\n\n\n\nSystems Epidemiology Tools\n\nSelf-Organizing Map (SOM): An unsupervised machine learning algorithm used on the cross-sectional data to simplify the high-dimensional metabolomics data into four discrete, biologically interpretable metabolic subgroups (A, B, C, D).\nLongitudinal Modeling: The study used the continuous trajectory of BMI and WHR, along with the metabolomic subgroups, to identify the specific metabolic changes associated with the development of obesity over the 15-year follow-up."
  },
  {
    "objectID": "metabolomics/makinen_2023_36823293.html#conclusions-and-implications",
    "href": "metabolomics/makinen_2023_36823293.html#conclusions-and-implications",
    "title": "Longitudinal metabolomics of increasing body-mass index and waist-hip ratio reveals two dynamic patterns of obesity",
    "section": "Conclusions and Implications",
    "text": "Conclusions and Implications\nThe study demonstrates that obesity is not a metabolically uniform state. The use of longitudinal metabolomics and systems epidemiology tools (like SOMs) is essential for dissecting the heterogeneity of obesity.\nThe key clinical implication is that WHR and BMI trajectories should be viewed as distinct risk factors: * WHR (central adiposity) is a more consistent marker of a severe, underlying adverse metabolic risk. * BMI alone may mask distinct metabolic phenotypes, and its risk assessment should be refined by incorporating the associated fatty acid saturation profile.\nThe findings support moving beyond simple single-time-point measurements to focus on dynamic changes in metabolic profiles for personalized risk assessment and intervention in obesity-related diseases."
  },
  {
    "objectID": "metabolomics/desouza_2020_32380880.html",
    "href": "metabolomics/desouza_2020_32380880.html",
    "title": "Network-based strategies in metabolomics data analysis and interpretation: from molecular networking to biological interpretation",
    "section": "",
    "text": "PubMed: 32380880 DOI: 10.1080/14789450.2020.1766975 Overview generated by: Gemini 2.5 Flash, 28/11/2025"
  },
  {
    "objectID": "metabolomics/desouza_2020_32380880.html#key-focus-network-based-approaches-for-metabolomics-data",
    "href": "metabolomics/desouza_2020_32380880.html#key-focus-network-based-approaches-for-metabolomics-data",
    "title": "Network-based strategies in metabolomics data analysis and interpretation: from molecular networking to biological interpretation",
    "section": "Key Focus: Network-Based Approaches for Metabolomics Data",
    "text": "Key Focus: Network-Based Approaches for Metabolomics Data\nThis review article focuses on network-based strategies as essential computational tools for the analysis and interpretation of complex metabolomics data. It highlights how these methods move beyond simple compound identification and quantification to provide crucial insights into metabolic pathways and biological context.\n\nThe Challenge and the Solution\n\nChallenge: Metabolomics datasets, especially those generated by untargeted Mass Spectrometry (MS), are vast and complex, containing numerous features (ions) that are often challenging to identify, map to biological functions, and integrate into a cohesive biological understanding.\nSolution: Network analysis offers a powerful way to organize these complex datasets into visual and interpretable structures, revealing relationships between metabolites based on either chemical similarity (Molecular Networking) or biological correlation (Metabolic Network Reconstruction)."
  },
  {
    "objectID": "metabolomics/desouza_2020_32380880.html#network-strategies-and-methods",
    "href": "metabolomics/desouza_2020_32380880.html#network-strategies-and-methods",
    "title": "Network-based strategies in metabolomics data analysis and interpretation: from molecular networking to biological interpretation",
    "section": "Network Strategies and Methods",
    "text": "Network Strategies and Methods\nThe review divides network strategies into two main categories:\n\n1. Molecular Networking (MN)\n\nGoal: To organize MS/MS spectra of unknown compounds based on chemical structural similarity.\nMechanism: MN algorithms cluster compounds whose fragmentation spectra suggest they are structurally related (e.g., belong to the same chemical family or pathway), thereby facilitating the identification of unknown compounds in a large family once a single member is characterized.\nAdvanced Tools: The use of complementary tools like MolNetEnhancer is highlighted, which combines MN with chemical classification tools to provide structural family enrichment and better chemical context.\n\n\n\n2. Metabolic Network Reconstruction\n\nGoal: To infer functional and biological relationships between metabolites.\nCorrelation Networks: These networks use statistical correlation between metabolite abundance levels across different samples (or time points) to infer shared regulation or sequential steps in a metabolic pathway.\nBiological Mapping: The ultimate aim is to map the identified metabolites and their relationships onto established biochemical pathways (e.g., KEGG, Reactome) to gain biological meaning and contextualize changes observed in response to a perturbation or disease state."
  },
  {
    "objectID": "metabolomics/desouza_2020_32380880.html#applications-and-importance",
    "href": "metabolomics/desouza_2020_32380880.html#applications-and-importance",
    "title": "Network-based strategies in metabolomics data analysis and interpretation: from molecular networking to biological interpretation",
    "section": "Applications and Importance",
    "text": "Applications and Importance\nNetwork-based strategies are crucial for several areas in metabolomics:\n\nUnknown Metabolite Identification: MN is especially valuable in natural product research and untargeted metabolomics for annotating large numbers of unknown spectral features.\nData Reduction and Visualization: Networks simplify highly dimensional data into intuitive visual representations that highlight key regulatory hubs or pathways.\nIntegration with Other Omics: The network approach facilitates the integration of metabolomics data with genomics, transcriptomics, and proteomics by mapping different molecular layers onto shared biological pathways."
  },
  {
    "objectID": "metabolomics/desouza_2020_32380880.html#conclusion",
    "href": "metabolomics/desouza_2020_32380880.html#conclusion",
    "title": "Network-based strategies in metabolomics data analysis and interpretation: from molecular networking to biological interpretation",
    "section": "Conclusion",
    "text": "Conclusion\nThe review concludes that network-based approaches are evolving rapidly, moving from basic visualization tools to sophisticated platforms for chemical and biological interpretation. They are indispensable for handling the inherent complexity of the metabolome and translating raw data into meaningful biological insights."
  },
  {
    "objectID": "proteomics.html",
    "href": "proteomics.html",
    "title": "Proteomics",
    "section": "",
    "text": "Current landscape of plasma proteomics from technical innovations to biological insights and biomarker discovery\n\nTopic: A systematic benchmarking and technical evaluation of the current landscape of plasma proteomics, directly comparing different affinity-based and mass spectrometry platforms.\nMethod: Eight state-of-the-art proteomics platforms were applied to the same human plasma cohort to compare performance across over 13,000 proteins, assessing metrics like coverage, sensitivity, and reproducibility.\nImpact: The study provides a critical resource for researchers, detailing the trade-offs in protein coverage and identifying the complementary strengths and limitations of each platform to guide informed decision-making for biomarker discovery.\n\n\n\nA human pan-disease blood atlas of the circulating proteome\n\nTopic: Creation of a definitive human pan-disease blood atlas of the circulating proteome to discover and validate protein biomarkers and develop predictive algorithms across a wide spectrum of human diseases.\nMethods: Utilized large-scale human cohorts and benchmarked high-throughput affinity-based proteomics, specifically the Olink Explore 1536/HT platforms, to profile thousands of proteins. The data was analyzed using machine learning for disease prediction and to assess protein profiles relative to time-to-diagnosis (temporal analysis) for major cancers (lung, colorectal, breast, ovarian, and prostate).\nKey Findings & Impact: The study confirmed the utility of circulating proteins (e.g., PAEP, CEACAM5, WFDC2) as robust biomarkers. The temporal analysis demonstrated the potential for predicting disease onset years before clinical diagnosis. The resulting atlas is a critical resource for developing next-generation diagnostics and therapeutics.\n\n\n\nPWAS: proteome-wide association study-linking genes and phenotypes by functional variation in proteins\n\nNovel Method: Introduces PWAS (Proteome-Wide Association Study), a protein-centric method that detects gene-phenotype associations by quantifying the cumulative functional damage caused by coding-region variants on the resulting protein product using a machine learning model called FIRM.\nKey Advantage: PWAS is specifically designed to model and detect non-additive heritability, demonstrating its power in identifying associations under the recessive inheritance model, which is often missed by standard GWAS.\nDiscovery Power: In analysis using the UK Biobank, PWAS uncovered numerous gene-phenotype associations unique from standard GWAS, including detecting the known colorectal cancer gene MUTYH with high significance under its characteristic recessive mode.\n\n\n\nDecoding the functional impact of the cancer genome through protein-protein interactions\n\nCore Concept: This review establishes that oncogenic driver mutations exert their functional impact largely by rewiring molecular signaling networks through the alteration of Protein-Protein Interactions (PPIs).\nMechanism: Mutations at the protein surface, particularly at PPI interfaces, can lead to the creation of neomorphic PPIs (neoPPIs) or the loss of existing hypomorphic PPIs (hypoPPIs), necessitating a mutation-focused analysis.\nTherapeutic Implication: The mutation-directed PPIs are presented as a new class of targets for precision oncology, paving the way for the development of small-molecule modulators that can selectively disrupt oncogenic neoPPIs or restore tumor-suppressive hypoPPIs.\n\n\n\nLongitudinal multi-omics study reveals common etiology underlying association between plasma proteome and BMI trajectories in adolescent and young adult twins\n\nDesign: A longitudinal multi-omics twin study (NTR and FinnTwin12 cohorts) examining the association between plasma protein levels and changes in BMI (trajectories) over approximately a decade.\nKey Finding: The association between the plasma proteome and BMI trajectories is largely explained by shared genetic factors and common environmental influences, pointing to a common underlying etiology.\nCausal Link: Mendelian Randomization analysis identified Apolipoprotein B (ApoB) as a key protein, providing evidence that genetically determined ApoB levels have a causal effect on BMI, but not the reverse.\n\n\n\nProteomic signatures improve risk prediction for common and rare diseases\n\nObjective: This large-scale study demonstrated the ability of plasma proteomic signatures to enhance the 10-year incidence risk prediction for 218 common and rare diseases in the UK Biobank (UKB-PPP) cohort.\nResult: Sparse proteomic models (using 5–20 proteins) significantly improved the C-index over models based on basic clinical information for 67 diseases, including hard-to-diagnose conditions like multiple myeloma and motor neuron disease.\nRobustness/Confounding: The analysis highlighted that residual confounding is a major issue, as over 80% of initial associations attenuated after adjusting for demographic and clinical factors, underscoring the necessity of understanding protein determinants to ensure findings are biologically relevant and not spurious.\n\n\n\nGenetic associations with ratios between protein levels detect new pQTLs and reveal protein-protein interactions\n\nNovel Method: Introduces Ratio-QTLs (rQTLs), a proteogenomic method that analyzes the genetic determinants of ratios between pairs of plasma protein levels rather than individual protein levels.\nKey Finding: The rQTL approach provided an enormous increase in statistical power, strengthening associations at known pQTL loci by several hundred orders of magnitude (p-gain) and enabling the discovery of new cis-pQTLs.\nBiological Relevance: rQTLs were 7.6-fold enriched in established Protein-Protein Interactions (PPIs), confirming the method’s unique ability to uncover genetic variants that regulate the functional interaction or shared biological regulation between two proteins.\n\n\n\nMachine learning-guided deconvolution of plasma protein levels\n\nObjective: Used machine learning (ML) to systematically identify and quantify the contribution of over 1,800 characteristics (health, genetic, technical) to the variation in approximately 3,000 plasma protein levels across 43,240 UK Biobank individuals.\nKey Result: A median of 20 factors explained an average of 19.4% of protein variance. Modifiable characteristics (median: 10.0%) were found to explain significantly more variation than genetic factors (median: 3.9%).\nImplication: The study provides a crucial resource (knowledge graph, R package) and framework for understanding protein origins, clustering proteins by their drivers (e.g., disease, pre-analytical factors), and guiding the identification of biologically relevant biomarkers and drug target engagement markers.\n\n\n\nBlood protein assessment of leading incident diseases and mortality in the UK Biobank\n\nObjective: To identify and validate protein biomarkers for the 10-year incidence of 23 age-related diseases and all-cause mortality using proteomics data from 47,600 UK Biobank participants.\nKey Result: Multi-protein risk scores (ProteinScores), developed using penalized Cox regression, significantly improved the Area Under the Curve (AUC) for the 10-year onset prediction of six major outcomes, including all-cause mortality, coronary artery disease (CAD), and Type 2 diabetes (T2D), even after adjusting for 24 comprehensive clinical and lifestyle factors.\nImplication: The ProteinScores capture independent biological information related to underlying aging and systemic disease risk not found in standard clinical measures, validating the use of multi-protein panels for enhanced personalized risk stratification.\n\n\n\nMapping the proteo-genomic convergence of human diseases\n\nObjective: To construct a proteo-genomic map by systematically linking genetic risk for hundreds of human diseases and traits to variations in the levels of ~3,000 plasma proteins using data from &gt;54,000 individuals.\nKey Result: Identified 2,228 instances of genetic colocalization across 1,440 proteins and 498 diseases/traits, indicating that a substantial portion of genetic disease risk is mediated by altered protein levels.\nCausal Inference: Through Mendelian Randomization (MR), the study robustly predicted 44 proteins to be causally linked to 37 diseases/traits (e.g., CRP and CAD), thereby providing high-priority candidates for drug targets."
  },
  {
    "objectID": "pgs.html",
    "href": "pgs.html",
    "title": "polygenic scores",
    "section": "",
    "text": "Genetic prediction of complex traits with polygenic scores: A statistical review\n\nThis statistical review comprehensively analyzes 46 methods for Polygenic Score (PGS) construction, unifying most of them under a multiple linear regression framework to clarify their assumptions regarding effect size distribution and Linkage Disequilibrium (LD).\nThe review concludes that optimal PGS performance (accuracy) is highly dependent on the trait’s genetic architecture and is significantly improved by using Bayesian/Regularization methods (e.g., LDpred, PRS-CS) that explicitly model LD and incorporate informed prior distributions for SNP effects.\nA critical challenge highlighted is the significant loss of transferability across ancestral populations, underscoring the need for more diverse training data and methods that better address ancestral heterogeneity and incorporate non-additive and Gene-by-Environment (GxE) effects.\n\n\n\nGenome-wide risk prediction of common diseases across ancestries in one million people\n\nThis large-scale study evaluated the cross-ancestry transferability of Polygenic Risk Scores (PRSs) for four common diseases (CAD, T2D, breast, and prostate cancer) using data from six biobanks and over one million individuals of diverse global ancestries.\nThe analysis found that PRS transferability was high and robust across different populations and substructures of European ancestry, but was significantly lower for individuals of African, South Asian, and East Asian ancestry.\nThe poor transferability, which was most pronounced in African ancestry individuals, highlights the critical issue of ancestral bias in genomic research and the potential for current PRS implementation to exacerbate health disparities until more diverse training data are available.\n\n\n\nPolygenic scoring accuracy varies across the genetic ancestry continuum\n\nCore Principle: This landmark study challenged the traditional use of discrete ancestry groups for polygenic scores (PGSs), proposing a framework that evaluates individual-level PGS accuracy based on Genetic Distance (GD) from the GWAS training population.\nKey Finding: They demonstrated that individual-level PGS accuracy experiences a continuous, steep decay as GD from the training data increases, with an average Pearson correlation of R = -0.95 across 84 complex traits, confirming that performance loss is a predictable function of genetic dissimilarity.\nHealth Equity Implication: The study quantified a major health disparity, showing that the genetically closest individuals of non-European ancestry (e.g., Hispanic/Latino American) had PGS accuracy comparable to the most distant European-ancestry individuals, highlighting the severe and systematic bias due to lack of diversity in training cohorts.\nRecommendation: The authors advocate for moving beyond discrete ancestry labels and using continuous metrics like GD to characterize and correct for performance disparities, thus ensuring more equitable clinical translation of PGSs.\n\n\n\nDefining type 2 diabetes polygenic risk scores through colocalization and network-based clustering of metabolic trait genetic associations\n\nCore Principle: This study partitioned the genetic heterogeneity of Type 2 Diabetes (T2D) using a novel colocalization-first approach followed by network-based clustering of T2D and 20 related metabolic traits across 243 loci.\nKey Finding: The method identified five distinct T2D biological pathways (Obesity, Lipodystrophic insulin resistance, Liver/lipid metabolism, Hepatic glucose metabolism, and Beta-cell dysfunction), successfully isolating genetically distinct disease mechanisms.\nClinical Significance: Partitioned Polygenic Risk Scores (PRSs) showed heterogeneous clinical associations in a validation cohort (n=21,742 T2D individuals); notably, the Lipodystrophic insulin resistance PRS and Beta-cell dysfunction PRS were causally associated with lower BMI, providing genetic validation for the clinically important “lean diabetes” sub-type.\nMethodological Advance: By integrating colocalization and Mendelian Randomization, the framework provided stronger inferences on the causality and directionality of the genetic associations, which is essential for translating genetic discoveries into targeted T2D treatments."
  },
  {
    "objectID": "pgs/ma_2021_34243982.html",
    "href": "pgs/ma_2021_34243982.html",
    "title": "Genetic prediction of complex traits with polygenic scores: A statistical review",
    "section": "",
    "text": "PubMed: 34243982\nDOI: 10.1016/j.tig.2021.06.004\nOverview generated by: Gemini 2.5 Flash, 26/11/2025"
  },
  {
    "objectID": "pgs/ma_2021_34243982.html#key-findings",
    "href": "pgs/ma_2021_34243982.html#key-findings",
    "title": "Genetic prediction of complex traits with polygenic scores: A statistical review",
    "section": "Key Findings",
    "text": "Key Findings\nThis comprehensive statistical review provides an exhaustive analysis of the landscape of Polygenic Score (PGS) methods, a critical area in human genetics focused on predicting complex traits and disease risk using genetic data. The authors review 46 different methods for PGS construction, establishing a unifying multiple linear regression framework to connect and categorize the majority of these techniques. The core conclusion is that the optimal PGS method is highly dependent on the genetic architecture of the target trait (e.g., polygenicity, effect size distribution) and the nature of the available training and target data. The review serves as an essential reference, providing both the statistical underpinnings for method developers and practical guidance (including a decision tree) for analysts performing PGS analysis in clinical and research settings."
  },
  {
    "objectID": "pgs/ma_2021_34243982.html#categorization-and-statistical-framework",
    "href": "pgs/ma_2021_34243982.html#categorization-and-statistical-framework",
    "title": "Genetic prediction of complex traits with polygenic scores: A statistical review",
    "section": "Categorization and Statistical Framework",
    "text": "Categorization and Statistical Framework\nThe review structures the diverse landscape of PGS methods into a coherent statistical framework, primarily centered on how they estimate the SNP effect sizes, which serve as the weights for the Polygenic Score.\n\nCore PGS Calculation\nIn its simplest form, the PGS for an individual (\\(i\\)) is a weighted sum of their genotypes across \\(M\\) single nucleotide polymorphisms (SNPs): \\[\\text{PGS}_i = \\sum_{j=1}^{M} G_{ij} \\hat{\\beta}_j\\] where \\(G_{ij}\\) is the genotype of individual \\(i\\) at SNP \\(j\\), and \\(\\hat{\\beta}_j\\) is the estimated genetic effect size (the weight) for that SNP.\n\n\nUnifying Multiple Linear Regression Framework\nThe authors classify most modern PGS methods as variants of a general multiple linear regression model: \\[\\mathbf{Y} = \\mathbf{G}\\mathbf{\\beta} + \\mathbf{\\epsilon}\\] where \\(\\mathbf{Y}\\) is the phenotype vector, \\(\\mathbf{G}\\) is the genotype matrix, \\(\\mathbf{\\beta}\\) is the vector of true genetic effects, and \\(\\mathbf{\\epsilon}\\) is the error term. Different PGS methods (e.g., LDpred, S-Bayes) are distinguished by their assumptions about the effect size vector, \\(\\mathbf{\\beta}\\), and how they account for the correlation structure of the genotypes (Linkage Disequilibrium or LD).\n\n\nClassification of PGS Methods\nThe review categorizes the 46 analyzed methods based on their underlying statistical approach:\n\nClassical Methods: These include P-value Thresholding and Clumping (P+T), which selects a subset of independent SNPs based on their p-values. This remains a simple and surprisingly effective baseline method.\nBayesian and Regularization Methods: These methods explicitly model the genetic architecture by incorporating prior distributions on the SNP effect sizes (\\(\\mathbf{\\beta}\\)) and/or regularizing the estimates to account for noise and LD. Examples include:\n\nLDpred/LDpred2: Assumes a fraction of SNPs are causal and uses an LD reference panel to shrink effect sizes.\nBayesC/BayesR: Assumes effect sizes come from a mixture of normal distributions, allowing for a few large effects and many small ones.\nLasso/Ridge Regression: Uses penalization to optimize effect size selection and estimation.\n\nSummary Statistics-Based Methods: Methods that operate entirely on GWAS summary statistics and an external LD reference panel (e.g., LDpred, S-Bayes, PRS-CS). These are the most computationally efficient and widely used due to data sharing conventions.\nMeta-Dimensional Methods: These include methods that incorporate information beyond SNP effects, such as functional annotations or gene expression data (e.g., MetaXcan)."
  },
  {
    "objectID": "pgs/ma_2021_34243982.html#practical-considerations-and-performance",
    "href": "pgs/ma_2021_34243982.html#practical-considerations-and-performance",
    "title": "Genetic prediction of complex traits with polygenic scores: A statistical review",
    "section": "Practical Considerations and Performance",
    "text": "Practical Considerations and Performance\n\nFactors Affecting PGS Performance\nThe prediction accuracy of a PGS (typically measured by the coefficient of determination, \\(R^2\\)) is highly sensitive to several factors:\n\nGenetic Architecture: Methods that accurately model the true genetic architecture (e.g., high polygenicity vs. oligenicity) perform best. Bayesian methods often excel because they flexibly model the prior distribution of effect sizes.\nTraining Sample Size: Performance is highly dependent on the size of the GWAS used to estimate \\(\\hat{\\beta}\\). Larger sample sizes generally lead to more accurate effect size estimates and thus better PGS performance.\nAncestral Divergence: Prediction accuracy significantly drops when the target population ancestry differs substantially from the training population (e.g., GWAS conducted primarily in European populations but applied to African populations). This highlights the critical issue of transferability and health equity.\nLinkage Disequilibrium (LD) Modeling: Explicitly accounting for LD, such as in LDpred and PRS-CS, is crucial for improving prediction accuracy compared to simple methods like P+T.\n\n\n\nDecision Tree for PGS Analysis\nThe review provides a practical decision tree to help researchers select the appropriate PGS method based on the available data and research question:\nKey branches include: * Input Data: Is individual-level data or only GWAS summary data available? * Trait Complexity: Analyzing a single trait or multiple correlated traits? * Modeling Approach: Choosing between model-based (e.g., linear mixed models) and algorithm-based (e.g., machine learning) methods."
  },
  {
    "objectID": "pgs/ma_2021_34243982.html#challenges-and-future-directions",
    "href": "pgs/ma_2021_34243982.html#challenges-and-future-directions",
    "title": "Genetic prediction of complex traits with polygenic scores: A statistical review",
    "section": "Challenges and Future Directions",
    "text": "Challenges and Future Directions\nThe authors identify several major challenges that need to be addressed to realize the full clinical potential of PGS:\n\nAddressing Heterogeneity (Ancestry): Developing methods that maintain high predictive accuracy across diverse ancestral populations and that can properly perform multi-ancestry meta-analysis to generate universally accurate PGS.\nNon-Additive and GxE Effects: Incorporating non-additive genetic effects (dominance, epistasis) and Gene-by-Environment (GxE) interactions into the PGS model. Current models are largely additive and thus leave substantial variance unexplained.\nCausal Variants and Fine-Mapping: Moving beyond marker SNPs to accurately identify and weight the true causal variants to improve biological relevance and prediction.\nClinical Implementation: Establishing standardized reporting guidelines, validating PGS in independent clinical cohorts, and addressing the ethical concerns related to using PGS in personalized medicine."
  },
  {
    "objectID": "pgs/ghatan_2024_38200577.html",
    "href": "pgs/ghatan_2024_38200577.html",
    "title": "Defining type 2 diabetes polygenic risk scores through colocalization and network-based clustering of metabolic trait genetic associations",
    "section": "",
    "text": "PubMed: 38200577 DOI: 10.1186/s13073-023-01255-7 Overview generated by: Gemini 2.5 Flash, 26/11/2025"
  },
  {
    "objectID": "pgs/ghatan_2024_38200577.html#key-findings-dissecting-t2d-heterogeneity-via-pleiotropy",
    "href": "pgs/ghatan_2024_38200577.html#key-findings-dissecting-t2d-heterogeneity-via-pleiotropy",
    "title": "Defining type 2 diabetes polygenic risk scores through colocalization and network-based clustering of metabolic trait genetic associations",
    "section": "Key Findings: Dissecting T2D Heterogeneity via Pleiotropy",
    "text": "Key Findings: Dissecting T2D Heterogeneity via Pleiotropy\nThis study addresses the profound genetic and clinical heterogeneity of Type 2 Diabetes (T2D) by developing a refined framework to partition T2D-associated genetic variants into distinct biological pathways. The goal is to move beyond a single, monolithic T2D diagnosis toward stratified risk prediction and targeted therapeutic strategies. The method leverages the pleiotropic nature of genetic variants—their influence on multiple related traits—to define distinct mechanisms of T2D pathogenesis.\n\nColocalization-First Partitioning and Clustering\nThe authors improved upon previous clustering approaches by integrating rigorous statistical checks for shared causality, enhancing the mechanistic interpretability of the resulting risk scores.\n\nColocalization Analysis: They applied colocalization analysis between T2D and 20 related metabolic traits (selected based on established risk factors and genetic correlation) across 243 T2D loci. This step robustly identified 146 T2D loci where the T2D association was likely caused by the same causal variant as the associated metabolic trait.\nNetwork-Based Clustering: A network-based unsupervised hierarchical clustering approach was then performed using the colocalized variant-trait associations. This successfully grouped the T2D risk loci into five distinct clusters, each representing a unique, interconnected set of T2D and metabolic risk factors.\nCausality Check (Mendelian Randomization): The study explicitly assessed the causality and directionality of the variant-trait associations using the Mendelian randomization (MR) Steiger’s Z-test. This confirmed that the genetic associations identified in the clusters were largely causal for the corresponding metabolic phenotypes.\n\n\n\nFive Distinct T2D Pathophysiological Clusters\nThe five identified genetic clusters, which align with distinct T2D pathophysiologies, are: 1. Obesity (High BMI-T2D risk) 2. Lipodystrophic insulin resistance (T2D risk associated with fat distribution/dysfunction) 3. Liver and lipid metabolism 4. Hepatic glucose metabolism 5. Beta-cell dysfunction (Impaired insulin secretion)\n\n\nHeterogeneous Clinical Profiles and “Lean Diabetes”\nPartitioned Polygenic Risk Scores (PRSs) were generated for each cluster and externally validated in 21,742 individuals with T2D across three independent cohorts, demonstrating unique associations with metabolic and clinical outcomes:\n\nOpposite BMI Associations: The Obesity PRS was strongly associated with a higher BMI, as expected. Critically, the Lipodystrophic insulin resistance PRS and Beta-cell dysfunction PRS were both associated with lower BMI.\nSupport for Lean Diabetes: The MR Steiger analysis provided causal evidence that increased T2D risk in the lipodystrophic insulin resistance and beta-cell dysfunction pathways was causally associated with lower BMI. This provides a genetic foundation for the “lean diabetes” or non-obese T2D sub-type, where risk is driven by dysfunctional fat/insulin-secretion rather than overall fat mass.\nComorbidity Stratification: The Lipodystrophic insulin resistance PRS was uniquely and specifically associated with a higher odds of chronic kidney disease (CKD) (Odds Ratio 1.29), suggesting that individuals whose T2D risk is driven by this pathway may require specific monitoring and intervention for renal complications.\n\n\n\nConclusion\nThe colocalization-first, network-based clustering methodology successfully and robustly partitioned the genetic heterogeneity underlying T2D. The resulting pathway-specific PRSs provide valuable tools for risk stratification, sub-type identification, and the development of targeted therapies based on an individual’s distinct genetic mechanism of disease."
  },
  {
    "objectID": "metabolomics.html",
    "href": "metabolomics.html",
    "title": "metabolomics",
    "section": "",
    "text": "Network-based strategies in metabolomics data analysis and interpretation: from molecular networking to biological interpretation\n\nFocus: This review highlights the use of network-based strategies as essential tools for analyzing and interpreting complex metabolomics data, especially from untargeted Mass Spectrometry (MS).\nMolecular Networking (MN): Used to organize MS/MS spectral data based on chemical structural similarity, thereby facilitating the identification of unknown compounds in chemical families.\nMetabolic Networks: Constructed using statistical correlation between metabolite abundance levels to infer functional and biological relationships, enabling mapping onto known biochemical pathways for biological contextualization.\n\n\n\nLongitudinal metabolomics of increasing body-mass index and waist-hip ratio reveals two dynamic patterns of obesity\n\nObjective: This longitudinal study used metabolomics and systems epidemiology tools (Self-Organizing Map, SOM) on over 12,800 participants to dissect the complex temporal associations between Body-Mass Index (BMI), Waist-Hip Ratio (WHR), and the circulating metabolome.\nKey Finding: The study revealed two dynamically different metabolic patterns for increasing obesity:\n\nAn increase in BMI that was not accompanied by an increase in fatty acid (FA) saturation was associated with a relatively favorable metabolic profile.\nAn increase in WHR (central adiposity) was uniformly associated with a single, highly adverse metabolic profile, characterized by worse glucose metabolism, inflammation, and high amino acid levels.\n\nImplication: The results suggest that obesity is metabolically heterogeneous and that WHR is a more consistent marker of severe adverse metabolic risk than BMI alone, emphasizing the need to consider dynamic metabolic changes for personalized risk assessment.\n\n\n\nCharacterization of missing values in untargeted MS-based metabolomics data and evaluation of missing data handling strategies\n\nObjective: The study systematically characterized the sources of missing values (MVs) in untargeted Mass Spectrometry (MS)-based metabolomics data and evaluated various imputation strategies.\nMissing Value Types: Distinguished between systematic missingness primarily due to Limits of Detection (LOD) and random missingness due to technical issues.\nBest Strategy: For the prevalent LOD-related MVs, which represent concentrations near zero, simple methods like imputation with half of the minimum observed value were found to be effective and often outperformed complex data-driven methods (e.g., PPCA).\nRecommendation: Researchers should use targeted imputation strategies based on the nature of the missingness to avoid introducing bias and reducing statistical power.\n\n\n\nNetworks and Graphs Discovery in Metabolomics Data Analysis and Interpretation\n\nFocus: This review highlights the use of networks and graph theory as powerful computational tools for analyzing and interpreting complex metabolomics data.\nNetwork Types:\n\nAnalytical/Chemical Networks: Derived from Mass Spectrometry (MS) data, such as Molecular Networking (MN), which connects spectra based on chemical structural similarity to aid in compound identification.\nBiological/Correlation Networks: Derived from quantitative metabolite abundance data. These networks use statistical correlation between metabolites to infer shared biological regulation, map metabolites onto known biochemical pathways, and facilitate multi-omics integration (connecting metabolites to genes/proteins).\n\nImplication: Graph theory enables researchers to move beyond simple lists of metabolites to view the metabolome as a structured, interactive system, which is essential for biological interpretation and hypothesis generation.\n\n\n\nBest practices and tools in R and Python for statistical processing and visualization of lipidomics and metabolomics data\n\nObjective: This review compiles best practices and freely accessible tools in R and Python for the statistical processing and visualization of extensive mass spectrometry-based lipidomics and metabolomics data.\nFocus: The article provides a “solid core” of resources for exploratory data analysis (EDA) and visualization to help researchers identify and visualize statistically significant trends and biologically relevant differences within their complex datasets.\nImplication: It guides researchers on using modern computational platforms (R/Python) and integrating metadata (e.g., clinical parameters) with their omics data to perform robust and reproducible downstream analysis.\n\n\n\nAltered metabolite levels in cancer: implications for tumour biology and cancer therapy\n\nFocus: This review examines how altered intracellular metabolite concentrations in cancer cells, often driven by genetic mutations, actively promote tumor initiation and progression, moving beyond the idea that metabolic changes are merely a consequence of cancer.\nOncometabolites: Specific metabolites act as effector molecules:\n\n2-Hydroxyglutarate (2-HG): Produced by IDH1/2 mutations, it inhibits epigenetic regulators (like TET enzymes) leading to globally altered gene expression and oncogenesis.\nFumarate and Succinate: Accumulate due to FH/SDH mutations, leading to the stabilization of HIF-\\(1\\alpha\\) (pseudohypoxia), which drives proliferation and the Warburg effect.\n\nImplication: The altered metabolome is a rich source of therapeutic targets. Strategies can focus on counteracting the effects of oncometabolites or exploiting the metabolic dependencies created by these shifts (e.g., limited aspartate for nucleotide synthesis)."
  },
  {
    "objectID": "interaction.html",
    "href": "interaction.html",
    "title": "interaction",
    "section": "",
    "text": "PIGEON: a statistical framework for estimating gene-environment interaction for polygenic traits\n\nPIGEON Framework: The paper introduces PIGEON, a unified statistical framework that uses a variance component analytical approach to quantify polygenic Gene-Environment Interaction (GxE).\nInput and Scalability: PIGEON is highly scalable as its estimation procedure requires only GWAS and GWIS summary statistics (not individual-level data).\nKey Objectives: The framework provides rigorous methods for both detecting the presence of GxE (by estimating GxE variance) and interpreting its mechanism (by estimating Oracle PGSxE), demonstrating its utility across multiple empirical settings including gene-by-sex and gene-by-education studies.\n\n\n\nPathway polygenic risk scores (pPRS) for the analysis of gene-environment interaction\n\nNovel Method: The paper introduces Pathway Polygenic Risk Scores (pPRS), a targeted method for GxE analysis that restricts genetic variants to specific, biologically informed genomic pathways.\nIncreased Power: Simulations and empirical analysis demonstrated that pPRS yields substantially greater statistical power to detect true GxE interactions compared to using a standard, overall PRS.\nEmpirical Example: The method identified a significant interaction between pPRS based on the TGF-\\(\\beta\\)/GRHR pathway and NSAID use for colorectal cancer (CRC) risk, suggesting the strongest protective effect of NSAIDs in those with high pathway-specific genetic risk."
  },
  {
    "objectID": "genetics/spence_2025_41193809.html",
    "href": "genetics/spence_2025_41193809.html",
    "title": "Specificity, length and luck drive gene rankings in association studies",
    "section": "",
    "text": "PubMed: 41193809\nDOI: 10.1038/s41586-025-09703-7\nOverview generated by: Claude Sonnet 4.5, 25/11/2025"
  },
  {
    "objectID": "genetics/spence_2025_41193809.html#key-findings",
    "href": "genetics/spence_2025_41193809.html#key-findings",
    "title": "Specificity, length and luck drive gene rankings in association studies",
    "section": "Key Findings",
    "text": "Key Findings\nThis study provides a systematic comparison of genome-wide association studies (GWAS) and rare variant loss-of-function (LoF) burden tests, revealing fundamental differences in how these methods prioritize genes.\n\nMain Discoveries\n\nDifferent Gene Rankings: GWAS and LoF burden tests systematically prioritize different genes, even when accounting for power differences and gene-mapping issues\nTrait Specificity vs. Importance:\n\nBurden tests prioritize trait-specific genes (genes primarily affecting the studied trait)\nGWAS prioritize trait-specific variants (which can act on pleiotropic genes through context-specific effects)\n\nThree Key Factors:\n\nSpecificity: How specific a gene/variant’s effects are to the trait under study\nLength: Longer genes are more likely to be discovered by burden tests due to more LoF sites\nLuck: Random genetic drift causes variant frequencies to vary, making GWAS rankings partially stochastic"
  },
  {
    "objectID": "genetics/spence_2025_41193809.html#study-design",
    "href": "genetics/spence_2025_41193809.html#study-design",
    "title": "Specificity, length and luck drive gene rankings in association studies",
    "section": "Study Design",
    "text": "Study Design\n\nData Source\n\n209 quantitative traits from UK Biobank\nAnalyzed both GWAS and LoF burden test results\n~360,000 individuals for GWAS\n~450,000 individuals for burden tests\n\n\n\nAnalytical Framework\nThe authors proposed two criteria for gene prioritization:\n\nTrait Importance: How much a gene quantitatively affects a trait (effect size)\nTrait Specificity: The importance of a gene for the studied trait relative to all other traits"
  },
  {
    "objectID": "genetics/spence_2025_41193809.html#major-results",
    "href": "genetics/spence_2025_41193809.html#major-results",
    "title": "Specificity, length and luck drive gene rankings in association studies",
    "section": "Major Results",
    "text": "Major Results\n\n1. Discordant Gene Rankings\n\nOnly 26% of burden test hits fall within top GWAS loci\n74.6% of burden hits are within any GWAS locus, but often ranked much lower\nExample: NPR2 is the 2nd most significant burden test gene for height but contained in the 243rd most significant GWAS locus\n\n\n\n2. Why Burden Tests Prioritize Trait-Specific Genes\nBurden tests aggregate LoF variants within genes. The expected association strength is proportional to:\n\\[\\frac{\\gamma_1^2}{\\sum_t \\gamma_t^2}\\]\nwhere \\(\\gamma_1^2\\) is the gene’s effect on the studied trait and \\(\\sum_t \\gamma_t^2\\) is its total effect across all traits.\nKey mechanisms: - Natural selection acts more strongly on genes with larger total effects across traits - This keeps LoF frequencies lower for pleiotropic genes - Result: Trait-specific genes have more power in burden tests despite potentially smaller effects\n\n\n3. Why GWAS Capture Pleiotropic Genes\nGWAS prioritize trait-specific variants, which can arise in two ways:\n\nVariants affecting trait-specific genes\nContext-specific variants on pleiotropic genes (e.g., tissue-specific regulatory variants)\n\nThe study found: - Variants in tissue-specific ATAC peaks show higher heritability enrichment - Coding variants in specifically expressed genes contribute more to heritability - This explains why GWAS can identify highly pleiotropic genes missed by burden tests\n\n\n4. Gene Length Bias in Burden Tests\n\nLonger genes have more potential LoF sites\nThis increases LoF carrier frequency, boosting statistical power\nEffect: Longer genes appear more significant and more pleiotropic, independent of their true biological importance\n\n\n\n5. The Role of Genetic Drift in GWAS\n\nRandom drift causes variant frequencies to vary widely around their expected values\nFor sufficiently important variants, GWAS rankings become largely random with respect to true effect size\nHigh-frequency variants have more power, creating an apparent “pleiotropy” of top GWAS hits (statistical artifact)"
  },
  {
    "objectID": "genetics/spence_2025_41193809.html#estimating-trait-importance",
    "href": "genetics/spence_2025_41193809.html#estimating-trait-importance",
    "title": "Specificity, length and luck drive gene rankings in association studies",
    "section": "Estimating Trait Importance",
    "text": "Estimating Trait Importance\nNeither method directly ranks genes by trait importance:\n\nBurden tests: Flattening effect - most important genes are most constrained, leading to smallest frequencies and largest standard errors\nGWAS: Individual variant rankings dominated by random frequency variation\n\nSolution: Aggregate signals across multiple variant types - Methods like AMM that combine evidence across many variants per gene - Better correlates with selection coefficients (proxy for importance) - Overcomes the flattening problem"
  },
  {
    "objectID": "genetics/spence_2025_41193809.html#biological-implications",
    "href": "genetics/spence_2025_41193809.html#biological-implications",
    "title": "Specificity, length and luck drive gene rankings in association studies",
    "section": "Biological Implications",
    "text": "Biological Implications\n\nContext-Specific Variants\nThe finding that many GWAS loci lack burden signals suggests context-specific variants acting on pleiotropic genes are major drivers of complex traits. The authors hypothesize these may include: - Developmental genes - Variants that perturb developmental trajectories in trait-specific ways - Tissue-specific regulatory elements\n\n\nDrug Target Discovery\n\nTrait-specific genes (identified by burden tests) may be better drug targets due to fewer side effects\nExplains why LoF burden evidence is more predictive of drug trial success than GWAS evidence\nHowever, if pleiotropic genes can be targeted context-specifically, they may have greater clinical impact"
  },
  {
    "objectID": "genetics/spence_2025_41193809.html#methodological-insights",
    "href": "genetics/spence_2025_41193809.html#methodological-insights",
    "title": "Specificity, length and luck drive gene rankings in association studies",
    "section": "Methodological Insights",
    "text": "Methodological Insights\n\nS-LDSC Analysis\nThe study used stratified LD score regression to show: - Heritability enrichment in tissue-specific ATAC peaks - Coding variants in specifically expressed genes contribute more to heritability - Both axes (gene specificity and variant context-specificity) independently contribute to GWAS signals\n\n\nPopulation Genetics Modeling\nUsed stabilizing selection models to predict: - LoF frequencies inversely proportional to selection coefficient (\\(s_{het}\\)) - Selection coefficient proportional to total trait effects across all traits - Explains observed relationship between constraint and LoF frequency"
  },
  {
    "objectID": "genetics/spence_2025_41193809.html#limitations-and-considerations",
    "href": "genetics/spence_2025_41193809.html#limitations-and-considerations",
    "title": "Specificity, length and luck drive gene rankings in association studies",
    "section": "Limitations and Considerations",
    "text": "Limitations and Considerations\n\nComplexity of gene effects: The simplified model (\\(\\alpha = \\beta \\gamma\\)) may not capture non-linear relationships\nIncomplete pleiotropy landscape: Only 27 traits analyzed, actual pleiotropy may be higher\nContext-specificity: Not all pleiotropic genes can be therapeutically targeted in context-specific ways"
  },
  {
    "objectID": "genetics/spence_2025_41193809.html#practical-recommendations",
    "href": "genetics/spence_2025_41193809.html#practical-recommendations",
    "title": "Specificity, length and luck drive gene rankings in association studies",
    "section": "Practical Recommendations",
    "text": "Practical Recommendations\n\nFor Association Study Interpretation\n\nUse both methods: GWAS and burden tests reveal complementary aspects of trait biology\nConsider gene length: Longer genes in burden test results may be artifacts\nAggregate variants: For trait importance, use methods that combine signals across variants (AMM, MAGMA)\nMind the drift: Top GWAS hits are partially determined by random frequency variation\n\n\n\nFor Drug Development\n\nBurden test hits may indicate better targets for minimizing side effects\nGWAS hits may reveal pleiotropic genes with larger phenotypic impact\nConsider whether context-specific targeting is feasible"
  },
  {
    "objectID": "genetics/spence_2025_41193809.html#conclusions",
    "href": "genetics/spence_2025_41193809.html#conclusions",
    "title": "Specificity, length and luck drive gene rankings in association studies",
    "section": "Conclusions",
    "text": "Conclusions\nGWAS and LoF burden tests systematically prioritize different genes because:\n\nBurden tests rank by gene-level trait specificity, favoring long, trait-specific genes\nGWAS rank by variant-level trait specificity, capturing both trait-specific genes and context-specific variants on pleiotropic genes\n\nNeither method directly ranks by trait importance due to: - Burden tests: Flattening from natural selection - GWAS: Random genetic drift\nBoth methods are valuable and reveal distinct aspects of trait biology. The choice of method depends on the research question and application, with burden tests better for identifying specific biology and GWAS better for comprehensive discovery including pleiotropic mechanisms."
  },
  {
    "objectID": "genetics/spence_2025_41193809.html#related-concepts",
    "href": "genetics/spence_2025_41193809.html#related-concepts",
    "title": "Specificity, length and luck drive gene rankings in association studies",
    "section": "Related Concepts",
    "text": "Related Concepts\n\nTrait specificity (\\(\\Psi_G\\)): \\(\\gamma_1^2 / \\sum_t \\gamma_t^2\\) for genes\nTrait importance: \\(\\gamma_1^2\\) (squared effect size)\nFlattening: Association strength becomes independent of effect size for highly important genes\nContext-specificity: Variants acting only in certain cellular contexts or developmental stages"
  },
  {
    "objectID": "genetics/wu_2023_37601976.html",
    "href": "genetics/wu_2023_37601976.html",
    "title": "Joint analysis of GWAS and multi-omics QTL summary statistics reveals a large fraction of GWAS signals shared with molecular phenotypes",
    "section": "",
    "text": "PubMed: 37601976 DOI: 10.1016/j.xgen.2023.100344 Overview generated by: Gemini 2.5 Flash, 27/11/2025"
  },
  {
    "objectID": "genetics/wu_2023_37601976.html#background-and-objective",
    "href": "genetics/wu_2023_37601976.html#background-and-objective",
    "title": "Joint analysis of GWAS and multi-omics QTL summary statistics reveals a large fraction of GWAS signals shared with molecular phenotypes",
    "section": "Background and Objective",
    "text": "Background and Objective\nGenome-wide association studies (GWAS) have successfully identified thousands of genetic loci associated with complex human traits and diseases. However, the precise molecular mechanisms by which these non-coding genetic variants exert their effects—often by regulating gene expression or other molecular phenotypes—remain largely unknown. Quantitative Trait Loci (QTL) studies provide molecular data (e.g., gene expression, methylation) but are often limited by sample size.\nThe objective of this study was to develop a new method, OPERA (Overlap-Based Partitioned Estimation and Regression Analysis), to integrate summary statistics from GWAS and various multi-omics QTL (xQTL) studies to: 1. Quantify the proportion of GWAS signals that are shared with, and likely mediated by, specific molecular phenotypes. 2. Improve the power for fine-mapping and gene discovery for complex traits."
  },
  {
    "objectID": "genetics/wu_2023_37601976.html#methods-the-opera-framework",
    "href": "genetics/wu_2023_37601976.html#methods-the-opera-framework",
    "title": "Joint analysis of GWAS and multi-omics QTL summary statistics reveals a large fraction of GWAS signals shared with molecular phenotypes",
    "section": "Methods: The OPERA Framework",
    "text": "Methods: The OPERA Framework\n\nData Integration\nOPERA is a computational method that jointly analyzes GWAS summary statistics and multi-omics xQTL summary statistics (including eQTLs for gene expression, pQTLs for protein levels, sQTLs for splicing, etc.).\n\n\nKey Innovation: Shared Genetic Variance\nThe core of OPERA is its ability to partition the heritability of a complex trait into components explained by genetic variants that are shared with different molecular phenotypes (i.e., those that are QTLs for specific molecular traits). This partitioning allows the study to estimate the proportion of GWAS signals (or heritability) that is mediated through each molecular phenotype. OPERA is robust to issues like linkage disequilibrium (LD) and confounding.\n\n\nApplication\nThe method was applied to 11 complex human traits from GWAS, integrated with xQTL data across 13 different molecular phenotypes (e.g., gene expression, DNA methylation, histone modifications) from various human tissues and cell types."
  },
  {
    "objectID": "genetics/wu_2023_37601976.html#key-findings",
    "href": "genetics/wu_2023_37601976.html#key-findings",
    "title": "Joint analysis of GWAS and multi-omics QTL summary statistics reveals a large fraction of GWAS signals shared with molecular phenotypes",
    "section": "Key Findings",
    "text": "Key Findings\n\nLarge Fraction of Shared Signals\nThe primary and most significant finding was that, on average, approximately 50% of the genetic signals identified in GWAS are shared with at least one molecular phenotype (xQTL). This provides strong statistical evidence that a substantial portion of complex trait heritability is mediated by genetic regulatory effects on molecular traits.\n\neQTLs are Major Mediators: Among the molecular phenotypes studied, expression QTLs (eQTLs), which regulate gene expression, were the most significant molecular mediators, accounting for the largest shared fraction of GWAS signals.\n\n\n\nEnhanced Discovery Power\nBy jointly analyzing the data, OPERA achieved: * Identification of novel genes/variants: The joint analysis led to the discovery of 89 novel genes for the 11 complex traits studied, primarily through more effective fine-mapping in previously identified GWAS loci. * Improved fine-mapping: The ability to integrate the xQTL data dramatically enhanced the precision of identifying the likely causal variant within a GWAS locus."
  },
  {
    "objectID": "genetics/wu_2023_37601976.html#conclusions-and-significance",
    "href": "genetics/wu_2023_37601976.html#conclusions-and-significance",
    "title": "Joint analysis of GWAS and multi-omics QTL summary statistics reveals a large fraction of GWAS signals shared with molecular phenotypes",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nThe OPERA framework successfully demonstrated that a large fraction of the genetic basis of complex traits is shared with molecular phenotypes, confirming that these molecular traits (particularly gene expression) are critical intermediate steps between genetic variants and disease risk.\nThis integrated approach provides a powerful tool for converting GWAS signals from abstract associations into biologically actionable regulatory mechanisms, aiding in the discovery of novel therapeutic targets."
  },
  {
    "objectID": "genetics/tokolyi_2025_40038547.html",
    "href": "genetics/tokolyi_2025_40038547.html",
    "title": "The contribution of genetic determinants of blood gene expression and splicing to molecular phenotypes and health outcomes",
    "section": "",
    "text": "PubMed: 40038547 DOI: 10.1038/s41588-025-02096-3 Overview generated by: Gemini 2.5 Flash, 27/11/2025"
  },
  {
    "objectID": "genetics/tokolyi_2025_40038547.html#background-and-objective",
    "href": "genetics/tokolyi_2025_40038547.html#background-and-objective",
    "title": "The contribution of genetic determinants of blood gene expression and splicing to molecular phenotypes and health outcomes",
    "section": "Background and Objective",
    "text": "Background and Objective\nThe biological mechanisms through which the majority of nonprotein-coding genetic variants influence disease risk remain largely unknown. These variants often function by regulating gene activity. The objective of this study was to comprehensively investigate these gene-regulatory mechanisms by mapping genetic determinants of gene expression and splicing in blood and integrating them with various molecular and health outcomes."
  },
  {
    "objectID": "genetics/tokolyi_2025_40038547.html#methods-multi-omics-qtl-mapping-and-integration",
    "href": "genetics/tokolyi_2025_40038547.html#methods-multi-omics-qtl-mapping-and-integration",
    "title": "The contribution of genetic determinants of blood gene expression and splicing to molecular phenotypes and health outcomes",
    "section": "Methods: Multi-Omics QTL Mapping and Integration",
    "text": "Methods: Multi-Omics QTL Mapping and Integration\nThe researchers conducted a comprehensive multi-omics study using data from 4,732 participants to identify quantitative trait loci (QTLs):\n\nQTL Mapping: They mapped gene expression QTLs (eQTLs) and splicing QTLs (sQTLs) in blood using bulk RNA sequencing, identifying cis-QTLs for 17,233 genes and 29,514 splicing events.\nMulti-omics Integration: They integrated the identified genetic associations with data on protein, metabolite, and lipid levels from the same individuals.\nCausal Inference: They employed colocalization analyses to pinpoint instances where the same causal genetic variant affects both a transcriptional/splicing phenotype and a molecular/health outcome.\nMediation Analysis: They quantified the relative contribution of genetic effects at loci with shared etiology to determine which molecular phenotypes are significantly mediated by gene expression or splicing."
  },
  {
    "objectID": "genetics/tokolyi_2025_40038547.html#key-results-and-significance",
    "href": "genetics/tokolyi_2025_40038547.html#key-results-and-significance",
    "title": "The contribution of genetic determinants of blood gene expression and splicing to molecular phenotypes and health outcomes",
    "section": "Key Results and Significance",
    "text": "Key Results and Significance\nThe study successfully bridged the gap between genetic variation and downstream molecular and clinical phenotypes:\n\nExtensive Shared Genetic Etiology: Colocalization analyses revealed a shared genetic association signal with gene expression or splicing for 3,430 proteomic and metabolomic traits.\nMediated Molecular Phenotypes: They found that 222 molecular phenotypes were significantly mediated by gene expression or splicing, providing strong evidence for gene-regulatory causality.\nMechanistic Insights: The approach uncovered specific gene-regulatory mechanisms at disease loci with potential therapeutic relevance, such as \\(WARS1\\) in hypertension, \\(IL7R\\) in dermatitis, and \\(IFNAR2\\) in COVID-19$.\nPublic Resource: The findings provide a comprehensive, open-access resource on the shared genetic etiology across transcriptional phenotypes, molecular traits, and health outcomes (https://IntervalRNA.org.uk)."
  },
  {
    "objectID": "genetics/ren_2023_37181332.html",
    "href": "genetics/ren_2023_37181332.html",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "",
    "text": "PubMed: 37181332\nDOI: 10.1016/j.xhgg.2023.100197\nOverview generated by: Claude Sonnet 4.5, 26/11/2025"
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#key-findings",
    "href": "genetics/ren_2023_37181332.html#key-findings",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Key Findings",
    "text": "Key Findings\nThis study introduces LS-imputation, a nonparametric method that uses GWAS summary statistics combined with individual-level genotypes to impute trait values, enabling nonlinear SNP-trait association analyses and machine learning applications that are impossible with summary statistics alone.\n\nMain Discoveries\n\nNovel imputation approach: First method to recover genetic components of traits from GWAS summary data for nonlinear association analysis\nPerfect recovery property: When test genotypes match training genotypes (X = X*), the method perfectly recovers (centered) trait values, capturing nonlinear SNP-trait information despite using only linear marginal associations\nSuperior performance for association analysis: LS-imputation outperforms state-of-the-art PRS method (PRS-CS) for subsequent association analyses under non-additive models and SNP-SNP interaction detection\nEnables new analyses: Makes possible three applications currently impossible with GWAS summary data: non-additive genetic models, SNP-SNP interaction detection, and nonlinear prediction models"
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#study-design",
    "href": "genetics/ren_2023_37181332.html#study-design",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Study Design",
    "text": "Study Design\n\nCore Problem\nGWAS summary statistics measure only linear marginal SNP-trait associations, limiting their use to linear analyses. The method addresses: How to use summary data for nonlinear SNP-trait analyses?\n\n\nGenetic Model\nAssumes unspecified functional form: \\[y = E(y|x) + \\varepsilon = g(x) + \\varepsilon\\]\nwhere: - \\(g(x)\\) is the unknown genetic component (possibly nonlinear) - \\(\\varepsilon\\) captures environmental effects and noise - No parametric assumptions on \\(g(\\cdot)\\)\n\n\nMethod Overview\nInput: - GWAS summary data: \\(\\{(\\hat{\\beta}_j^*, s_j^*): j=1,\\ldots,p\\}\\) from training data \\((X^*, Y^*)\\) - Test genotype matrix: \\(X\\) (\\(n_2 \\times p\\))\nOutput: Imputed trait values \\(\\hat{Y}\\) for test individuals\nKey insight: With large samples, \\(\\hat{\\beta}^* \\approx \\hat{\\beta}\\) (both estimate same true \\(\\beta\\)), which can be used to formulate least-squares problem."
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#the-ls-imputation-method",
    "href": "genetics/ren_2023_37181332.html#the-ls-imputation-method",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "The LS-Imputation Method",
    "text": "The LS-Imputation Method\n\nFormulation\nIf \\(Y\\) were available, marginal association estimates would be: \\[\\hat{\\beta} = \\frac{1}{n_2-1}X'Y\\]\nSince \\(\\hat{\\beta}^*\\) (from training) \\(\\approx \\hat{\\beta}\\) (from test), solve:\n\\[\\hat{Y} = \\arg\\min_Y \\|\\hat{\\beta}^* - \\frac{1}{n_2-1}X'Y\\|^2\\]\nSolution: \\[\\hat{Y} = (n_2-1)(XX')^+X\\hat{\\beta}^*\\]\nwhere \\((XX')^+\\) is the Moore-Penrose generalized inverse (due to centering of SNPs).\n\n\nRegularized Implementation\nFor computational stability, use ridge regularization: \\[\\hat{Y}(\\lambda) = (n_2-1)(XX' + \\lambda I)^{-1}X\\hat{\\beta}^*\\]\nDefault: \\(\\lambda = 10^{-6}\\) (computationally fast and stable)\n\n\nBatch Processing\nFor large \\(n_2\\): - Divide test data into batches of size \\(m\\) - Apply method to each batch separately - Requires \\(p &gt; m\\) (preferably both \\(n_1\\) and \\(p\\) large) - Choose \\(m\\) giving marginal association results similar to training data"
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#uk-biobank-application",
    "href": "genetics/ren_2023_37181332.html#uk-biobank-application",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "UK Biobank Application",
    "text": "UK Biobank Application\n\nDataset\n\nTrait: HDL cholesterol\nTotal individuals: 356,351 (White British ancestry)\nSNPs: 715,783 (after QC: MAF&gt;0.05, missing&lt;10%, HWE p&gt;0.001, LD pruning r²&lt;0.8)\nSplit:\n\nTraining: \\(n_1 = 178,175\\)\nTest: \\(n_2 = 178,176\\)\n\nImplementation: 50,000 SNPs (p&lt;0.05 in training), 9 batches (8×20K + 1×18K individuals)\n\n\n\nPerfect Recovery Test\nWhen \\(X = X^*\\) (same genotypes as training): - LS-imputation: Correlation with true values = 0.999+ - PRS-CS: Correlation &lt; 0.5 (imperfect recovery)\nDemonstrates unique property: LS-imputation can perfectly recover trait values for training genotypes, capturing nonlinear information.\n\n\nTest Data Imputation Performance\nCorrelation between observed and imputed HDL:\n\n\n\nMethod\nCorrelation (unadjusted)\nCorrelation (adjusted*)\n\n\n\n\nLS-imputation\n0.177\n0.204\n\n\nPRS-CS\n0.279\n0.313\n\n\n\n*Adjusted for sex and age\nInterpretation: PRS-CS shows higher correlation (expected, as linear effects dominate heritability), but LS-imputation better preserves information for association analyses."
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#application-i-non-additive-genetic-models",
    "href": "genetics/ren_2023_37181332.html#application-i-non-additive-genetic-models",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Application I: Non-Additive Genetic Models",
    "text": "Application I: Non-Additive Genetic Models\n\nAdditive Model Results\nComparison of significant SNPs at genome-wide threshold (\\(5\\times10^{-8}\\)):\n\n\n\n\n\n\n\n\nAnalysis\nApproach\nPerformance\n\n\n\n\nTraining (observed)\nStandard GWAS\nBaseline\n\n\nTest (observed)\nStandard GWAS\nSimilar to training\n\n\nTest (LS-imputed)\nOur method\nSimilar to observed, slightly conservative\n\n\nTest (PRS-CS-imputed)\nPRS method\nWay too many significant SNPs\n\n\n\nManhattan plot patterns: LS-imputation closely matched observed data distribution, while PRS-CS identified excessive associations (any SNP in PRS model or LD with them becomes significant).\n\n\nRecessive Model Results\nTesting SNPs under recessive genetic model:\nLS-imputation: - Distribution of significant SNPs similar to observed - Slightly more conservative (fewer false positives) - Effect size estimates highly correlated with true estimates\nPRS-CS: - Severe inflation of significant associations - Not suitable for non-additive model testing\n\n\nDominant Model Results\nSimilar pattern observed (Supplementary results): - LS-imputation: Good agreement with observed - PRS-CS: Excessive false positives\n\n\nQuantitative Comparison\nEffect size correlations (50,000 SNPs):\n\n\n\nModel\nLS vs. Observed\nPRS-CS vs. Observed\n\n\n\n\nAdditive\n0.90+\n0.40-0.60\n\n\nRecessive\n0.85+\n0.30-0.50\n\n\n\nConclusion: LS-imputation preserves information needed for non-additive model testing; PRS-CS does not."
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#application-ii-snp-snp-interaction-detection",
    "href": "genetics/ren_2023_37181332.html#application-ii-snp-snp-interaction-detection",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Application II: SNP-SNP Interaction Detection",
    "text": "Application II: SNP-SNP Interaction Detection\n\nAnalysis Strategy\n\nIdentified 1,758 marginally significant SNPs (p&lt;10⁻⁶) in training data\nRemoved high-LD SNPs (r²&gt;0.99) → 1,652 SNPs\nTested all pairwise interactions: \\(\\binom{1652}{2} = 1,364,026\\) tests\n\nModel for each pair: \\[Y_i = \\alpha_0 + \\text{SNP}_{1i} \\times \\alpha_1 + \\text{SNP}_{2i} \\times \\alpha_2 + \\text{SNP}_{1i} \\times \\text{SNP}_{2i} \\times \\alpha_{12} + \\varepsilon_i\\]\nTest: \\(H_0: \\alpha_{12} = 0\\) (Wald test)\nSignificance threshold: \\(2.5 \\times 10^{-8}\\) (Bonferroni correction)\n\n\nResults\nInteraction effect estimates:\n\n\n\nComparison\nCorrelation\n\n\n\n\nTraining (observed) vs. Test (observed)\nBaseline\n\n\nTest (observed) vs. Test (LS-imputed)\n0.95+\n\n\nTest (observed) vs. Test (PRS-CS-imputed)\n0.60-0.70\n\n\n\nP-value correlations: Similar pattern, with LS-imputation showing strong agreement with observed data.\n\n\nSignificant Interactions Identified\nSNP-SNP pairs (Bonferroni p&lt;\\(2.5\\times10^{-8}\\)):\n\n\n\nDataset\nSignificant pairs\nAgreement with observed\n\n\n\n\nTraining (observed)\nBaseline\n-\n\n\nTest (observed)\nSimilar\nReference\n\n\nTest (LS-imputed)\nHigh overlap\n85-90%\n\n\nTest (PRS-CS-imputed)\nModerate overlap\n60-70%\n\n\n\nLocus-locus interactions: Defined using 1,703 independent LD blocks - LS-imputation: High concordance with observed - Differences between LS-imputed and observed ≤ differences between training and test (both observed)\nConclusion: LS-imputation successfully detects SNP-SNP interactions; PRS-CS less suitable."
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#application-iii-nonlinear-trait-prediction",
    "href": "genetics/ren_2023_37181332.html#application-iii-nonlinear-trait-prediction",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Application III: Nonlinear Trait Prediction",
    "text": "Application III: Nonlinear Trait Prediction\n\nRandom Forest Setup\nTraining: 70% of test data (random subset) Validation: Remaining 30% Features: 1,652 marginally significant SNPs\nGoal: Compare RF predictions using observed vs. imputed traits for training\n\n\nResults\nCorrelation of RF predictions on validation data:\n\n\n\nTraining data\nCorrelation with true trait\n\n\n\n\nObserved traits\nBaseline\n\n\nLS-imputed traits\n0.722\n\n\nPRS-CS-imputed traits\n0.658\n\n\n\nInterpretation: LS-imputed traits retain more information about SNP-trait associations (possibly nonlinear) than PRS-CS-imputed traits.\nWhy LS-imputation performs better: - Captures nonlinear relationships in training data - No parametric model assumptions - Information borrowing across similar individuals"
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#statistical-properties",
    "href": "genetics/ren_2023_37181332.html#statistical-properties",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Statistical Properties",
    "text": "Statistical Properties\n\nInformation Content\nFor test genotypes \\(X\\), imputed trait is: \\[\\hat{Y} \\approx \\frac{n_2-1}{n_1-1}(XX'/p)C_{n_1}Y^*\\]\nwhere: - \\(XX'/p\\) measures genotypic similarities - \\(C_{n_1} = I - 11'/n_1\\) is centering matrix - \\(Y^*\\) are training trait values\nImplication: Imputed trait is weighted average of training traits, weights determined by genotypic similarity.\n\n\nSpecial Case: Perfect Recovery\nWhen \\(X = X^*\\): \\[\\hat{Y} \\rightarrow^P C_{n_1}Y^*\\]\nAs \\(p \\rightarrow \\infty\\), imputed values converge to centered training trait values, which contain nonlinear SNP-trait information.\n\n\nVariance Properties\nFor imputed trait: \\[\\text{Var}(\\hat{Y}) = (n_2-1)^2(XX')^+X\\text{Var}(\\hat{\\beta}^*)X'(XX')^+\\]\nKey points: - Elements of \\(\\hat{Y}\\) are correlated (not iid) - Variances unequal across individuals - Practical solution: Treat as independent in subsequent analyses (simplification) - Choose appropriate batch size \\(m\\) to minimize bias-variance trade-off\n\n\nAsymptotic Behavior\nWith iid normal X (simplified case):\nSmall \\(n_2\\) (fixed): \\[\\text{Var}(\\hat{Y}_j) \\approx n_2\\tau^2/p\\]\nLarge \\(n_2\\) (with \\(n_2/p \\rightarrow c \\in (0,1)\\)): \\[\\text{Var}(\\hat{Y}_j) = n_2 O(1)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)\\tau^2\\]\nRecommendation: Use smaller \\(n_2\\) (or batch size \\(m\\)) for smaller variances."
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#comparison-ls-imputation-vs.-prs-cs",
    "href": "genetics/ren_2023_37181332.html#comparison-ls-imputation-vs.-prs-cs",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Comparison: LS-Imputation vs. PRS-CS",
    "text": "Comparison: LS-Imputation vs. PRS-CS\n\nPrediction Performance\nTrait value correlation (higher is better for prediction): - PRS-CS &gt; LS-imputation - Expected: Linear effects dominate heritability - PRS-CS optimized for prediction\n\n\nAssociation Analysis Performance\nEffect size estimation (for subsequent GWAS): - LS-imputation &gt;&gt; PRS-CS - Critical for non-additive models - Essential for interaction detection\n\n\nWhy PRS Methods Fail for Association Analysis\nPRS-CS assumptions: \\[Y = X\\beta + \\varepsilon\\] \\[\\beta \\sim \\text{ContinuousShrinkage}(\\text{prior})\\]\nProblems: 1. Assumes linear model with specific SNPs 2. Imputed traits reflect estimated linear effects only 3. Any SNP in model (or LD with them) will be “significant” by definition 4. Not suitable for testing associations\n\n\nFundamental Difference\n\n\n\nFeature\nLS-imputation\nPRS-CS\n\n\n\n\nModel\nNonparametric\nParametric linear\n\n\nCaptures nonlinearity\nYes\nNo\n\n\nPerfect recovery\nYes\nNo\n\n\nPrediction\nGood\nBetter\n\n\nAssociation analysis\nBest\nPoor\n\n\nInteraction detection\nBest\nPoor"
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#implementation-details",
    "href": "genetics/ren_2023_37181332.html#implementation-details",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Implementation Details",
    "text": "Implementation Details\n\nComputational Considerations\nRequirements: - \\(p &gt; n_2\\) (or \\(p &gt; m\\) if batches used) - More constraints than unknowns - Unique solution (up to centering)\nMatrix inversion: - Used linalg.inv from Python numpy - Default \\(\\lambda = 10^{-6}\\) for regularization - Fast and stable computation\nMemory management: - Batch processing for large \\(n_2\\) - Typical batch: \\(m = 20,000\\) individuals - Trade-off: smaller \\(m\\) → smaller variance, but information loss between batches\n\n\nParameter Selection\nSNP number (\\(p\\)): - Larger is better (more constraints) - Example: Used 50,000 SNPs (p&lt;0.05 in training) - Can use all available (memory permitting)\nTraining sample (\\(n_1\\)): - Larger is better (more accurate \\(\\hat{\\beta}^*\\)) - Example: 178,175 individuals\nBatch size (\\(m\\)): - Choose to give marginal results similar to training - Not too large (information loss between batches) - Not too small (computational inefficiency) - Example: 20,000 individuals per batch\n\n\nQuality Control Strategy\nRecommended: Choose \\(m\\) where imputed trait gives: 1. Marginal effect estimates ≈ training estimates 2. Standard errors ≈ training SEs (after rescaling)\nSE rescaling: \\(\\sqrt{n_2/n_1} \\times SE_{test}\\) for comparison"
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#extensions-and-variations",
    "href": "genetics/ren_2023_37181332.html#extensions-and-variations",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Extensions and Variations",
    "text": "Extensions and Variations\n\nBinary Traits\nMethod extended to binary outcomes (Supplementary): - Similar formulation - Logistic regression framework - Applied to UK Biobank hypertension data - Promising preliminary results\n\n\nWeighted Least Squares\nAlternative to ordinary least squares: \\[\\hat{Y}_{WLS} = \\arg\\min_Y (\\hat{\\beta}^* - \\frac{1}{n_2-1}X'Y)'W(\\hat{\\beta}^* - \\frac{1}{n_2-1}X'Y)\\]\nwhere \\(W\\) = diagonal matrix with weights \\(\\propto 1/\\text{Var}(\\hat{\\beta}_j^*)\\)\nResult: Similar to OLS (Supplementary), not pursued further.\n\n\nIntercept Known Case\nIf intercept \\(\\alpha_0\\) available for each SNP: - No centering needed - \\(X\\) is full rank - \\((XX')^+ = (XX')^{-1}\\) - Simpler interpretation\n\n\nSample Size Sensitivity\nTraining sample \\(n_1\\): Larger always better SNP number \\(p\\): Larger always better Test sample \\(n_2\\): Results stable for \\(n_2 \\geq 25,000\\) Batch size \\(m\\): Complex trade-off, choose empirically"
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#practical-applications",
    "href": "genetics/ren_2023_37181332.html#practical-applications",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Practical Applications",
    "text": "Practical Applications\n\nUse Case 1: Augmenting Incomplete Data\nScenario: Biobank with genotypes but missing trait - Late-onset disease (e.g., Alzheimer’s) not yet manifested - Expensive/difficult-to-measure phenotype - Large GWAS summary data available externally\nSolution: Impute trait values to augment analyses\n\n\nUse Case 2: Cross-Study Integration\nScenario: Multiple related studies - Different traits measured - Want to analyze trait not measured in focal study - GWAS summary available from other studies\nSolution: Use summary data to impute unmeasured traits\n\n\nUse Case 3: Privacy-Preserving Collaboration\nScenario: Private breeding programs or clinical cohorts - Cannot share individual-level data - Can share summary statistics - Want to conduct joint analyses\nSolution: Each site imputes traits using others’ summaries\n\n\nUse Case 4: Nonlinear Model Development\nScenario: Develop complex prediction models - Neural networks, deep learning, gradient boosting - Require individual-level data - Only summary data available\nSolution: Impute traits to enable nonlinear model training"
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#limitations-and-considerations",
    "href": "genetics/ren_2023_37181332.html#limitations-and-considerations",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Limitations and Considerations",
    "text": "Limitations and Considerations\n\nCurrent Limitations\n\nVariance structure: Elements of \\(\\hat{Y}\\) are correlated and have unequal variances\n\nCurrently ignored in subsequent analyses\nMay cause slight over/under-estimation of SEs\nAccounting for correlations computationally prohibitive\n\nCentering effects: Each batch centered at mean 0\n\nInformation loss between batches (relative levels)\nMitigated by using larger batches\nTrade-off with variance considerations\n\nConstraint requirement: Needs \\(p &gt; m\\)\n\nMust have more SNPs than individuals per batch\nMay limit applicability in some scenarios\n\nRare variant use: Current implementation uses common variants only\n\nRare variants have lower genotyping quality\nExpected to contain less heritability information\nCould be explored with sequencing data\n\n\n\n\nStatistical Assumptions\n\nSame population: Training and test from same population\nUnrelated individuals: No close relatives in test data\nWhite noise errors: \\(\\varepsilon\\) independent of genotypes\nLarge samples: Asymptotic properties require large \\(n_1\\), \\(p\\)\n\n\n\nInterpretation Caveats\nImputed values: - Represent genetic components only - Do not capture environmental variation - Centered (mean 0 within each batch) - Should not be treated as observed phenotypes in all contexts\nSubsequent analyses: - Slightly conservative p-values (good for Type I error control) - Effect size estimates unbiased - Power may be slightly reduced vs. observed data"
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#comparison-to-alternative-approaches",
    "href": "genetics/ren_2023_37181332.html#comparison-to-alternative-approaches",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Comparison to Alternative Approaches",
    "text": "Comparison to Alternative Approaches\n\nvs. PRS Methods\nAll existing PRS methods for summary data: - Assume linear models - Optimize for prediction - Not designed for association analysis - Cannot detect nonlinear effects\nLS-imputation: - Nonparametric - Optimizes for association analysis - Can detect nonlinear effects - Less optimal for pure prediction\n\n\nvs. Multi-Trait Imputation\nPrevious methods (Dahl et al., Hormozdiari et al.): - Impute focal trait using other measured traits - Problem: Any variants associated with imputation traits will appear associated with focal trait - Loss of specificity\nLS-imputation: - Uses only genotypes and summary data for focal trait - Maintains specificity to focal trait - Suitable for association analysis\n\n\nvs. Direct GWAS Summary Analysis\nStandard approach with summary data: - Can only test linear marginal associations - Cannot detect non-additive effects - Cannot detect interactions - Cannot use nonlinear prediction models\nLS-imputation approach: - Enables all of the above - More flexible for exploratory analysis - Can combine with machine learning methods"
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#future-directions",
    "href": "genetics/ren_2023_37181332.html#future-directions",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Future Directions",
    "text": "Future Directions\n\nMethodological Extensions\n\nEfficient algorithms: Handle larger datasets without batching\nGeneralized least squares: Account for correlated marginal estimates\nBetter variance estimation: Properly handle correlated imputed values\nRare variant integration: Extend to sequencing-based data\n\n\n\nAdditional Applications\n\nMulti-trait analysis: Jointly impute multiple correlated traits\nTranscriptome-wide studies: Impute gene expression for TWAS\nPolygenic score development: Use imputed traits to train complex nonlinear PRS models\nPathway analysis: Enable pathway-level nonlinear analyses\n\n\n\nPractical Improvements\n\nAutomated parameter selection: Data-driven choice of \\(m\\), \\(p\\)\nDistributed computing: Parallel batch processing\nMemory optimization: More efficient matrix operations\nQuality metrics: Better diagnostics for imputation quality"
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#related-concepts",
    "href": "genetics/ren_2023_37181332.html#related-concepts",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Related Concepts",
    "text": "Related Concepts\n\nMoore-Penrose inverse: Generalized inverse for non-full-rank matrices\nCentering matrix: \\(C_n = I - 11'/n\\) projects to zero-mean subspace\nGenotypic similarity: Measured by \\(XX'/p\\) (normalized dot products)\nInformation borrowing: Imputed value uses data from similar individuals\nLeast squares: Optimization framework minimizing squared residuals\nRegularization: Adding penalty (ridge) for numerical stability\nBatch processing: Dividing large datasets for computational efficiency"
  },
  {
    "objectID": "genetics/schwanhausser_2011_21593866.html",
    "href": "genetics/schwanhausser_2011_21593866.html",
    "title": "Global quantification of mammalian gene expression control",
    "section": "",
    "text": "PubMed: 21593866 DOI: 10.1038/nature10098 Overview generated by: Gemini 2.5 Flash, 28/11/2025"
  },
  {
    "objectID": "genetics/schwanhausser_2011_21593866.html#key-findings-translation-is-the-primary-control-point-of-protein-abundance",
    "href": "genetics/schwanhausser_2011_21593866.html#key-findings-translation-is-the-primary-control-point-of-protein-abundance",
    "title": "Global quantification of mammalian gene expression control",
    "section": "Key Findings: Translation is the Primary Control Point of Protein Abundance",
    "text": "Key Findings: Translation is the Primary Control Point of Protein Abundance\nThis seminal study provides the first genome-scale, absolute quantification of the entire gene expression cascade—including mRNA abundance, protein abundance, and their respective synthesis and degradation rates—in a mammalian cell line (NIH3T3 fibroblasts).\nThe main conclusion fundamentally changes the view of gene regulation: the cellular abundance of proteins is predominantly controlled at the level of translation, not transcription.\n\nQuantification Results\n\nmRNA vs. Protein Abundance: The study found a better-than-expected correlation between mRNA and protein levels across the genome, with a correlation coefficient (\\(R\\)) of approximately 0.73. This is higher than previously estimated.\nAbundance vs. Turnover: There was no correlation observed between the half-lives (turnover rates) of corresponding mRNAs and proteins. This means a stable mRNA does not necessarily encode a stable protein, and vice versa.\nSynthesis Rates: The quantitative model allowed for the prediction of synthesis rates for over 5,000 genes. This revealed that the large variation in protein abundance is primarily achieved through highly variable translational efficiency (the number of protein molecules produced per mRNA molecule) rather than changes in mRNA levels."
  },
  {
    "objectID": "genetics/schwanhausser_2011_21593866.html#methods-parallel-metabolic-pulse-labeling-and-absolute-quantification",
    "href": "genetics/schwanhausser_2011_21593866.html#methods-parallel-metabolic-pulse-labeling-and-absolute-quantification",
    "title": "Global quantification of mammalian gene expression control",
    "section": "Methods: Parallel Metabolic Pulse Labeling and Absolute Quantification",
    "text": "Methods: Parallel Metabolic Pulse Labeling and Absolute Quantification\nThe study developed and utilized a quantitative approach involving parallel metabolic pulse labeling and mass spectrometry.\n\nExperimental Design\n\nStable Isotope Labeling: NIH3T3 cells were cultured with heavy amino acids (for protein labeling) and heavy nucleosides (for mRNA labeling) for a defined period (pulse labeling).\nParallel Measurement:\n\nProteomics: Liquid chromatography and tandem mass spectrometry (LC-MS/MS) were used to measure the absolute number of protein molecules and their turnover (half-lives).\nTranscriptomics: Microarrays were used to measure the absolute number of mRNA molecules and their turnover (half-lives).\n\n\n\n\nQuantitative Modeling\nThe absolute measurements of abundance and turnover were integrated into a quantitative model that mathematically links the four fundamental processes of gene expression:\n\\[\n\\text{Protein Abundance} \\propto \\frac{\\text{mRNA Abundance} \\times \\text{Translation Rate}}{\\text{Protein Degradation Rate}}\n\\]\nThis model allowed the derivation of previously unknown parameters: the mRNA synthesis rate (transcription rate) and the protein synthesis rate (translation rate)."
  },
  {
    "objectID": "genetics/schwanhausser_2011_21593866.html#implications-the-design-principles-of-gene-expression",
    "href": "genetics/schwanhausser_2011_21593866.html#implications-the-design-principles-of-gene-expression",
    "title": "Global quantification of mammalian gene expression control",
    "section": "Implications: The Design Principles of Gene Expression",
    "text": "Implications: The Design Principles of Gene Expression\nThe quantitative data supports a model where cells use degradation/turnover rates to fine-tune the functional properties of proteins.\n\nStability and Function\n\nUnstable Components: Highly abundant components of essential molecular machinery (e.g., ribosomes) were found to be both abundant and stable (long half-lives), which minimizes the energetic cost of replacement.\nRegulatory/Signaling Components: Proteins with key regulatory or signalling functions (e.g., transcription factors, cell-cycle regulators) tend to have short half-lives and are often less abundant. This allows the cell to achieve rapid and dynamic changes in response to environmental cues.\n\n\n\nControl Mechanism Summary\n\nAbundance Control: Protein steady-state abundance is mostly controlled by translation rates.\nKinetic Control: Protein and mRNA turnover rates (stability) control the time scale over which the protein or mRNA abundance can respond to changes in their synthesis rates."
  },
  {
    "objectID": "genetics/schwanhausser_2011_21593866.html#conclusions",
    "href": "genetics/schwanhausser_2011_21593866.html#conclusions",
    "title": "Global quantification of mammalian gene expression control",
    "section": "Conclusions",
    "text": "Conclusions\nThis study provides an unprecedented quantitative atlas of gene expression in a mammalian cell. The discovery that translational control is the dominant mechanism for determining steady-state protein levels establishes a critical point of regulation in the central dogma of biology and provides a foundational resource for systems biology and computational modeling."
  },
  {
    "objectID": "other.html",
    "href": "other.html",
    "title": "other",
    "section": "",
    "text": "Data workflow with Apache Arrow\n\nworkshop/tutorial on using Apache Arrow in R to work efficiently with datasets that are larger than available memory\nhow to use the R arrow package for fast reading/writing (Parquet, Feather) and for scalable data manipulation using familiar dplyr workflows\n\n\n\nReproducible, scalable, and shareable analysis pipelines with bioinformatics workflow managers\n\nTopic: A Perspective piece highlighting the necessity and benefits of using bioinformatics workflow managers to create reproducible, scalable, and shareable computational analysis pipelines for high-throughput data.\nFunction: Workflow managers simplify pipeline development, optimize resource usage, and manage software installation/versions for portability across different compute platforms.\nResource: The paper outlines community-curated pipeline initiatives that allow users to perform complex, best-practice analyses without manual assembly, contributing to better reproducibility in biomedical research."
  },
  {
    "objectID": "proteomics/drouard_2023_38129841.html",
    "href": "proteomics/drouard_2023_38129841.html",
    "title": "Longitudinal multi-omics study reveals common etiology underlying association between plasma proteome and BMI trajectories in adolescent and young adult twins",
    "section": "",
    "text": "PubMed: 38129841 DOI: 10.1186/s12916-023-03198-7 Overview generated by: Gemini 2.5 Flash, 28/11/2025"
  },
  {
    "objectID": "proteomics/drouard_2023_38129841.html#key-findings-shared-etiology-of-plasma-proteome-and-bmi-trajectories",
    "href": "proteomics/drouard_2023_38129841.html#key-findings-shared-etiology-of-plasma-proteome-and-bmi-trajectories",
    "title": "Longitudinal multi-omics study reveals common etiology underlying association between plasma proteome and BMI trajectories in adolescent and young adult twins",
    "section": "Key Findings: Shared Etiology of Plasma Proteome and BMI Trajectories",
    "text": "Key Findings: Shared Etiology of Plasma Proteome and BMI Trajectories\nThis longitudinal multi-omics study utilized a twin design to dissect the influence of genetic and environmental factors on the association between the plasma proteome and Body Mass Index (BMI) trajectories during adolescence and young adulthood.\nThe central finding is that the observed associations between protein levels and BMI trajectories (changes in BMI over time) are largely attributable to common etiological factors, specifically:\n\nShared Genetic Factors (A): Genetic effects explained a significant portion of the covariance between plasma proteins and BMI, suggesting that heritable factors simultaneously influence both protein expression and adiposity development.\nShared Environmental Factors (C): Common environmental influences also played a role in explaining the protein-BMI association.\n\n\nIdentification of Key Proteins and Causal Links\nThe study identified 38 plasma proteins whose levels were significantly associated with an individual’s BMI trajectory. Clustering analysis showed these proteins grouped into pathways related to lipid transport/metabolism (e.g., Apolipoproteins) and immune/inflammatory response."
  },
  {
    "objectID": "proteomics/drouard_2023_38129841.html#methods-and-study-design",
    "href": "proteomics/drouard_2023_38129841.html#methods-and-study-design",
    "title": "Longitudinal multi-omics study reveals common etiology underlying association between plasma proteome and BMI trajectories in adolescent and young adult twins",
    "section": "Methods and Study Design",
    "text": "Methods and Study Design\nThe study employed a rigorous, longitudinal twin design spanning approximately 10 years of follow-up.\n\nCohorts and Data\n\nParticipants: Two independent cohorts of twins: FinnTwin12 (N=651) and the Netherlands Twin Register (NTR) (N=665).\nPhenotypes: BMI was measured four times per individual over the follow-up period to model BMI trajectories.\nOmics Data: Plasma proteomics (Olink Proximity Extension Assays), metabolomics, and genotype data were utilized.\n\n\n\nStatistical Analysis\n\nTwin Modeling: Structural equation modeling (using ACE models) was applied to estimate the proportion of variance and covariance in protein levels and BMI trajectories explained by additive genetics (A), common environment (C), and unique environment (E).\nMendelian Randomization (MR): Two-sample MR was performed using public GWAS and pQTL data to investigate potential causal effects between genetically determined protein levels and BMI, and vice-versa."
  },
  {
    "objectID": "proteomics/drouard_2023_38129841.html#results-focus-on-apolipoprotein-b",
    "href": "proteomics/drouard_2023_38129841.html#results-focus-on-apolipoprotein-b",
    "title": "Longitudinal multi-omics study reveals common etiology underlying association between plasma proteome and BMI trajectories in adolescent and young adult twins",
    "section": "Results: Focus on Apolipoprotein B",
    "text": "Results: Focus on Apolipoprotein B\nA key protein highlighted by the analysis was Apolipoprotein B (ApoB).\n\nGenetic Correlation: ApoB showed a strong genetic correlation with BMI, suggesting the genetic architecture underlying ApoB concentration substantially overlaps with the genetic architecture of BMI.\nCausality: The MR analysis provided evidence for a causal effect of ApoB on BMI, meaning genetically predicted higher ApoB levels lead to higher BMI. Conversely, there was no evidence suggesting that genetically predicted BMI causally affects ApoB levels.\nMetabolite Connection: The ApoB-BMI association was also linked to other metabolic markers, reinforcing ApoB’s role as a central, genetically mediated biomarker in adiposity."
  },
  {
    "objectID": "proteomics/drouard_2023_38129841.html#conclusions-and-recommendations",
    "href": "proteomics/drouard_2023_38129841.html#conclusions-and-recommendations",
    "title": "Longitudinal multi-omics study reveals common etiology underlying association between plasma proteome and BMI trajectories in adolescent and young adult twins",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study concludes that the association between the plasma proteome and BMI trajectories is predominantly driven by shared genetic and common environmental factors, providing a strong biological basis for the co-occurrence of these traits. The findings prioritize the ApoB-coding gene (APOB) as a genetically determined factor with a likely causal influence on BMI, supporting its role as a potential therapeutic target in the early development of adiposity. The multi-omics approach, especially the use of longitudinal twin data, offers a powerful framework for dissecting the complex etiology of chronic diseases."
  },
  {
    "objectID": "proteomics/pietzner_2021_34648354.html",
    "href": "proteomics/pietzner_2021_34648354.html",
    "title": "Mapping the proteo-genomic convergence of human diseases",
    "section": "",
    "text": "PubMed: 34648354 DOI: 10.1126/science.abj1541 Overview generated by: Gemini 2.5 Flash, 28/11/2025"
  },
  {
    "objectID": "proteomics/pietzner_2021_34648354.html#key-findings-linking-genetic-risk-to-disease-via-plasma-proteins",
    "href": "proteomics/pietzner_2021_34648354.html#key-findings-linking-genetic-risk-to-disease-via-plasma-proteins",
    "title": "Mapping the proteo-genomic convergence of human diseases",
    "section": "Key Findings: Linking Genetic Risk to Disease via Plasma Proteins",
    "text": "Key Findings: Linking Genetic Risk to Disease via Plasma Proteins\nThis study provides a comprehensive “proteo-genomic map” that links genetic risk for hundreds of human diseases and traits to changes in the levels of thousands of circulating plasma proteins. The core innovation is the systematic use of genetic colocalization to identify instances where the same genetic variant influences both a plasma protein level (as a pQTL) and a clinical disease/trait (as a GWAS hit).\nThe map provides two major insights:\n\nProteins Mediate Genetic Risk: The study identified 2,228 instances of genetic colocalization across 1,440 proteins and 498 diseases/traits. This finding suggests that a significant fraction of genetic disease risk acts by altering the level of a specific circulating protein.\nCausal Inference and Drug Targets: By integrating these colocalization events with Mendelian Randomization (MR), the map pinpoints proteins that are likely to be causally related to a disease, making them high-priority candidates for therapeutic drug targeting.\n\n\nColocalization and Mendelian Randomization\n\nColocalization: The analysis used a Bayesian method to confirm that the genetic signal for a plasma protein (pQTL) and the genetic signal for a disease (GWAS) at a given locus are driven by the same causal variant. This identified 864 proteins with a shared genetic signal with at least one clinical trait.\nCausal Relationships: Integrating this information with MR, the study identified 44 proteins that were robustly predicted to be causally linked to 37 diseases/traits, often validating established biological pathways. For example, C-reactive protein (CRP) was found to be causally associated with increased risk for coronary artery disease and other inflammatory conditions."
  },
  {
    "objectID": "proteomics/pietzner_2021_34648354.html#methods-and-design",
    "href": "proteomics/pietzner_2021_34648354.html#methods-and-design",
    "title": "Mapping the proteo-genomic convergence of human diseases",
    "section": "Methods and Design",
    "text": "Methods and Design\n\nData Sources\n\nProteomics: Measured levels of ~3,000 plasma proteins in over 54,000 individuals from the UK Biobank Pharma Proteomics Project (UKB-PPP) and other cohorts.\nGenetics (pQTLs): Genome-wide association study (GWAS) summary statistics for protein quantitative trait loci (pQTLs).\nDisease/Trait Genetics (GWAS): GWAS summary statistics for 498 diseases and complex traits.\n\n\n\nAnalytical Framework\n\nGWAS for Proteins and Traits: Conducted GWAS for all proteins and aggregated existing GWAS results for traits.\nGenetic Colocalization: Performed systematic colocalization analysis between all protein pQTLs and all trait GWAS loci to find shared genetic drivers.\nCausal Inference (MR): Applied Mendelian Randomization to the colocalized pairs to determine the likely causal direction (i.e., whether the protein level causes the disease or vice versa).\nKnowledge Graph: Constructed a “proteo-genomic knowledge graph” to visualize and connect the colocalized and causal protein-disease relationships."
  },
  {
    "objectID": "proteomics/pietzner_2021_34648354.html#implications-for-biology-and-drug-discovery",
    "href": "proteomics/pietzner_2021_34648354.html#implications-for-biology-and-drug-discovery",
    "title": "Mapping the proteo-genomic convergence of human diseases",
    "section": "Implications for Biology and Drug Discovery",
    "text": "Implications for Biology and Drug Discovery\n\nConvergence of Diseases\nThe proteo-genomic map revealed convergence points where the genetic signals for multiple diseases colocalized with the same protein. This means that genetic variation affecting a single protein can predispose an individual to several different, often seemingly unrelated, conditions (e.g., genetic variation at the SULT2A1 locus linked to increased SULT2A1 protein activity and a higher risk of gallstones).\n\n\nPrioritization of Drug Targets\nThe study’s causal protein-disease links provide a strong basis for prioritizing drug development. If a protein is causally linked to a disease, modulating that protein’s level with a drug is likely to be therapeutically effective. The study validated known drug targets and also highlighted new potential targets based on the strength of the genetic evidence."
  },
  {
    "objectID": "proteomics/pietzner_2021_34648354.html#conclusions",
    "href": "proteomics/pietzner_2021_34648354.html#conclusions",
    "title": "Mapping the proteo-genomic convergence of human diseases",
    "section": "Conclusions",
    "text": "Conclusions\nThe proteo-genomic map serves as a fundamental resource for understanding the molecular mechanisms underlying genetic disease risk. By demonstrating that genetic variation for many diseases converges on a shared set of plasma proteins, the study validates plasma proteomics as a key layer for translational medicine, facilitating drug target discovery and providing a causal foundation for biomarker development."
  },
  {
    "objectID": "proteomics/fu_2025_39810024.html",
    "href": "proteomics/fu_2025_39810024.html",
    "title": "Decoding the functional impact of the cancer genome through protein-protein interactions",
    "section": "",
    "text": "PubMed: 39810024 DOI: 10.1038/s41568-024-00784-6 Overview generated by: Gemini 2.5 Flash, 28/11/2025"
  },
  {
    "objectID": "proteomics/fu_2025_39810024.html#key-findings-linking-oncogenic-mutations-to-ppi-network-rewiring",
    "href": "proteomics/fu_2025_39810024.html#key-findings-linking-oncogenic-mutations-to-ppi-network-rewiring",
    "title": "Decoding the functional impact of the cancer genome through protein-protein interactions",
    "section": "Key Findings: Linking Oncogenic Mutations to PPI Network Rewiring",
    "text": "Key Findings: Linking Oncogenic Mutations to PPI Network Rewiring\nThis review focuses on the paradigm that Protein-Protein Interactions (PPIs) are the critical functional intermediaries between genomic mutations and the resulting oncogenic phenotype. The core assertion is that the functional impact of individual genomic alterations is often transmitted through altered nodes and hubs of PPIs, leading to the rewiring of molecular signaling cascades.\n\nMutational Effects on PPIs: NeoPPIs and HypoPPIs\nOncogenic mutations frequently map to the protein surface, particularly at or near the hotspot residues of the PPI interface. These mutations can perturb the interactome in two primary ways:\n\nNeomorphic PPIs (neoPPIs): Oncogenic mutations may lead to modified residues that create new contact sites or neo-epitopes, enabling interactions with other proteins that the wild-type protein does not typically bind to.\nHypomorphic PPIs (hypoPPIs): Conversely, mutations can decrease or disrupt the interaction of existing protein complexes, leading to a loss-of-function phenotype.\n\nUnderstanding the mechanisms of these mutation-driven, differential PPIs is crucial for deciphering tumor heterogeneity and developing precision oncology strategies."
  },
  {
    "objectID": "proteomics/fu_2025_39810024.html#molecular-basis-of-cancer-associated-ppi-perturbation",
    "href": "proteomics/fu_2025_39810024.html#molecular-basis-of-cancer-associated-ppi-perturbation",
    "title": "Decoding the functional impact of the cancer genome through protein-protein interactions",
    "section": "Molecular Basis of Cancer-Associated PPI Perturbation",
    "text": "Molecular Basis of Cancer-Associated PPI Perturbation\nThe functional outcome of a driver mutation is determined by its location and the nature of the amino acid substitution, which is why a mutation-focused approach is necessary, rather than just a driver gene-focused approach.\n\nSite-Specific and Lineage-Dependent Effects\n\nDifferent mutations within the same gene can affect various structural components, including defined protein domains, Short Linear Motifs (SLIMs), and Intrinsically Disordered Regions (IDRs).\nThe same gene can be an oncogenic driver in one cancer type and a tumor suppressor in another. For example, different hotspot mutations in PIK3CA (in the helical vs. kinase domain) are prevalent in different cancer types (cervical vs. breast cancer, respectively), exhibiting differential oncogenic activities.\nEven different amino acid substitutions at the same hotspot position can lead to distinct functional outcomes, as seen with IDH1 R132H (associated with a less-aggressive phenotype in gliomas) and IDH1 R132C (associated with enhanced proliferation in AML), due to differential neo-enzymatic activity."
  },
  {
    "objectID": "proteomics/fu_2025_39810024.html#technologies-for-ppi-identification-and-prediction",
    "href": "proteomics/fu_2025_39810024.html#technologies-for-ppi-identification-and-prediction",
    "title": "Decoding the functional impact of the cancer genome through protein-protein interactions",
    "section": "Technologies for PPI Identification and Prediction",
    "text": "Technologies for PPI Identification and Prediction\nThe review summarizes the experimental and computational methods used to study mutation-affected PPIs:\n\nExperimental Monitoring\nExperimental approaches are categorized into:\n\nCo-complex affinity-based technologies: These detect co-complex formation and include methods like co-immunoprecipitation, and Affinity Purification coupled with Mass Spectrometry (AP-MS).\nProximity-based assays: These detect interactions based on close spatial proximity and include FRET, BRET, Proximity Ligation Assays (PLAs), and yeast two-hybrid (Y2H) systems.\n\n\n\nComputational and AI Methods\n\nComputational tools (e.g., FoldX, Flex ddG) quantify mutation-induced changes in free binding energy.\nAI/Machine Learning methods are rapidly advancing the prediction of protein and protein complex structures. Recent models like AlphaFold3 (AF3) and AlphaFold Multimer can predict the structures of protein-protein, protein-DNA, and protein-ligand complexes. However, these models have limitations, such as difficulty in predicting mutational effects in highly flexible regions or capturing dynamic conformational transitions."
  },
  {
    "objectID": "proteomics/fu_2025_39810024.html#therapeutic-potential",
    "href": "proteomics/fu_2025_39810024.html#therapeutic-potential",
    "title": "Decoding the functional impact of the cancer genome through protein-protein interactions",
    "section": "Therapeutic Potential",
    "text": "Therapeutic Potential\nThe intersection of cancer variants and altered PPI interfaces opens up a new frontier for developing tumor-selective therapeutic strategies:\n\nTargeting Mutant Residues: The development of agents that specifically target the modified residues of the driver protein itself (e.g., KRAS(G12C)-targeted therapies).\nModulating NeoPPIs and HypoPPIs: New therapeutic strategies involve designing small-molecule PPI modulators to either disrupt oncogenic neoPPIs or to restore function to lost hypoPPIs (e.g., using small-molecule glues to re-establish a suppressed interaction). This approach represents a path toward personalized medicine by directly addressing the network vulnerabilities created by specific driver mutations."
  },
  {
    "objectID": "proteomics/pietzner_2025_41068475.html",
    "href": "proteomics/pietzner_2025_41068475.html",
    "title": "Machine learning-guided deconvolution of plasma protein levels",
    "section": "",
    "text": "PubMed: 41068475 DOI: 10.1038/s44320-025-00158-6 Overview generated by: Gemini 2.5 Flash, 28/11/2025"
  },
  {
    "objectID": "proteomics/pietzner_2025_41068475.html#key-findings-deconvoluting-the-sources-of-plasma-protein-variation",
    "href": "proteomics/pietzner_2025_41068475.html#key-findings-deconvoluting-the-sources-of-plasma-protein-variation",
    "title": "Machine learning-guided deconvolution of plasma protein levels",
    "section": "Key Findings: Deconvoluting the Sources of Plasma Protein Variation",
    "text": "Key Findings: Deconvoluting the Sources of Plasma Protein Variation\nThis study used a machine learning (ML) approach to systematically identify and quantify the key factors that determine the variation in thousands of plasma protein levels, aiming to overcome the challenge of limited understanding of protein origins that hampers biomarker translation.\n\nPrimary Determinants of Protein Levels\nThe ML model, which assessed over 1,800 participant and sample characteristics, found that a median of 20 factors (ranging from 1 to 37) jointly explained an average of 19.4% (up to 100.0%) of the variance in approximately 3,000 protein targets.\nCrucially, modifiable characteristics (e.g., health metrics, disease status, lifestyle) explained significantly more variance (median: 10.0%) compared to genetic variation (median: 3.9%). This suggests that dynamic, non-genetic factors are the primary drivers of plasma protein differences between individuals.\n\n\nSegregation and Clustering\nProteins were found to segregate into distinct clusters based on their shared explanatory factors. These clusters revealed proteins primarily driven by: * Human Health and Disease: Indicators of health status and disease. * Pre-analytical Variation: Technical and sample-handling measures, such as accidental activation of platelets.\n\n\nAncestry, Sex, and Robustness\nThe overall explanatory factors were largely consistent across different sexes and ancestral groups. However, the analysis identified specific proteins where the underlying explanatory factors differed by: * Sex: 1,374 proteins. * Ancestry: 74 proteins.\n\n\nResource and Application\nThe study establishes a valuable resource to guide biomarker and drug target discovery, including: 1. Knowledge Graph: An integrated knowledge graph linking the identified explanatory factors with genetic studies and drug characteristics, intended to guide the identification of drug target engagement markers. 2. Biomarker Identification: Demonstrated utility by identifying disease-specific biomarkers, such as matrix metalloproteinase 12 (MMP12) for abdominal aortic aneurysm. 3. Framework: Developed a widely applicable R package and an interactive web portal for researchers to explore all results and integrate the findings into ongoing studies."
  },
  {
    "objectID": "proteomics/pietzner_2025_41068475.html#methods",
    "href": "proteomics/pietzner_2025_41068475.html#methods",
    "title": "Machine learning-guided deconvolution of plasma protein levels",
    "section": "Methods",
    "text": "Methods\n\nCohort: 43,240 participants from the UK Biobank.\nData: Approximately 3,000 plasma proteins were measured, alongside &gt;1,800 participant and sample characteristics.\nAnalysis: Machine learning was used to identify and quantify the variance explained by different factors, with models being consistent across sexes and ancestral groups."
  },
  {
    "objectID": "interaction/gauderman_2025_40763299.html",
    "href": "interaction/gauderman_2025_40763299.html",
    "title": "Pathway polygenic risk scores (pPRS) for the analysis of gene-environment interaction",
    "section": "",
    "text": "PubMed: 40763299 DOI: 10.1371/journal.pgen.1011543 Overview generated by: Gemini 2.5 Flash, 28/11/2025"
  },
  {
    "objectID": "interaction/gauderman_2025_40763299.html#key-findings-enhancing-power-for-gene-environment-interaction-gxe",
    "href": "interaction/gauderman_2025_40763299.html#key-findings-enhancing-power-for-gene-environment-interaction-gxe",
    "title": "Pathway polygenic risk scores (pPRS) for the analysis of gene-environment interaction",
    "section": "Key Findings: Enhancing Power for Gene-Environment Interaction (GxE)",
    "text": "Key Findings: Enhancing Power for Gene-Environment Interaction (GxE)\nThe paper introduces pathway polygenic risk scores (pPRS) as a novel and more powerful method for detecting Polygenic Risk Score by Environment (PRS x E) interactions in complex human traits. The central finding is that standard PRS often include too many genetic variants that affect disease independently of the environment, which dilutes the true GxE signal and reduces statistical power. By focusing the PRS on biologically relevant pathways, pPRS substantially improves the ability to detect these crucial interactions."
  },
  {
    "objectID": "interaction/gauderman_2025_40763299.html#study-design-and-motivation",
    "href": "interaction/gauderman_2025_40763299.html#study-design-and-motivation",
    "title": "Pathway polygenic risk scores (pPRS) for the analysis of gene-environment interaction",
    "section": "Study Design and Motivation",
    "text": "Study Design and Motivation\n\nThe Problem with Standard PRS x E Analysis\nA standard Polygenic Risk Score (PRS) is a comprehensive aggregate of many genetic variants. When testing for PRS x E interaction, the inclusion of a large number of variants that do not interact with the environmental factor (E) “waters down” the underlying signal from the few truly interacting variants. This leads to reduced statistical power to identify genuine GxE effects, potentially masking important biological insights and opportunities for targeted prevention.\n\n\nIntroducing Pathway Polygenic Risk Scores (pPRS)\nThe authors propose the use of pPRS scores, which are constructed by annotating subsets of genetic variants (SNPs) to specific genomic pathways using state-of-the-art annotation tools. This approach integrates existing biological knowledge to create a more targeted genetic risk measure, hypothesized to be more sensitive to GxE effects mediated through those pathways."
  },
  {
    "objectID": "interaction/gauderman_2025_40763299.html#methods-and-results",
    "href": "interaction/gauderman_2025_40763299.html#methods-and-results",
    "title": "Pathway polygenic risk scores (pPRS) for the analysis of gene-environment interaction",
    "section": "Methods and Results",
    "text": "Methods and Results\n\nSimulation Studies\nThrough extensive simulation studies, the researchers demonstrated that testing a targeted pPRS x E interaction yields substantially greater statistical power compared to testing the interaction using a broad, overall PRS.\n\n\nEmpirical Application: Colorectal Cancer (CRC) and NSAIDs\nThe pPRS method was applied to a large case-control study (N = 78,253) of colorectal cancer (CRC), using non-steroidal anti-inflammatory drugs (NSAIDs) as the environmental factor, a known protective exposure for CRC.\n\nOverall PRS Result: No evidence of an overall PRS x NSAIDs interaction was observed (\\(p = 0.41\\)).\npPRS Result: A highly significant pPRS x NSAIDs interaction (\\(p = 0.0003\\)) was identified based on SNPs restricted to the TGF-\\(\\beta\\)/gonadotropin releasing hormone receptor (GRHR) pathway.\n\n\n\nInterpretation of Empirical Results\nThe interaction analysis showed that the protective effect of NSAIDs against CRC was significantly stronger among individuals with higher genetic risk captured by the TGF-\\(\\beta\\)/GRHR pPRS. For example, the odds ratio (OR) for NSAIDs protecting against CRC was 0.84 for individuals at the 5th percentile of the pPRS (low risk) but significantly better at 0.70 for those at the 95th percentile (high risk)."
  },
  {
    "objectID": "interaction/gauderman_2025_40763299.html#conclusions-and-recommendations",
    "href": "interaction/gauderman_2025_40763299.html#conclusions-and-recommendations",
    "title": "Pathway polygenic risk scores (pPRS) for the analysis of gene-environment interaction",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe pPRS approach successfully addresses the power limitations of standard PRS in GxE analysis. From a biological perspective, this suggests that NSAIDs may act to reduce CRC risk specifically through genes within the identified pathways. From a population health perspective, the findings support a precision prevention approach, suggesting that focusing on genetic susceptibility within biologically informed pathways may be more sensitive for identifying individuals who would benefit most from specific prevention efforts. The authors recommend the use of pPRS to integrate biological insight and maximize statistical power in future GxE studies."
  }
]