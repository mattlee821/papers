[
  {
    "objectID": "interaction/miao_2025_40410536.html",
    "href": "interaction/miao_2025_40410536.html",
    "title": "PIGEON: a statistical framework for estimating gene-environment interaction for polygenic traits",
    "section": "",
    "text": "PubMed: 40410536 DOI: 10.1038/s41562-025-02202-9 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "interaction",
      "Papers",
      "PIGEON: a statistical framework for estimating gene-environment interaction for polygenic traits"
    ]
  },
  {
    "objectID": "interaction/miao_2025_40410536.html#key-findings-and-motivation",
    "href": "interaction/miao_2025_40410536.html#key-findings-and-motivation",
    "title": "PIGEON: a statistical framework for estimating gene-environment interaction for polygenic traits",
    "section": "Key Findings and Motivation",
    "text": "Key Findings and Motivation\nUnderstanding Gene-Environment Interaction (GxE) is crucial for deciphering the genetic architecture of human complex traits. The authors introduce the PIGEON (PolygenIc Gene-Environment interactiON) framework to address the challenges in scalability and interpretability faced by current GxE methods. PIGEON provides a unified and statistically grounded approach for quantifying polygenic GxE effects, requiring only summary statistics data as input.",
    "crumbs": [
      "interaction",
      "Papers",
      "PIGEON: a statistical framework for estimating gene-environment interaction for polygenic traits"
    ]
  },
  {
    "objectID": "interaction/miao_2025_40410536.html#study-design-and-methods",
    "href": "interaction/miao_2025_40410536.html#study-design-and-methods",
    "title": "PIGEON: a statistical framework for estimating gene-environment interaction for polygenic traits",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\n\nPIGEON Framework\nPIGEON is a unified statistical framework designed to model polygenic GxE effects for complex traits using a variance component analytical approach. It allows researchers to define the parameters of interest and systematically compare different existing GxE methodologies, providing a clear map of the GxE landscape.\n\n\nEstimation Procedure\nThe core methodology of PIGEON is an extension of Linkage Disequilibrium (LD) score regression adapted for GxE analysis. The key feature of the PIGEON estimation procedure is its reliance solely on Genome-Wide Interaction Study (GWIS) and Genome-Wide Association Study (GWAS) summary statistics, avoiding the need for individual-level genotype data. The framework outlines two main objectives in polygenic GxE inference:\n\nDetecting GxE: Estimating the GxE variance component (\\(\\sigma_I^2\\)), where a value greater than zero indicates the presence of GxE.\nInterpreting GxE: Estimating covariant GxE (\\(\\rho_{GI}\\)) and Oracle Polygenic Score GxE (PGSxE), which quantifies the interaction between the environment and an individual’s true additive genetic component (PGS). The estimation of Oracle PGSxE is shown to be equivalent to estimating covariant GxE.\n\n\n\nAnalytical Advantages\nThe PIGEON method is robust to issues such as arbitrary sample overlap between GWAS and GWIS data and heteroskedasticity (non-constant residual variance across environments), which often complicate traditional GxE methods.",
    "crumbs": [
      "interaction",
      "Papers",
      "PIGEON: a statistical framework for estimating gene-environment interaction for polygenic traits"
    ]
  },
  {
    "objectID": "interaction/miao_2025_40410536.html#results-and-empirical-application",
    "href": "interaction/miao_2025_40410536.html#results-and-empirical-application",
    "title": "PIGEON: a statistical framework for estimating gene-environment interaction for polygenic traits",
    "section": "Results and Empirical Application",
    "text": "Results and Empirical Application\nThe paper demonstrates the effectiveness of PIGEON through extensive theoretical and empirical analyses:\n\nGene-by-Education Interaction: A quasi-experimental study of gene-by-education interaction was performed on health outcomes, showcasing PIGEON’s ability to analyze real-world policy-relevant exposures.\nGene-by-Sex Interaction: The method was successfully applied to quantify gene-by-sex interaction for a large set of 530 traits using data from the UK Biobank.\nGene-by-Treatment Interaction: PIGEON was used to identify genetic interactors that help explain the heterogeneity of treatment effects in a clinical trial focused on smoking cessation.",
    "crumbs": [
      "interaction",
      "Papers",
      "PIGEON: a statistical framework for estimating gene-environment interaction for polygenic traits"
    ]
  },
  {
    "objectID": "interaction/miao_2025_40410536.html#conclusions-and-recommendations",
    "href": "interaction/miao_2025_40410536.html#conclusions-and-recommendations",
    "title": "PIGEON: a statistical framework for estimating gene-environment interaction for polygenic traits",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nPIGEON provides an innovative and rigorous solution to long-standing challenges in polygenic GxE inference. By leveraging summary statistics and a unified variance component framework, the paper suggests a promising path that may fundamentally reshape analytical strategies in future GxE studies for complex human traits.",
    "crumbs": [
      "interaction",
      "Papers",
      "PIGEON: a statistical framework for estimating gene-environment interaction for polygenic traits"
    ]
  },
  {
    "objectID": "interaction/gauderman_2025_40763299.html",
    "href": "interaction/gauderman_2025_40763299.html",
    "title": "Pathway polygenic risk scores (pPRS) for the analysis of gene-environment interaction",
    "section": "",
    "text": "PubMed: 40763299 DOI: 10.1371/journal.pgen.1011543 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "interaction",
      "Papers",
      "Pathway polygenic risk scores (pPRS) for the analysis of gene-environment interaction"
    ]
  },
  {
    "objectID": "interaction/gauderman_2025_40763299.html#key-findings-enhancing-power-for-gene-environment-interaction-gxe",
    "href": "interaction/gauderman_2025_40763299.html#key-findings-enhancing-power-for-gene-environment-interaction-gxe",
    "title": "Pathway polygenic risk scores (pPRS) for the analysis of gene-environment interaction",
    "section": "Key Findings: Enhancing Power for Gene-Environment Interaction (GxE)",
    "text": "Key Findings: Enhancing Power for Gene-Environment Interaction (GxE)\nThe paper introduces pathway polygenic risk scores (pPRS) as a novel and more powerful method for detecting Polygenic Risk Score by Environment (PRS x E) interactions in complex human traits. The central finding is that standard PRS often include too many genetic variants that affect disease independently of the environment, which dilutes the true GxE signal and reduces statistical power. By focusing the PRS on biologically relevant pathways, pPRS substantially improves the ability to detect these crucial interactions.",
    "crumbs": [
      "interaction",
      "Papers",
      "Pathway polygenic risk scores (pPRS) for the analysis of gene-environment interaction"
    ]
  },
  {
    "objectID": "interaction/gauderman_2025_40763299.html#study-design-and-motivation",
    "href": "interaction/gauderman_2025_40763299.html#study-design-and-motivation",
    "title": "Pathway polygenic risk scores (pPRS) for the analysis of gene-environment interaction",
    "section": "Study Design and Motivation",
    "text": "Study Design and Motivation\n\nThe Problem with Standard PRS x E Analysis\nA standard Polygenic Risk Score (PRS) is a comprehensive aggregate of many genetic variants. When testing for PRS x E interaction, the inclusion of a large number of variants that do not interact with the environmental factor (E) “waters down” the underlying signal from the few truly interacting variants. This leads to reduced statistical power to identify genuine GxE effects, potentially masking important biological insights and opportunities for targeted prevention.\n\n\nIntroducing Pathway Polygenic Risk Scores (pPRS)\nThe authors propose the use of pPRS scores, which are constructed by annotating subsets of genetic variants (SNPs) to specific genomic pathways using state-of-the-art annotation tools. This approach integrates existing biological knowledge to create a more targeted genetic risk measure, hypothesized to be more sensitive to GxE effects mediated through those pathways.",
    "crumbs": [
      "interaction",
      "Papers",
      "Pathway polygenic risk scores (pPRS) for the analysis of gene-environment interaction"
    ]
  },
  {
    "objectID": "interaction/gauderman_2025_40763299.html#methods-and-results",
    "href": "interaction/gauderman_2025_40763299.html#methods-and-results",
    "title": "Pathway polygenic risk scores (pPRS) for the analysis of gene-environment interaction",
    "section": "Methods and Results",
    "text": "Methods and Results\n\nSimulation Studies\nThrough extensive simulation studies, the researchers demonstrated that testing a targeted pPRS x E interaction yields substantially greater statistical power compared to testing the interaction using a broad, overall PRS.\n\n\nEmpirical Application: Colorectal Cancer (CRC) and NSAIDs\nThe pPRS method was applied to a large case-control study (N = 78,253) of colorectal cancer (CRC), using non-steroidal anti-inflammatory drugs (NSAIDs) as the environmental factor, a known protective exposure for CRC.\n\nOverall PRS Result: No evidence of an overall PRS x NSAIDs interaction was observed (\\(p = 0.41\\)).\npPRS Result: A highly significant pPRS x NSAIDs interaction (\\(p = 0.0003\\)) was identified based on SNPs restricted to the TGF-\\(\\beta\\)/gonadotropin releasing hormone receptor (GRHR) pathway.\n\n\n\nInterpretation of Empirical Results\nThe interaction analysis showed that the protective effect of NSAIDs against CRC was significantly stronger among individuals with higher genetic risk captured by the TGF-\\(\\beta\\)/GRHR pPRS. For example, the odds ratio (OR) for NSAIDs protecting against CRC was 0.84 for individuals at the 5th percentile of the pPRS (low risk) but significantly better at 0.70 for those at the 95th percentile (high risk).",
    "crumbs": [
      "interaction",
      "Papers",
      "Pathway polygenic risk scores (pPRS) for the analysis of gene-environment interaction"
    ]
  },
  {
    "objectID": "interaction/gauderman_2025_40763299.html#conclusions-and-recommendations",
    "href": "interaction/gauderman_2025_40763299.html#conclusions-and-recommendations",
    "title": "Pathway polygenic risk scores (pPRS) for the analysis of gene-environment interaction",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe pPRS approach successfully addresses the power limitations of standard PRS in GxE analysis. From a biological perspective, this suggests that NSAIDs may act to reduce CRC risk specifically through genes within the identified pathways. From a population health perspective, the findings support a precision prevention approach, suggesting that focusing on genetic susceptibility within biologically informed pathways may be more sensitive for identifying individuals who would benefit most from specific prevention efforts. The authors recommend the use of pPRS to integrate biological insight and maximize statistical power in future GxE studies.",
    "crumbs": [
      "interaction",
      "Papers",
      "Pathway polygenic risk scores (pPRS) for the analysis of gene-environment interaction"
    ]
  },
  {
    "objectID": "proteomics/pietzner_2025_41068475.html",
    "href": "proteomics/pietzner_2025_41068475.html",
    "title": "Machine learning-guided deconvolution of plasma protein levels",
    "section": "",
    "text": "PubMed: 41068475 DOI: 10.1038/s44320-025-00158-6 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "proteomics",
      "Papers",
      "Machine learning-guided deconvolution of plasma protein levels"
    ]
  },
  {
    "objectID": "proteomics/pietzner_2025_41068475.html#key-findings-deconvoluting-the-sources-of-plasma-protein-variation",
    "href": "proteomics/pietzner_2025_41068475.html#key-findings-deconvoluting-the-sources-of-plasma-protein-variation",
    "title": "Machine learning-guided deconvolution of plasma protein levels",
    "section": "Key Findings: Deconvoluting the Sources of Plasma Protein Variation",
    "text": "Key Findings: Deconvoluting the Sources of Plasma Protein Variation\nThis study used a machine learning (ML) approach to systematically identify and quantify the key factors that determine the variation in thousands of plasma protein levels, aiming to overcome the challenge of limited understanding of protein origins that hampers biomarker translation.\n\nPrimary Determinants of Protein Levels\nThe ML model, which assessed over 1,800 participant and sample characteristics, found that a median of 20 factors (ranging from 1 to 37) jointly explained an average of 19.4% (up to 100.0%) of the variance in approximately 3,000 protein targets.\nCrucially, modifiable characteristics (e.g., health metrics, disease status, lifestyle) explained significantly more variance (median: 10.0%) compared to genetic variation (median: 3.9%). This suggests that dynamic, non-genetic factors are the primary drivers of plasma protein differences between individuals.\n\n\nSegregation and Clustering\nProteins were found to segregate into distinct clusters based on their shared explanatory factors. These clusters revealed proteins primarily driven by: * Human Health and Disease: Indicators of health status and disease. * Pre-analytical Variation: Technical and sample-handling measures, such as accidental activation of platelets.\n\n\nAncestry, Sex, and Robustness\nThe overall explanatory factors were largely consistent across different sexes and ancestral groups. However, the analysis identified specific proteins where the underlying explanatory factors differed by: * Sex: 1,374 proteins. * Ancestry: 74 proteins.\n\n\nResource and Application\nThe study establishes a valuable resource to guide biomarker and drug target discovery, including: 1. Knowledge Graph: An integrated knowledge graph linking the identified explanatory factors with genetic studies and drug characteristics, intended to guide the identification of drug target engagement markers. 2. Biomarker Identification: Demonstrated utility by identifying disease-specific biomarkers, such as matrix metalloproteinase 12 (MMP12) for abdominal aortic aneurysm. 3. Framework: Developed a widely applicable R package and an interactive web portal for researchers to explore all results and integrate the findings into ongoing studies.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Machine learning-guided deconvolution of plasma protein levels"
    ]
  },
  {
    "objectID": "proteomics/pietzner_2025_41068475.html#methods",
    "href": "proteomics/pietzner_2025_41068475.html#methods",
    "title": "Machine learning-guided deconvolution of plasma protein levels",
    "section": "Methods",
    "text": "Methods\n\nCohort: 43,240 participants from the UK Biobank.\nData: Approximately 3,000 plasma proteins were measured, alongside &gt;1,800 participant and sample characteristics.\nAnalysis: Machine learning was used to identify and quantify the variance explained by different factors, with models being consistent across sexes and ancestral groups.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Machine learning-guided deconvolution of plasma protein levels"
    ]
  },
  {
    "objectID": "proteomics/fu_2025_39810024.html",
    "href": "proteomics/fu_2025_39810024.html",
    "title": "Decoding the functional impact of the cancer genome through protein-protein interactions",
    "section": "",
    "text": "PubMed: 39810024 DOI: 10.1038/s41568-024-00784-6 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "proteomics",
      "Papers",
      "Decoding the functional impact of the cancer genome through protein-protein interactions"
    ]
  },
  {
    "objectID": "proteomics/fu_2025_39810024.html#key-findings-linking-oncogenic-mutations-to-ppi-network-rewiring",
    "href": "proteomics/fu_2025_39810024.html#key-findings-linking-oncogenic-mutations-to-ppi-network-rewiring",
    "title": "Decoding the functional impact of the cancer genome through protein-protein interactions",
    "section": "Key Findings: Linking Oncogenic Mutations to PPI Network Rewiring",
    "text": "Key Findings: Linking Oncogenic Mutations to PPI Network Rewiring\nThis review focuses on the paradigm that Protein-Protein Interactions (PPIs) are the critical functional intermediaries between genomic mutations and the resulting oncogenic phenotype. The core assertion is that the functional impact of individual genomic alterations is often transmitted through altered nodes and hubs of PPIs, leading to the rewiring of molecular signaling cascades.\n\nMutational Effects on PPIs: NeoPPIs and HypoPPIs\nOncogenic mutations frequently map to the protein surface, particularly at or near the hotspot residues of the PPI interface. These mutations can perturb the interactome in two primary ways:\n\nNeomorphic PPIs (neoPPIs): Oncogenic mutations may lead to modified residues that create new contact sites or neo-epitopes, enabling interactions with other proteins that the wild-type protein does not typically bind to.\nHypomorphic PPIs (hypoPPIs): Conversely, mutations can decrease or disrupt the interaction of existing protein complexes, leading to a loss-of-function phenotype.\n\nUnderstanding the mechanisms of these mutation-driven, differential PPIs is crucial for deciphering tumor heterogeneity and developing precision oncology strategies.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Decoding the functional impact of the cancer genome through protein-protein interactions"
    ]
  },
  {
    "objectID": "proteomics/fu_2025_39810024.html#molecular-basis-of-cancer-associated-ppi-perturbation",
    "href": "proteomics/fu_2025_39810024.html#molecular-basis-of-cancer-associated-ppi-perturbation",
    "title": "Decoding the functional impact of the cancer genome through protein-protein interactions",
    "section": "Molecular Basis of Cancer-Associated PPI Perturbation",
    "text": "Molecular Basis of Cancer-Associated PPI Perturbation\nThe functional outcome of a driver mutation is determined by its location and the nature of the amino acid substitution, which is why a mutation-focused approach is necessary, rather than just a driver gene-focused approach.\n\nSite-Specific and Lineage-Dependent Effects\n\nDifferent mutations within the same gene can affect various structural components, including defined protein domains, Short Linear Motifs (SLIMs), and Intrinsically Disordered Regions (IDRs).\nThe same gene can be an oncogenic driver in one cancer type and a tumor suppressor in another. For example, different hotspot mutations in PIK3CA (in the helical vs. kinase domain) are prevalent in different cancer types (cervical vs. breast cancer, respectively), exhibiting differential oncogenic activities.\nEven different amino acid substitutions at the same hotspot position can lead to distinct functional outcomes, as seen with IDH1 R132H (associated with a less-aggressive phenotype in gliomas) and IDH1 R132C (associated with enhanced proliferation in AML), due to differential neo-enzymatic activity.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Decoding the functional impact of the cancer genome through protein-protein interactions"
    ]
  },
  {
    "objectID": "proteomics/fu_2025_39810024.html#technologies-for-ppi-identification-and-prediction",
    "href": "proteomics/fu_2025_39810024.html#technologies-for-ppi-identification-and-prediction",
    "title": "Decoding the functional impact of the cancer genome through protein-protein interactions",
    "section": "Technologies for PPI Identification and Prediction",
    "text": "Technologies for PPI Identification and Prediction\nThe review summarizes the experimental and computational methods used to study mutation-affected PPIs:\n\nExperimental Monitoring\nExperimental approaches are categorized into:\n\nCo-complex affinity-based technologies: These detect co-complex formation and include methods like co-immunoprecipitation, and Affinity Purification coupled with Mass Spectrometry (AP-MS).\nProximity-based assays: These detect interactions based on close spatial proximity and include FRET, BRET, Proximity Ligation Assays (PLAs), and yeast two-hybrid (Y2H) systems.\n\n\n\nComputational and AI Methods\n\nComputational tools (e.g., FoldX, Flex ddG) quantify mutation-induced changes in free binding energy.\nAI/Machine Learning methods are rapidly advancing the prediction of protein and protein complex structures. Recent models like AlphaFold3 (AF3) and AlphaFold Multimer can predict the structures of protein-protein, protein-DNA, and protein-ligand complexes. However, these models have limitations, such as difficulty in predicting mutational effects in highly flexible regions or capturing dynamic conformational transitions.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Decoding the functional impact of the cancer genome through protein-protein interactions"
    ]
  },
  {
    "objectID": "proteomics/fu_2025_39810024.html#therapeutic-potential",
    "href": "proteomics/fu_2025_39810024.html#therapeutic-potential",
    "title": "Decoding the functional impact of the cancer genome through protein-protein interactions",
    "section": "Therapeutic Potential",
    "text": "Therapeutic Potential\nThe intersection of cancer variants and altered PPI interfaces opens up a new frontier for developing tumor-selective therapeutic strategies:\n\nTargeting Mutant Residues: The development of agents that specifically target the modified residues of the driver protein itself (e.g., KRAS(G12C)-targeted therapies).\nModulating NeoPPIs and HypoPPIs: New therapeutic strategies involve designing small-molecule PPI modulators to either disrupt oncogenic neoPPIs or to restore function to lost hypoPPIs (e.g., using small-molecule glues to re-establish a suppressed interaction). This approach represents a path toward personalized medicine by directly addressing the network vulnerabilities created by specific driver mutations.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Decoding the functional impact of the cancer genome through protein-protein interactions"
    ]
  },
  {
    "objectID": "proteomics/index.html",
    "href": "proteomics/index.html",
    "title": "Proteomics",
    "section": "",
    "text": "Longitudinal multi-omics study reveals common etiology underlying association between plasma proteome and BMI trajectories in adolescent and young adult twins\n\n\n\nDesign: A longitudinal multi-omics twin study (NTR and FinnTwin12 cohorts) examining the association between plasma protein levels and changes in BMI (trajectories) over approximately a decade.\nKey Finding: The association between the plasma proteome and BMI trajectories is largely explained by shared genetic factors and common environmental influences, pointing to a common underlying etiology.\nCausal Link: Mendelian Randomization analysis identified Apolipoprotein B (ApoB) as a key protein, providing evidence that genetically determined ApoB levels have a causal effect on BMI, but not the reverse.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nImputation of label-free quantitative mass spectrometry-based proteomics data using self-supervised deep learning\n\n\n\nGoal: To develop and validate PIMMS (Proteomics Imputation Modeling Mass Spectrometry), a deep learning-based framework for imputing missing values in label-free quantitative (LFQ) mass spectrometry data.\nMethod: PIMMS leverages three self-supervised models—Collaborative Filtering (CF), Denoising Autoencoder (DAE), and Variational Autoencoder (VAE)—which significantly outperformed 27 other imputation methods, including median imputation and R-based KNN, on simulated Missing Not At Random (MNAR) data.\nImpact: Applying PIMMS-VAE to a clinical ALD cohort identified 30 additional significantly differentially abundant proteins (+13.2%) compared to non-imputed data, demonstrating that DL imputation can enhance the biological conclusions derived from proteomics analysis.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nBlood protein assessment of leading incident diseases and mortality in the UK Biobank\n\n\n\nObjective: To identify and validate protein biomarkers for the 10-year incidence of 23 age-related diseases and all-cause mortality using proteomics data from 47,600 UK Biobank participants.\nKey Result: Multi-protein risk scores (ProteinScores), developed using penalized Cox regression, significantly improved the Area Under the Curve (AUC) for the 10-year onset prediction of six major outcomes, including all-cause mortality, coronary artery disease (CAD), and Type 2 diabetes (T2D), even after adjusting for 24 comprehensive clinical and lifestyle factors.\nImplication: The ProteinScores capture independent biological information related to underlying aging and systemic disease risk not found in standard clinical measures, validating the use of multi-protein panels for enhanced personalized risk stratification.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nProteomic signatures improve risk prediction for common and rare diseases\n\n\n\nObjective: This large-scale study demonstrated the ability of plasma proteomic signatures to enhance the 10-year incidence risk prediction for 218 common and rare diseases in the UK Biobank (UKB-PPP) cohort.\nResult: Sparse proteomic models (using 5–20 proteins) significantly improved the C-index over models based on basic clinical information for 67 diseases, including hard-to-diagnose conditions like multiple myeloma and motor neuron disease.\nRobustness/Confounding: The analysis highlighted that residual confounding is a major issue, as over 80% of initial associations attenuated after adjusting for demographic and clinical factors, underscoring the necessity of understanding protein determinants to ensure findings are biologically relevant and not spurious.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nMapping the proteo-genomic convergence of human diseases\n\n\n\nObjective: To construct a proteo-genomic map by systematically linking genetic risk for hundreds of human diseases and traits to variations in the levels of ~3,000 plasma proteins using data from &gt;54,000 individuals.\nKey Result: Identified 2,228 instances of genetic colocalization across 1,440 proteins and 498 diseases/traits, indicating that a substantial portion of genetic disease risk is mediated by altered protein levels.\nCausal Inference: Through Mendelian Randomization (MR), the study robustly predicted 44 proteins to be causally linked to 37 diseases/traits (e.g., CRP and CAD), thereby providing high-priority candidates for drug targets.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nCurrent landscape of plasma proteomics from technical innovations to biological insights and biomarker discovery\n\n\n\nTopic: A systematic benchmarking and technical evaluation of the current landscape of plasma proteomics, directly comparing different affinity-based and mass spectrometry platforms.\nMethod: Eight state-of-the-art proteomics platforms were applied to the same human plasma cohort to compare performance across over 13,000 proteins, assessing metrics like coverage, sensitivity, and reproducibility.\nImpact: The study provides a critical resource for researchers, detailing the trade-offs in protein coverage and identifying the complementary strengths and limitations of each platform to guide informed decision-making for biomarker discovery.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nProteomic prediction of disease largely reflects environmental risk exposure\n\n\n\nCore Finding: The high disease predictive value of the plasma proteome primarily reflects its sensitivity as a quantitative readout of environmental risk factors (like smoking and alcohol intake), rather than identifying proteins that are causal drivers of disease.\nCausality Assessment: Using Mendelian Randomization (MR) on thousands of protein-disease associations in the UK Biobank, the study found only 8% showed suggestive evidence of a causal relationship, confirming that causal drivers are rare and disease-specific.\nEnvironmental Biomarkers: The vast majority of non-causal proteins, particularly those broadly associated with multiple diseases, were found to be exposure-associated. The developed Proteomic Score for Smoking (SmokingPS) achieved an AUC of 0.96, validating the proteome’s role as an objective, quantitative index of lifestyle behaviors.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nDecoding the functional impact of the cancer genome through protein-protein interactions\n\n\n\nCore Concept: This review establishes that oncogenic driver mutations exert their functional impact largely by rewiring molecular signaling networks through the alteration of Protein-Protein Interactions (PPIs).\nMechanism: Mutations at the protein surface, particularly at PPI interfaces, can lead to the creation of neomorphic PPIs (neoPPIs) or the loss of existing hypomorphic PPIs (hypoPPIs), necessitating a mutation-focused analysis.\nTherapeutic Implication: The mutation-directed PPIs are presented as a new class of targets for precision oncology, paving the way for the development of small-molecule modulators that can selectively disrupt oncogenic neoPPIs or restore tumor-suppressive hypoPPIs.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nGenetic associations with ratios between protein levels detect new pQTLs and reveal protein-protein interactions\n\n\n\nNovel Method: Introduces Ratio-QTLs (rQTLs), a proteogenomic method that analyzes the genetic determinants of ratios between pairs of plasma protein levels rather than individual protein levels.\nKey Finding: The rQTL approach provided an enormous increase in statistical power, strengthening associations at known pQTL loci by several hundred orders of magnitude (p-gain) and enabling the discovery of new cis-pQTLs.\nBiological Relevance: rQTLs were 7.6-fold enriched in established Protein-Protein Interactions (PPIs), confirming the method’s unique ability to uncover genetic variants that regulate the functional interaction or shared biological regulation between two proteins.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nMachine learning-guided deconvolution of plasma protein levels\n\n\n\nObjective: Used machine learning (ML) to systematically identify and quantify the contribution of over 1,800 characteristics (health, genetic, technical) to the variation in approximately 3,000 plasma protein levels across 43,240 UK Biobank individuals.\nKey Result: A median of 20 factors explained an average of 19.4% of protein variance. Modifiable characteristics (median: 10.0%) were found to explain significantly more variation than genetic factors (median: 3.9%).\nImplication: The study provides a crucial resource (knowledge graph, R package) and framework for understanding protein origins, clustering proteins by their drivers (e.g., disease, pre-analytical factors), and guiding the identification of biologically relevant biomarkers and drug target engagement markers.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nPWAS: proteome-wide association study-linking genes and phenotypes by functional variation in proteins\n\n\n\nNovel Method: Introduces PWAS (Proteome-Wide Association Study), a protein-centric method that detects gene-phenotype associations by quantifying the cumulative functional damage caused by coding-region variants on the resulting protein product using a machine learning model called FIRM.\nKey Advantage: PWAS is specifically designed to model and detect non-additive heritability, demonstrating its power in identifying associations under the recessive inheritance model, which is often missed by standard GWAS.\nDiscovery Power: In analysis using the UK Biobank, PWAS uncovered numerous gene-phenotype associations unique from standard GWAS, including detecting the known colorectal cancer gene MUTYH with high significance under its characteristic recessive mode.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "proteomics",
      "Papers"
    ]
  },
  {
    "objectID": "proteomics/pietzner_2021_34648354.html",
    "href": "proteomics/pietzner_2021_34648354.html",
    "title": "Mapping the proteo-genomic convergence of human diseases",
    "section": "",
    "text": "PubMed: 34648354 DOI: 10.1126/science.abj1541 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "proteomics",
      "Papers",
      "Mapping the proteo-genomic convergence of human diseases"
    ]
  },
  {
    "objectID": "proteomics/pietzner_2021_34648354.html#key-findings-linking-genetic-risk-to-disease-via-plasma-proteins",
    "href": "proteomics/pietzner_2021_34648354.html#key-findings-linking-genetic-risk-to-disease-via-plasma-proteins",
    "title": "Mapping the proteo-genomic convergence of human diseases",
    "section": "Key Findings: Linking Genetic Risk to Disease via Plasma Proteins",
    "text": "Key Findings: Linking Genetic Risk to Disease via Plasma Proteins\nThis study provides a comprehensive “proteo-genomic map” that links genetic risk for hundreds of human diseases and traits to changes in the levels of thousands of circulating plasma proteins. The core innovation is the systematic use of genetic colocalization to identify instances where the same genetic variant influences both a plasma protein level (as a pQTL) and a clinical disease/trait (as a GWAS hit).\nThe map provides two major insights:\n\nProteins Mediate Genetic Risk: The study identified 2,228 instances of genetic colocalization across 1,440 proteins and 498 diseases/traits. This finding suggests that a significant fraction of genetic disease risk acts by altering the level of a specific circulating protein.\nCausal Inference and Drug Targets: By integrating these colocalization events with Mendelian Randomization (MR), the map pinpoints proteins that are likely to be causally related to a disease, making them high-priority candidates for therapeutic drug targeting.\n\n\nColocalization and Mendelian Randomization\n\nColocalization: The analysis used a Bayesian method to confirm that the genetic signal for a plasma protein (pQTL) and the genetic signal for a disease (GWAS) at a given locus are driven by the same causal variant. This identified 864 proteins with a shared genetic signal with at least one clinical trait.\nCausal Relationships: Integrating this information with MR, the study identified 44 proteins that were robustly predicted to be causally linked to 37 diseases/traits, often validating established biological pathways. For example, C-reactive protein (CRP) was found to be causally associated with increased risk for coronary artery disease and other inflammatory conditions.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Mapping the proteo-genomic convergence of human diseases"
    ]
  },
  {
    "objectID": "proteomics/pietzner_2021_34648354.html#methods-and-design",
    "href": "proteomics/pietzner_2021_34648354.html#methods-and-design",
    "title": "Mapping the proteo-genomic convergence of human diseases",
    "section": "Methods and Design",
    "text": "Methods and Design\n\nData Sources\n\nProteomics: Measured levels of ~3,000 plasma proteins in over 54,000 individuals from the UK Biobank Pharma Proteomics Project (UKB-PPP) and other cohorts.\nGenetics (pQTLs): Genome-wide association study (GWAS) summary statistics for protein quantitative trait loci (pQTLs).\nDisease/Trait Genetics (GWAS): GWAS summary statistics for 498 diseases and complex traits.\n\n\n\nAnalytical Framework\n\nGWAS for Proteins and Traits: Conducted GWAS for all proteins and aggregated existing GWAS results for traits.\nGenetic Colocalization: Performed systematic colocalization analysis between all protein pQTLs and all trait GWAS loci to find shared genetic drivers.\nCausal Inference (MR): Applied Mendelian Randomization to the colocalized pairs to determine the likely causal direction (i.e., whether the protein level causes the disease or vice versa).\nKnowledge Graph: Constructed a “proteo-genomic knowledge graph” to visualize and connect the colocalized and causal protein-disease relationships.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Mapping the proteo-genomic convergence of human diseases"
    ]
  },
  {
    "objectID": "proteomics/pietzner_2021_34648354.html#implications-for-biology-and-drug-discovery",
    "href": "proteomics/pietzner_2021_34648354.html#implications-for-biology-and-drug-discovery",
    "title": "Mapping the proteo-genomic convergence of human diseases",
    "section": "Implications for Biology and Drug Discovery",
    "text": "Implications for Biology and Drug Discovery\n\nConvergence of Diseases\nThe proteo-genomic map revealed convergence points where the genetic signals for multiple diseases colocalized with the same protein. This means that genetic variation affecting a single protein can predispose an individual to several different, often seemingly unrelated, conditions (e.g., genetic variation at the SULT2A1 locus linked to increased SULT2A1 protein activity and a higher risk of gallstones).\n\n\nPrioritization of Drug Targets\nThe study’s causal protein-disease links provide a strong basis for prioritizing drug development. If a protein is causally linked to a disease, modulating that protein’s level with a drug is likely to be therapeutically effective. The study validated known drug targets and also highlighted new potential targets based on the strength of the genetic evidence.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Mapping the proteo-genomic convergence of human diseases"
    ]
  },
  {
    "objectID": "proteomics/pietzner_2021_34648354.html#conclusions",
    "href": "proteomics/pietzner_2021_34648354.html#conclusions",
    "title": "Mapping the proteo-genomic convergence of human diseases",
    "section": "Conclusions",
    "text": "Conclusions\nThe proteo-genomic map serves as a fundamental resource for understanding the molecular mechanisms underlying genetic disease risk. By demonstrating that genetic variation for many diseases converges on a shared set of plasma proteins, the study validates plasma proteomics as a key layer for translational medicine, facilitating drug target discovery and providing a causal foundation for biomarker development.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Mapping the proteo-genomic convergence of human diseases"
    ]
  },
  {
    "objectID": "proteomics/gadd_2024_38987645.html",
    "href": "proteomics/gadd_2024_38987645.html",
    "title": "Blood protein assessment of leading incident diseases and mortality in the UK Biobank",
    "section": "",
    "text": "PubMed: 38987645 DOI: 10.1038/s43587-024-00655-7 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "proteomics",
      "Papers",
      "Blood protein assessment of leading incident diseases and mortality in the UK Biobank"
    ]
  },
  {
    "objectID": "proteomics/gadd_2024_38987645.html#key-findings-protein-scores-predict-incident-disease-and-mortality",
    "href": "proteomics/gadd_2024_38987645.html#key-findings-protein-scores-predict-incident-disease-and-mortality",
    "title": "Blood protein assessment of leading incident diseases and mortality in the UK Biobank",
    "section": "Key Findings: Protein Scores Predict Incident Disease and Mortality",
    "text": "Key Findings: Protein Scores Predict Incident Disease and Mortality\nThis large-scale study leveraged proteomics data from the UK Biobank (n=47,600) to identify and validate protein biomarkers for the risk of 23 common age-related diseases and all-cause mortality. The core finding is that multi-protein scores (“ProteinScores”) significantly enhance the prediction of incident diseases and mortality, even when accounting for comprehensive clinical and lifestyle information.\n\nDiscovery of Associations\n\nThe study reported 3,209 associations between 963 unique plasma protein levels and 21 incident outcomes (including diseases and mortality).\nCardiovascular disease (CVD), specifically coronary artery disease (CAD) and atrial fibrillation (AF), had the largest number of associated proteins, highlighting the strong systemic link between the proteome and heart health.\n\n\n\nPredictive Power of ProteinScores\n\nProteinScores were developed using penalized Cox regression (elastic net) to combine multiple protein measurements into a single risk score for each outcome.\nThese scores were applied to independent test sets and were found to improve the Area Under the Curve (AUC) estimates for the 10-year onset of incident outcomes, beyond a minimally adjusted model that already included age, sex, and a comprehensive set of 24 lifestyle and clinical factors.\nProteinScores were validated for six major outcomes, including:\n\nAll-cause mortality\nCoronary artery disease (CAD)\nAtrial fibrillation (AF)\nType 2 diabetes (T2D)\nColorectal cancer (CRC)\nGlaucoma",
    "crumbs": [
      "proteomics",
      "Papers",
      "Blood protein assessment of leading incident diseases and mortality in the UK Biobank"
    ]
  },
  {
    "objectID": "proteomics/gadd_2024_38987645.html#methods-and-design",
    "href": "proteomics/gadd_2024_38987645.html#methods-and-design",
    "title": "Blood protein assessment of leading incident diseases and mortality in the UK Biobank",
    "section": "Methods and Design",
    "text": "Methods and Design\n\nCohort and Data\n\nParticipants: Up to 47,600 individuals from the UK Biobank.\nProteome: Measured levels for 1,468 plasma proteins using the Olink Proximity Extension Assay (PEA) platform.\nOutcomes: Incident diagnoses for 23 age-related diseases and all-cause mortality, monitored over a 10-year follow-up period.\n\n\n\nStatistical Modeling\n\nIndividual Protein Associations: Cox proportional hazards (PH) models were used to test the association between each individual protein and each outcome.\nProteinScore Development: Penalized Cox regression (elastic net) was employed across 50 randomized iterations of training/testing to select and weight the most predictive proteins for each outcome, resulting in a single, robust ProteinScore.\nBenchmarking: ProteinScore performance was rigorously compared to a minimally adjusted model (age, sex, and 24 clinical/lifestyle factors) using the incremental AUC difference (\\(\\Delta\\)AUC), with 10-year AUC being the primary metric.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Blood protein assessment of leading incident diseases and mortality in the UK Biobank"
    ]
  },
  {
    "objectID": "proteomics/gadd_2024_38987645.html#results-biological-insights-and-clinical-relevance",
    "href": "proteomics/gadd_2024_38987645.html#results-biological-insights-and-clinical-relevance",
    "title": "Blood protein assessment of leading incident diseases and mortality in the UK Biobank",
    "section": "Results: Biological Insights and Clinical Relevance",
    "text": "Results: Biological Insights and Clinical Relevance\n\nMortality and Aging\nThe ProteinScore for all-cause mortality was consistently one of the strongest performers. The proteins contributing most to the mortality score were related to fundamental biological pathways such as inflammation (e.g., C-Reactive Protein, CRP) and cellular stress/damage. The study demonstrated that protein levels capture biological information related to underlying aging processes that are independent of standard clinical risk factors.\n\n\nDisease Specificity\n\nCAD and AF: ProteinScores for these cardiovascular outcomes showed significant predictive improvement, highlighting the utility of proteomics in risk stratification for heart disease.\nCRC and Glaucoma: The successful prediction of these conditions demonstrates that plasma proteomics can capture systemic signals of diseases that are often viewed as localized.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Blood protein assessment of leading incident diseases and mortality in the UK Biobank"
    ]
  },
  {
    "objectID": "proteomics/gadd_2024_38987645.html#conclusions-and-recommendations",
    "href": "proteomics/gadd_2024_38987645.html#conclusions-and-recommendations",
    "title": "Blood protein assessment of leading incident diseases and mortality in the UK Biobank",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study confirms that plasma protein levels are powerful predictors of future health outcomes and mortality. The ProteinScore approach provides a robust, validated, and clinically actionable method for integrating this proteomic information into risk stratification models. The authors advocate for the routine use of multi-protein panels to improve personalized risk assessment for major age-related diseases and overall longevity.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Blood protein assessment of leading incident diseases and mortality in the UK Biobank"
    ]
  },
  {
    "objectID": "proteomics/drouard_2023_38129841.html",
    "href": "proteomics/drouard_2023_38129841.html",
    "title": "Longitudinal multi-omics study reveals common etiology underlying association between plasma proteome and BMI trajectories in adolescent and young adult twins",
    "section": "",
    "text": "PubMed: 38129841 DOI: 10.1186/s12916-023-03198-7 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "proteomics",
      "Papers",
      "Longitudinal multi-omics study reveals common etiology underlying association between plasma proteome and BMI trajectories in adolescent and young adult twins"
    ]
  },
  {
    "objectID": "proteomics/drouard_2023_38129841.html#key-findings-shared-etiology-of-plasma-proteome-and-bmi-trajectories",
    "href": "proteomics/drouard_2023_38129841.html#key-findings-shared-etiology-of-plasma-proteome-and-bmi-trajectories",
    "title": "Longitudinal multi-omics study reveals common etiology underlying association between plasma proteome and BMI trajectories in adolescent and young adult twins",
    "section": "Key Findings: Shared Etiology of Plasma Proteome and BMI Trajectories",
    "text": "Key Findings: Shared Etiology of Plasma Proteome and BMI Trajectories\nThis longitudinal multi-omics study utilized a twin design to dissect the influence of genetic and environmental factors on the association between the plasma proteome and Body Mass Index (BMI) trajectories during adolescence and young adulthood.\nThe central finding is that the observed associations between protein levels and BMI trajectories (changes in BMI over time) are largely attributable to common etiological factors, specifically:\n\nShared Genetic Factors (A): Genetic effects explained a significant portion of the covariance between plasma proteins and BMI, suggesting that heritable factors simultaneously influence both protein expression and adiposity development.\nShared Environmental Factors (C): Common environmental influences also played a role in explaining the protein-BMI association.\n\n\nIdentification of Key Proteins and Causal Links\nThe study identified 38 plasma proteins whose levels were significantly associated with an individual’s BMI trajectory. Clustering analysis showed these proteins grouped into pathways related to lipid transport/metabolism (e.g., Apolipoproteins) and immune/inflammatory response.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Longitudinal multi-omics study reveals common etiology underlying association between plasma proteome and BMI trajectories in adolescent and young adult twins"
    ]
  },
  {
    "objectID": "proteomics/drouard_2023_38129841.html#methods-and-study-design",
    "href": "proteomics/drouard_2023_38129841.html#methods-and-study-design",
    "title": "Longitudinal multi-omics study reveals common etiology underlying association between plasma proteome and BMI trajectories in adolescent and young adult twins",
    "section": "Methods and Study Design",
    "text": "Methods and Study Design\nThe study employed a rigorous, longitudinal twin design spanning approximately 10 years of follow-up.\n\nCohorts and Data\n\nParticipants: Two independent cohorts of twins: FinnTwin12 (N=651) and the Netherlands Twin Register (NTR) (N=665).\nPhenotypes: BMI was measured four times per individual over the follow-up period to model BMI trajectories.\nOmics Data: Plasma proteomics (Olink Proximity Extension Assays), metabolomics, and genotype data were utilized.\n\n\n\nStatistical Analysis\n\nTwin Modeling: Structural equation modeling (using ACE models) was applied to estimate the proportion of variance and covariance in protein levels and BMI trajectories explained by additive genetics (A), common environment (C), and unique environment (E).\nMendelian Randomization (MR): Two-sample MR was performed using public GWAS and pQTL data to investigate potential causal effects between genetically determined protein levels and BMI, and vice-versa.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Longitudinal multi-omics study reveals common etiology underlying association between plasma proteome and BMI trajectories in adolescent and young adult twins"
    ]
  },
  {
    "objectID": "proteomics/drouard_2023_38129841.html#results-focus-on-apolipoprotein-b",
    "href": "proteomics/drouard_2023_38129841.html#results-focus-on-apolipoprotein-b",
    "title": "Longitudinal multi-omics study reveals common etiology underlying association between plasma proteome and BMI trajectories in adolescent and young adult twins",
    "section": "Results: Focus on Apolipoprotein B",
    "text": "Results: Focus on Apolipoprotein B\nA key protein highlighted by the analysis was Apolipoprotein B (ApoB).\n\nGenetic Correlation: ApoB showed a strong genetic correlation with BMI, suggesting the genetic architecture underlying ApoB concentration substantially overlaps with the genetic architecture of BMI.\nCausality: The MR analysis provided evidence for a causal effect of ApoB on BMI, meaning genetically predicted higher ApoB levels lead to higher BMI. Conversely, there was no evidence suggesting that genetically predicted BMI causally affects ApoB levels.\nMetabolite Connection: The ApoB-BMI association was also linked to other metabolic markers, reinforcing ApoB’s role as a central, genetically mediated biomarker in adiposity.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Longitudinal multi-omics study reveals common etiology underlying association between plasma proteome and BMI trajectories in adolescent and young adult twins"
    ]
  },
  {
    "objectID": "proteomics/drouard_2023_38129841.html#conclusions-and-recommendations",
    "href": "proteomics/drouard_2023_38129841.html#conclusions-and-recommendations",
    "title": "Longitudinal multi-omics study reveals common etiology underlying association between plasma proteome and BMI trajectories in adolescent and young adult twins",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study concludes that the association between the plasma proteome and BMI trajectories is predominantly driven by shared genetic and common environmental factors, providing a strong biological basis for the co-occurrence of these traits. The findings prioritize the ApoB-coding gene (APOB) as a genetically determined factor with a likely causal influence on BMI, supporting its role as a potential therapeutic target in the early development of adiposity. The multi-omics approach, especially the use of longitudinal twin data, offers a powerful framework for dissecting the complex etiology of chronic diseases.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Longitudinal multi-omics study reveals common etiology underlying association between plasma proteome and BMI trajectories in adolescent and young adult twins"
    ]
  },
  {
    "objectID": "expression/pirrotta_2024_39363890.html",
    "href": "expression/pirrotta_2024_39363890.html",
    "title": "Exploring public cancer gene expression signatures across bulk, single-cell and spatial transcriptomics data with signifinder Bioconductor package",
    "section": "",
    "text": "PubMed: 39363890 DOI: 10.1093/nargab/lqae138 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "expression",
      "Papers",
      "Exploring public cancer gene expression signatures across bulk, single-cell and spatial transcriptomics data with signifinder Bioconductor package"
    ]
  },
  {
    "objectID": "expression/pirrotta_2024_39363890.html#core-problem-and-study-goal",
    "href": "expression/pirrotta_2024_39363890.html#core-problem-and-study-goal",
    "title": "Exploring public cancer gene expression signatures across bulk, single-cell and spatial transcriptomics data with signifinder Bioconductor package",
    "section": "Core Problem and Study Goal",
    "text": "Core Problem and Study Goal\nThe vast and growing collection of published gene expression signatures (GESs)—lists of genes whose expression collectively characterizes a specific biological state, such as a cancer subtype or prognosis—are often derived from bulk RNA-Seq data. Modern transcriptomic studies increasingly rely on high-resolution methods like single-cell RNA-Seq (scRNA-Seq) and Spatial Transcriptomics (ST). The challenge is that these bulk-derived signatures often fail to perform consistently or effectively when directly applied to the noisier, higher-resolution data.\nThis study introduces the signifinder Bioconductor package, a comprehensive computational workflow designed to allow researchers to effectively investigate the behavior and robustness of known GESs across bulk, single-cell, and spatial transcriptomics data types.",
    "crumbs": [
      "expression",
      "Papers",
      "Exploring public cancer gene expression signatures across bulk, single-cell and spatial transcriptomics data with signifinder Bioconductor package"
    ]
  },
  {
    "objectID": "expression/pirrotta_2024_39363890.html#methods-the-signifinder-bioconductor-package",
    "href": "expression/pirrotta_2024_39363890.html#methods-the-signifinder-bioconductor-package",
    "title": "Exploring public cancer gene expression signatures across bulk, single-cell and spatial transcriptomics data with signifinder Bioconductor package",
    "section": "Methods: The signifinder Bioconductor Package",
    "text": "Methods: The signifinder Bioconductor Package\nThe signifinder package provides a streamlined workflow implemented in R for the re-analysis and comparison of gene expression signatures.\n\n1. Signature Collection and Input\n\nInput: The package accepts user-defined GESs or a large compendium of over 100 publicly available, manually curated breast cancer (BC) signatures, allowing for standardized testing.\nData Handling: It handles transcriptomic data from various sources (e.g., TCGA, GEO) and formats, including count and normalized data for bulk, scRNA-Seq, and ST.\n\n\n\n2. Signature Scoring and Analysis\nThe package integrates multiple popular scoring methods, making it flexible for cross-platform comparison: * Bulk Data: Uses standard methods like Z-scores and GSVA (Gene Set Variation Analysis). * Single-Cell/Spatial Data: Integrates specialized methods designed for sparse data, such as AUCell and singscore.\n\n\n3. Visualization and Interpretation\nThe workflow provides structured output and visualization tools to interpret the results, including: * Robustness Metrics: Quantification of how consistently a signature identifies specific sample groups across different platforms. * Feature Visualization: Maps of signature scores onto spatial transcriptomics images to visualize the physical location of the cell states predicted by the signature.",
    "crumbs": [
      "expression",
      "Papers",
      "Exploring public cancer gene expression signatures across bulk, single-cell and spatial transcriptomics data with signifinder Bioconductor package"
    ]
  },
  {
    "objectID": "expression/pirrotta_2024_39363890.html#key-findings-application-to-breast-cancer-signatures",
    "href": "expression/pirrotta_2024_39363890.html#key-findings-application-to-breast-cancer-signatures",
    "title": "Exploring public cancer gene expression signatures across bulk, single-cell and spatial transcriptomics data with signifinder Bioconductor package",
    "section": "Key Findings: Application to Breast Cancer Signatures",
    "text": "Key Findings: Application to Breast Cancer Signatures\nThe authors applied signifinder to investigate the robustness of 106 published breast cancer GESs across three different data modalities:\n\n1. Robustness Across Modalities\n\nConsistent Signatures: Only a small subset of the tested signatures showed high robustness across all three data types (bulk, scRNA-Seq, ST).\nHigh-Resolution Data Value: The analysis revealed that some signatures that performed well in bulk data lost their predictive power in scRNA-Seq or ST, highlighting the need to re-validate bulk-derived signatures in high-resolution contexts.\nExample: Signatures based on Proliferation and Basal/Claudin-low subtypes were among the most robust, consistently classifying cell populations across technologies.\n\n\n\n2. Deconvoluting the Tumor Microenvironment (TME)\nUsing signifinder on spatial transcriptomics data (e.g., from Visium), the authors demonstrated the package’s ability to: * Locate Signatures: Accurately map the spatial enrichment of signatures associated with different cell populations (e.g., immune cells, fibroblasts) and cancer features (e.g., proliferation) within the breast tumor tissue. * TME Characterization: This capability allows researchers to refine the understanding of how the tumor microenvironment influences gene expression patterns, which is often masked in bulk sequencing.",
    "crumbs": [
      "expression",
      "Papers",
      "Exploring public cancer gene expression signatures across bulk, single-cell and spatial transcriptomics data with signifinder Bioconductor package"
    ]
  },
  {
    "objectID": "expression/pirrotta_2024_39363890.html#conclusions-and-recommendations",
    "href": "expression/pirrotta_2024_39363890.html#conclusions-and-recommendations",
    "title": "Exploring public cancer gene expression signatures across bulk, single-cell and spatial transcriptomics data with signifinder Bioconductor package",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe signifinder Bioconductor package addresses a critical need in computational cancer biology by providing a standardized, reproducible, and flexible tool for assessing the performance of gene expression signatures across emerging high-resolution transcriptomics platforms. The authors recommend that researchers use such methods to validate the relevance and consistency of their signatures when moving between bulk and single-cell/spatial data, ensuring reliable translation of findings.",
    "crumbs": [
      "expression",
      "Papers",
      "Exploring public cancer gene expression signatures across bulk, single-cell and spatial transcriptomics data with signifinder Bioconductor package"
    ]
  },
  {
    "objectID": "genetics/monsees_2009_19365863.html",
    "href": "genetics/monsees_2009_19365863.html",
    "title": "Genome-wide association scans for secondary traits using case-control samples",
    "section": "",
    "text": "PubMed: 19365863\nDOI: 10.1002/gepi.20424\nOverview generated by: Gemini 2.5 Flash, 26/11/2025",
    "crumbs": [
      "genetics",
      "Papers",
      "Genome-wide association scans for secondary traits using case-control samples"
    ]
  },
  {
    "objectID": "genetics/monsees_2009_19365863.html#key-findings",
    "href": "genetics/monsees_2009_19365863.html#key-findings",
    "title": "Genome-wide association scans for secondary traits using case-control samples",
    "section": "Key Findings",
    "text": "Key Findings\nThis paper addresses a critical methodological issue in genetic epidemiology where researchers attempt to maximize the return on investment from an expensive case-control Genome-Wide Association Study (GWAS) by analyzing additional, or secondary, quantitative traits (e.g., body mass index, mammographic density) collected on the same subjects. The core finding is that a naïve analysis (ignoring the case-control ascertainment) can lead to biased estimates of the association between a genetic marker and the secondary trait, particularly when both the marker and the secondary trait are independently associated with the primary disease risk. The authors demonstrate that the use of Inverse-Probability-of-Sampling-Weighted (IPW) regression provides unbiased estimates of the marker-secondary trait association in all scenarios, although it may suffer from reduced statistical power compared to the biased naïve methods.",
    "crumbs": [
      "genetics",
      "Papers",
      "Genome-wide association scans for secondary traits using case-control samples"
    ]
  },
  {
    "objectID": "genetics/monsees_2009_19365863.html#statistical-problem-ascertainment-bias",
    "href": "genetics/monsees_2009_19365863.html#statistical-problem-ascertainment-bias",
    "title": "Genome-wide association scans for secondary traits using case-control samples",
    "section": "Statistical Problem: Ascertainment Bias",
    "text": "Statistical Problem: Ascertainment Bias\nCase-control studies are designed to test the association between a genetic marker and a primary binary disease (e.g., breast cancer). When using these same samples to study a quantitative trait (the secondary trait, e.g., mammographic density), the selection process (ascertainment) based on the disease status introduces a bias.\n\nNaïve Analysis Scenarios\nThe study mathematically and via simulation tested the performance of two “naïve” approaches for testing the association between a marker (\\(G\\)) and a secondary trait (\\(T\\)) using case-control data, where \\(D\\) is the primary disease status:\n\nIgnoring \\(D\\): Regressing \\(T\\) on \\(G\\) in the combined sample, ignoring case-control status.\nStratifying on \\(D\\): Regressing \\(T\\) on \\(G\\) separately within cases and controls, and then combining the results (e.g., via meta-analysis).\n\nThe paper shows that both naïve methods have:\n\nProper Type I Error Rates (Unbiased Test): When testing the null hypothesis of no \\(G-T\\) association, the methods maintain the correct Type I error rate unless both \\(G\\) and \\(T\\) are independently associated with the primary disease \\(D\\).\nUnbiased Estimates (Under Alternative): Under the alternative hypothesis (i.e., a true \\(G-T\\) association exists), the estimated effect size is unbiased only if the secondary trait \\(T\\) is not associated with the primary disease \\(D\\).\n\n\n\nSource of Bias\nThe bias in the naïve methods occurs when a significant confounding pathway exists: \\(G \\rightarrow D \\leftarrow T\\). Since the case-control study non-randomly samples based on \\(D\\), this selection distorts the observed association between \\(G\\) and \\(T\\).",
    "crumbs": [
      "genetics",
      "Papers",
      "Genome-wide association scans for secondary traits using case-control samples"
    ]
  },
  {
    "objectID": "genetics/monsees_2009_19365863.html#solution-inverse-probability-of-sampling-weighting-ipw",
    "href": "genetics/monsees_2009_19365863.html#solution-inverse-probability-of-sampling-weighting-ipw",
    "title": "Genome-wide association scans for secondary traits using case-control samples",
    "section": "Solution: Inverse-Probability-of-Sampling Weighting (IPW)",
    "text": "Solution: Inverse-Probability-of-Sampling Weighting (IPW)\nThe authors propose using IPW regression to correct for the ascertainment bias. IPW uses weights in the regression calculation that are inversely proportional to the probability of an individual being sampled into the study.\n\nThe weights are calculated based on the sampling fractions of cases and controls.\nPerformance: IPW regression yielded unbiased estimates of the \\(G-T\\) association and maintained the proper Type I error rate in all scenarios considered, regardless of the association between \\(G\\), \\(T\\), and \\(D\\).\nTrade-off: IPW regression consistently demonstrated lower statistical power than the naïve analyses in situations where the naïve analyses were also unbiased.",
    "crumbs": [
      "genetics",
      "Papers",
      "Genome-wide association scans for secondary traits using case-control samples"
    ]
  },
  {
    "objectID": "genetics/monsees_2009_19365863.html#practical-recommendations",
    "href": "genetics/monsees_2009_19365863.html#practical-recommendations",
    "title": "Genome-wide association scans for secondary traits using case-control samples",
    "section": "Practical Recommendations",
    "text": "Practical Recommendations\nThe study concludes with practical recommendations for GWAS analysis of secondary traits:\n\nGeneral Markers: For the vast majority of markers tested in a GWAS (which are not associated with the primary disease \\(D\\)), the naïve analyses are valid tests of association and provide nearly unbiased estimates of the \\(G-T\\) association.\nDisease-Associated Markers: Care must be taken when both the marker (\\(G\\)) and the secondary trait (\\(T\\)) are associated with the primary disease (\\(D\\)). In this scenario, the naïve estimates will be biased, and IPW regression is the statistically valid method to obtain an unbiased estimate of the \\(G-T\\) effect.\nIllustration: The authors illustrate the potential for bias using an analysis of the relationship between a marker in the FGFR2 gene (a known breast cancer risk locus) and mammographic density in a breast cancer case-control sample.",
    "crumbs": [
      "genetics",
      "Papers",
      "Genome-wide association scans for secondary traits using case-control samples"
    ]
  },
  {
    "objectID": "genetics/benson_2023_37582364.html",
    "href": "genetics/benson_2023_37582364.html",
    "title": "Protein-metabolite association studies identify novel proteomic determinants of metabolite levels in human plasma",
    "section": "",
    "text": "PubMed: 37582364 DOI: 10.1016/j.cmet.2023.07.012 Overview generated by: Gemini 2.5 Flash, 27/11/2025",
    "crumbs": [
      "genetics",
      "Papers",
      "Protein-metabolite association studies identify novel proteomic determinants of metabolite levels in human plasma"
    ]
  },
  {
    "objectID": "genetics/benson_2023_37582364.html#background-and-objective",
    "href": "genetics/benson_2023_37582364.html#background-and-objective",
    "title": "Protein-metabolite association studies identify novel proteomic determinants of metabolite levels in human plasma",
    "section": "Background and Objective",
    "text": "Background and Objective\nCirculating levels of proteins and metabolites in human plasma reflect the physiological state of an individual and are strongly associated with the risk of various complex diseases, particularly cardio-metabolic disorders. While many associations have been identified, distinguishing between causal relationships (where a protein concentration change directly causes a metabolite level change) and confounded associations (where both are affected by a third factor) remains a major challenge.\nThe primary objective of this study was to integrate proteomic, metabolomic, and genomic data using Mendelian Randomization (MR) to systematically identify putative causal relationships between circulating proteins and metabolites in human plasma.",
    "crumbs": [
      "genetics",
      "Papers",
      "Protein-metabolite association studies identify novel proteomic determinants of metabolite levels in human plasma"
    ]
  },
  {
    "objectID": "genetics/benson_2023_37582364.html#study-methods-and-data-integration",
    "href": "genetics/benson_2023_37582364.html#study-methods-and-data-integration",
    "title": "Protein-metabolite association studies identify novel proteomic determinants of metabolite levels in human plasma",
    "section": "Study Methods and Data Integration",
    "text": "Study Methods and Data Integration\nThe study employed a large-scale, multi-stage, multi-omics approach:\n\nData Cohorts: Quantitative proteomic (1,302 proteins) and metabolomic (365 metabolites) data were meta-analyzed across three large population studies (Jackson Heart Study, Multi-Ethnic Study of Atherosclerosis, and Health, Risk Factors, Exercise Training and Genetics), totaling 3,626 individuals.\nPairwise Association Analysis: The study first identified 172,000 significant pairwise correlations between proteins and metabolites across the three cohorts.\nCausal Inference via Mendelian Randomization (MR): To overcome confounding, two-sample MR was applied using genetic instruments derived from protein-quantitative trait loci (pQTLs), specifically those located near the coding region of 535 proteins. These genetic variants served as instrumental variables to assess the causal effect of protein levels (exposure) on metabolite levels (outcome).\nMeta-Analysis and Validation: Causal estimates were meta-analyzed across the three studies. Sensitivity analyses (MR-Egger, weighted median) were performed to check for robustness against pleiotropy.\nIn Vivo Validation: To provide biological proof-of-concept, the top-ranking protein-to-metabolite causal associations were validated in vivo using metabolomic profiling of mouse knockout strains for the corresponding genes.",
    "crumbs": [
      "genetics",
      "Papers",
      "Protein-metabolite association studies identify novel proteomic determinants of metabolite levels in human plasma"
    ]
  },
  {
    "objectID": "genetics/benson_2023_37582364.html#key-results-and-findings",
    "href": "genetics/benson_2023_37582364.html#key-results-and-findings",
    "title": "Protein-metabolite association studies identify novel proteomic determinants of metabolite levels in human plasma",
    "section": "Key Results and Findings",
    "text": "Key Results and Findings\n\nCausal Protein-Metabolite Associations\n\nThe MR analysis identified 224 putative causal associations between 95 proteins and 96 metabolites.\nNovel Findings: Many of these causal links were novel, offering new insights into metabolic regulation. For instance, the study confirmed the causal role of Apolipoprotein C-III (ApoC3) in increasing triglycerides but also identified novel links, such as the causal role of protein ADAMTSL3 in regulating several plasma metabolites, particularly branched-chain amino acid (BCAA) metabolites.\n\n\n\nValidation of Causal Links\n\nIn Vivo Confirmation: Over 50% of the tested protein-to-metabolite causal associations were successfully validated in the mouse knockout models, providing strong experimental support for the causal nature of the in-silico findings. For example, knocking out the gene encoding the protein CSHL1 resulted in predicted changes in specific metabolites, validating the MR findings.\n\n\n\nNetwork and Pathway Insights\n\nThe study confirmed known metabolic hubs but also identified novel networks where proteins regulate metabolites. Many of the causal links highlighted pathways relevant to lipid metabolism and amino acid catabolism, known drivers of cardio-metabolic risk.",
    "crumbs": [
      "genetics",
      "Papers",
      "Protein-metabolite association studies identify novel proteomic determinants of metabolite levels in human plasma"
    ]
  },
  {
    "objectID": "genetics/benson_2023_37582364.html#conclusions-and-significance",
    "href": "genetics/benson_2023_37582364.html#conclusions-and-significance",
    "title": "Protein-metabolite association studies identify novel proteomic determinants of metabolite levels in human plasma",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nThis research successfully established a robust multi-omics-to-causality pipeline by integrating large-scale human proteomic, metabolomic, and genomic data. By using Mendelian Randomization and subsequent in vivo validation, the study identified 224 high-confidence protein-to-metabolite causal associations.\nThese findings significantly advance the understanding of the molecular determinants of metabolic traits, providing a valuable resource for identifying novel therapeutic targets for cardiovascular and metabolic diseases.",
    "crumbs": [
      "genetics",
      "Papers",
      "Protein-metabolite association studies identify novel proteomic determinants of metabolite levels in human plasma"
    ]
  },
  {
    "objectID": "genetics/aschard_2015_25640676.html",
    "href": "genetics/aschard_2015_25640676.html",
    "title": "Adjusting for Heritable Covariates Can Bias Effect Estimates in Genome-Wide Association Studies",
    "section": "",
    "text": "PubMed: 25640676\nDOI: 10.1016/j.ajhg.2014.12.021\nOverview generated by: Gemini 2.5 Flash, 26/11/2025",
    "crumbs": [
      "genetics",
      "Papers",
      "Adjusting for Heritable Covariates Can Bias Effect Estimates in Genome-Wide Association Studies"
    ]
  },
  {
    "objectID": "genetics/aschard_2015_25640676.html#key-findings",
    "href": "genetics/aschard_2015_25640676.html#key-findings",
    "title": "Adjusting for Heritable Covariates Can Bias Effect Estimates in Genome-Wide Association Studies",
    "section": "Key Findings",
    "text": "Key Findings\nThis seminal methodological report examines the statistical consequences of adjusting Genome-Wide Association Studies (GWAS) for heritable covariates (correlated traits that are themselves genetically influenced), conclusively demonstrating that this common practice introduces a significant collider bias.\n\nMain Discoveries\n\nCollider Bias Introduction: Adjusting a standard GWAS regression for a heritable covariate (\\(C\\)) that is correlated with the outcome (\\(Y\\)) and influenced by the SNP (\\(G\\)) introduces bias (also known as index event bias or selection bias). This happens because conditioning on the covariate (the collider) opens a spurious path between the SNP and unobserved confounders, which can lead to false positive associations with the primary outcome.\nUnbiased Estimation Condition: The resulting adjusted effect, \\(\\beta_{G \\rightarrow Y \\mid C}\\), accurately estimates the direct genetic effect on the outcome only under two strict causal models:\n\nThe SNP has no effect on the covariate (\\(\\beta_{G \\rightarrow C} = 0\\)).\nThe covariate (\\(C\\)) is a pure mediator, where the correlation between \\(C\\) and \\(Y\\) is entirely explained by a direct causal effect of \\(C\\) on \\(Y\\).\n\nBias Formula: For scenarios involving shared genetic or environmental risk factors, the bias (\\(\\text{Bias}\\)) in the adjusted genetic effect estimate is well-approximated by the equation: \\[\\text{Bias} \\approx -\\beta_{G \\rightarrow C} \\cdot \\rho_{C Y} \\cdot \\sqrt{\\frac{\\text{Var}(C)}{\\text{Var}(Y)}}\\] Where \\(\\beta_{G \\rightarrow C}\\) is the genetic effect on the covariate, and \\(\\rho_{C Y}\\) is the phenotypic correlation between the covariate and the outcome. The magnitude and direction of the bias are dependent on these terms.",
    "crumbs": [
      "genetics",
      "Papers",
      "Adjusting for Heritable Covariates Can Bias Effect Estimates in Genome-Wide Association Studies"
    ]
  },
  {
    "objectID": "genetics/aschard_2015_25640676.html#study-design",
    "href": "genetics/aschard_2015_25640676.html#study-design",
    "title": "Adjusting for Heritable Covariates Can Bias Effect Estimates in Genome-Wide Association Studies",
    "section": "Study Design",
    "text": "Study Design\nThe study employed a rigorous statistical approach using established causal inference frameworks, detailed theoretical modeling, and numerical simulations.\n\nTheoretical Framework\n\nModeling of the True Direct Effect: The study defined the desired quantity, the direct genetic effect \\(\\beta_{G \\rightarrow Y}\\), as the effect of the SNP (\\(G\\)) on the outcome (\\(Y\\)) independent of the covariate (\\(C\\)). This is achieved by adjusting for all causal factors of the covariate.\nLinear Regression Model: The estimation was performed using a standard linear regression: \\(Y = \\alpha + \\beta_{G \\rightarrow Y \\mid C} G + \\gamma C + \\epsilon\\). The authors derived the expected value of the adjusted coefficient, \\(\\mathbb{E}[\\beta_{G \\rightarrow Y \\mid C}]\\), showing that it equals the true direct effect plus the bias term under various causal scenarios.\nBias Derivation: The bias was derived by considering the correlation structure induced when the covariate and the outcome share unobserved causes, specifically showing how adjustment for \\(C\\) introduces conditioning on a collider related to the SNP’s effect. The full mathematical expression for the bias was derived, from which the simplified approximation was obtained.\n\n\n\nSimulation Methods\n\nScenarios Tested: Simulations covered all three major causal scenarios: C is a mediator of the effect of \\(G\\) on \\(Y\\); Y is a mediator of the effect of \\(G\\) on \\(C\\); and \\(G\\) is a shared cause (pleiotropy) where the effect of \\(G\\) on \\(Y\\) is mediated by \\(C\\), or the traits share a hidden common environmental cause (\\(U\\)).\nParameters: Simulations varied key parameters, including heritability of the traits (up to \\(h^2 = 0.5\\)), the phenotypic correlation (\\(\\rho_{C Y}\\), up to \\(0.5\\)), and the genetic effect sizes (\\(\\beta_G\\)).\nEvaluation Metrics: The primary metrics used to evaluate the consequences of the bias were the Type I error rate (false positive rate) and the statistical power of the adjusted association test. Results showed Type I error inflation when the adjusted model was used in biased scenarios.\n\n\n\nReal-World Data Application\n\nData Source: GWAS summary statistics from the GIANT consortium meta-analysis of anthropometric traits were used, specifically:\n\nGWAS of Waist-to-Hip Ratio (WHR) adjusted for BMI (\\(Y \\mid C\\)).\nGWAS of BMI (\\(C\\)).\n\nEmpirical Test: A test was performed to look for an enrichment of SNPs with marginal effects in opposite directions on WHR and BMI. A highly significant enrichment (\\(p=0.005\\)) was found, providing empirical evidence that the statistical bias was present and inflating the number of significant loci.",
    "crumbs": [
      "genetics",
      "Papers",
      "Adjusting for Heritable Covariates Can Bias Effect Estimates in Genome-Wide Association Studies"
    ]
  },
  {
    "objectID": "genetics/aschard_2015_25640676.html#major-results",
    "href": "genetics/aschard_2015_25640676.html#major-results",
    "title": "Adjusting for Heritable Covariates Can Bias Effect Estimates in Genome-Wide Association Studies",
    "section": "Major Results",
    "text": "Major Results\n\nPower Paradox: Adjustment for a heritable covariate results in increased statistical power when the signs of \\(\\beta_{G \\rightarrow Y \\mid C}\\) and the bias term are in opposite directions. This effect explains the increased detection of specific loci in the WHR adjusted for BMI GWAS.\nInterpretation Challenge: Since the adjusted estimates reflect a combination of the true direct effect and the bias term, they are neither the total genetic effect nor the direct genetic effect in most scenarios involving shared genetic or environmental risk factors.\nRelevance to Ratio Traits: The findings are directly relevant to analyses of ratio traits (e.g., WHR, fasting glucose/insulin ratios), as these are mathematically equivalent to performing a regression adjusted for the denominator.",
    "crumbs": [
      "genetics",
      "Papers",
      "Adjusting for Heritable Covariates Can Bias Effect Estimates in Genome-Wide Association Studies"
    ]
  },
  {
    "objectID": "genetics/aschard_2015_25640676.html#practical-implications",
    "href": "genetics/aschard_2015_25640676.html#practical-implications",
    "title": "Adjusting for Heritable Covariates Can Bias Effect Estimates in Genome-Wide Association Studies",
    "section": "Practical Implications",
    "text": "Practical Implications\n\nRecommendations for Future GWAS\n\nPrioritize Unadjusted Analysis: For general genetic discovery and estimation of the total genetic effect (which includes effects mediated through other traits), the unadjusted GWAS of the primary outcome is the statistically unbiased standard.\nUse Bivariate Methods: To gain statistical power and accurately account for correlated traits without introducing collider bias, researchers should prefer multivariate or bivariate methods (e.g., those simultaneously modeling both \\(C\\) and \\(Y\\)) over simple regression adjustment.\nCausal Inference: If the research goal is specifically to estimate the direct causal effect, more advanced methods like Multivariable Mendelian Randomization (MVMR) should be considered to isolate the effect while mitigating the bias.",
    "crumbs": [
      "genetics",
      "Papers",
      "Adjusting for Heritable Covariates Can Bias Effect Estimates in Genome-Wide Association Studies"
    ]
  },
  {
    "objectID": "genetics/aschard_2015_25640676.html#related-concepts",
    "href": "genetics/aschard_2015_25640676.html#related-concepts",
    "title": "Adjusting for Heritable Covariates Can Bias Effect Estimates in Genome-Wide Association Studies",
    "section": "Related Concepts",
    "text": "Related Concepts\n\nCollider Bias: A form of selection bias where conditioning on a variable that is a common effect (a collider) of two other variables induces a spurious association between the two causes.\nDirect vs. Total Genetic Effect: The direct effect is the association independent of the covariate; the total effect includes the portion mediated through the covariate.\nWaist-to-Hip Ratio (WHR) adjusted for BMI: The primary empirical example, where the adjustment for the highly heritable BMI creates a biased estimate for the remaining variance of WHR.",
    "crumbs": [
      "genetics",
      "Papers",
      "Adjusting for Heritable Covariates Can Bias Effect Estimates in Genome-Wide Association Studies"
    ]
  },
  {
    "objectID": "genetics/fu_2025_41366086.html",
    "href": "genetics/fu_2025_41366086.html",
    "title": "A biobank-scale test of marginal epistasis reveals genome-wide signals of polygenic interaction effects",
    "section": "",
    "text": "PubMed: 41366086 DOI: 10.1038/s41588-025-02411-y Overview generated by: Gemini 2.5 Flash, 10/12/2025",
    "crumbs": [
      "genetics",
      "Papers",
      "A biobank-scale test of marginal epistasis reveals genome-wide signals of polygenic interaction effects"
    ]
  },
  {
    "objectID": "genetics/fu_2025_41366086.html#research-goal-and-methodology",
    "href": "genetics/fu_2025_41366086.html#research-goal-and-methodology",
    "title": "A biobank-scale test of marginal epistasis reveals genome-wide signals of polygenic interaction effects",
    "section": "Research Goal and Methodology",
    "text": "Research Goal and Methodology\nThis study addresses the challenge of identifying genetic interactions (epistasis) in complex human traits, which is often hindered by computational complexity and statistical power limitations in biobank-scale datasets. The authors introduce FAME (FAst Marginal Epistasis test), a novel, computationally efficient method designed to detect marginal epistasis for a single-nucleotide polymorphism (SNP) on a quantitative trait.\n\nWhat is Marginal Epistasis?\nMarginal epistasis, in this context, refers to whether the effect of a given SNP on a trait is significantly modulated by the individual’s overall genetic background. It tests for the aggregate effect of interaction between a focal SNP and all other polygenic effects, rather than requiring the testing of millions of pairwise SNP interactions.\n\n\nFAME Algorithm\nThe FAME method is designed as an extension of standard linear mixed models (LMMs) used in GWAS. It estimates the interaction effect of a focal SNP with the polygenic background by:\n\nEstimating Polygenic Background: The effect of the entire genetic background is estimated using a standard LMM and captured by the individual-level polygenic scores (or ‘residual additive effect’).\nTesting Interaction: FAME then tests the interaction between the focal SNP’s genotype and this estimated polygenic background effect.\n\nThis formulation allows FAME to be highly efficient, enabling genome-wide testing of marginal epistasis, which was previously intractable for large biobanks.",
    "crumbs": [
      "genetics",
      "Papers",
      "A biobank-scale test of marginal epistasis reveals genome-wide signals of polygenic interaction effects"
    ]
  },
  {
    "objectID": "genetics/fu_2025_41366086.html#key-findings-genome-wide-marginal-epistasis-signals",
    "href": "genetics/fu_2025_41366086.html#key-findings-genome-wide-marginal-epistasis-signals",
    "title": "A biobank-scale test of marginal epistasis reveals genome-wide signals of polygenic interaction effects",
    "section": "Key Findings: Genome-Wide Marginal Epistasis Signals",
    "text": "Key Findings: Genome-Wide Marginal Epistasis Signals\nThe authors applied FAME to GWAS-significant trait-SNP associations across 53 quantitative traits in approximately 300,000 unrelated White British individuals from the UK Biobank (UKBB).\n\nSignificant Signals: FAME identified 16 significant marginal epistasis signals across 12 unique traits, with the most robust signals found for anthropometric traits (e.g., height, standing height) and blood cell phenotypes.\nTrait-Specific Patterns:\n\nHeight: Two of the most significant epistasis signals were found for height, suggesting that the effect of specific height-associated SNPs is non-linearly dependent on the individual’s overall polygenic predisposition for height.\nBlood Cell Traits: Strong signals were also detected for mean corpuscular volume and monocyte count, indicating that complex cell biology is also influenced by polygenic interaction effects.\n\nNovel Findings: While previous studies have hinted at epistasis, the 16 signals identified by FAME are novel findings at a genome-wide scale, demonstrating the power of marginal epistasis testing to capture aggregate interaction effects.",
    "crumbs": [
      "genetics",
      "Papers",
      "A biobank-scale test of marginal epistasis reveals genome-wide signals of polygenic interaction effects"
    ]
  },
  {
    "objectID": "genetics/fu_2025_41366086.html#implications-for-heritability-and-genetic-architecture",
    "href": "genetics/fu_2025_41366086.html#implications-for-heritability-and-genetic-architecture",
    "title": "A biobank-scale test of marginal epistasis reveals genome-wide signals of polygenic interaction effects",
    "section": "Implications for Heritability and Genetic Architecture",
    "text": "Implications for Heritability and Genetic Architecture\nThe study provides evidence for the widespread presence of polygenic interaction effects, which has significant implications for understanding the total genetic architecture of complex traits:\n\nContribution to Variation: The authors estimated the proportion of phenotypic variance explained by these marginal epistasis effects, finding that, while individually small (typically \\(&lt;0.05\\%\\)), the cumulative contribution across the genome may be substantial, potentially helping to explain some of the “missing heritability” gap.\nAdditive Model Limitations: The presence of these polygenic interaction effects suggests that the common assumption in GWAS—that genetic effects are purely additive—is an oversimplification. FAME provides a scalable way to account for the systematic non-additive contributions to trait variation.",
    "crumbs": [
      "genetics",
      "Papers",
      "A biobank-scale test of marginal epistasis reveals genome-wide signals of polygenic interaction effects"
    ]
  },
  {
    "objectID": "genetics/fu_2025_41366086.html#conclusions",
    "href": "genetics/fu_2025_41366086.html#conclusions",
    "title": "A biobank-scale test of marginal epistasis reveals genome-wide signals of polygenic interaction effects",
    "section": "Conclusions",
    "text": "Conclusions\nFAME is a powerful and scalable statistical method that has successfully identified genome-wide signals of marginal epistasis in a biobank-scale cohort. The identification of 16 robust marginal epistasis signals across diverse traits underscores that genetic interactions with the polygenic background are a fundamental component of the genetic architecture of human complex traits. This work opens the door for more comprehensive modeling of non-additive effects in future large-scale genetic studies.",
    "crumbs": [
      "genetics",
      "Papers",
      "A biobank-scale test of marginal epistasis reveals genome-wide signals of polygenic interaction effects"
    ]
  },
  {
    "objectID": "genetics/tokolyi_2025_40038547.html",
    "href": "genetics/tokolyi_2025_40038547.html",
    "title": "The contribution of genetic determinants of blood gene expression and splicing to molecular phenotypes and health outcomes",
    "section": "",
    "text": "PubMed: 40038547 DOI: 10.1038/s41588-025-02096-3 Overview generated by: Gemini 2.5 Flash, 27/11/2025",
    "crumbs": [
      "genetics",
      "Papers",
      "The contribution of genetic determinants of blood gene expression and splicing to molecular phenotypes and health outcomes"
    ]
  },
  {
    "objectID": "genetics/tokolyi_2025_40038547.html#background-and-objective",
    "href": "genetics/tokolyi_2025_40038547.html#background-and-objective",
    "title": "The contribution of genetic determinants of blood gene expression and splicing to molecular phenotypes and health outcomes",
    "section": "Background and Objective",
    "text": "Background and Objective\nThe biological mechanisms through which the majority of nonprotein-coding genetic variants influence disease risk remain largely unknown. These variants often function by regulating gene activity. The objective of this study was to comprehensively investigate these gene-regulatory mechanisms by mapping genetic determinants of gene expression and splicing in blood and integrating them with various molecular and health outcomes.",
    "crumbs": [
      "genetics",
      "Papers",
      "The contribution of genetic determinants of blood gene expression and splicing to molecular phenotypes and health outcomes"
    ]
  },
  {
    "objectID": "genetics/tokolyi_2025_40038547.html#methods-multi-omics-qtl-mapping-and-integration",
    "href": "genetics/tokolyi_2025_40038547.html#methods-multi-omics-qtl-mapping-and-integration",
    "title": "The contribution of genetic determinants of blood gene expression and splicing to molecular phenotypes and health outcomes",
    "section": "Methods: Multi-Omics QTL Mapping and Integration",
    "text": "Methods: Multi-Omics QTL Mapping and Integration\nThe researchers conducted a comprehensive multi-omics study using data from 4,732 participants to identify quantitative trait loci (QTLs):\n\nQTL Mapping: They mapped gene expression QTLs (eQTLs) and splicing QTLs (sQTLs) in blood using bulk RNA sequencing, identifying cis-QTLs for 17,233 genes and 29,514 splicing events.\nMulti-omics Integration: They integrated the identified genetic associations with data on protein, metabolite, and lipid levels from the same individuals.\nCausal Inference: They employed colocalization analyses to pinpoint instances where the same causal genetic variant affects both a transcriptional/splicing phenotype and a molecular/health outcome.\nMediation Analysis: They quantified the relative contribution of genetic effects at loci with shared etiology to determine which molecular phenotypes are significantly mediated by gene expression or splicing.",
    "crumbs": [
      "genetics",
      "Papers",
      "The contribution of genetic determinants of blood gene expression and splicing to molecular phenotypes and health outcomes"
    ]
  },
  {
    "objectID": "genetics/tokolyi_2025_40038547.html#key-results-and-significance",
    "href": "genetics/tokolyi_2025_40038547.html#key-results-and-significance",
    "title": "The contribution of genetic determinants of blood gene expression and splicing to molecular phenotypes and health outcomes",
    "section": "Key Results and Significance",
    "text": "Key Results and Significance\nThe study successfully bridged the gap between genetic variation and downstream molecular and clinical phenotypes:\n\nExtensive Shared Genetic Etiology: Colocalization analyses revealed a shared genetic association signal with gene expression or splicing for 3,430 proteomic and metabolomic traits.\nMediated Molecular Phenotypes: They found that 222 molecular phenotypes were significantly mediated by gene expression or splicing, providing strong evidence for gene-regulatory causality.\nMechanistic Insights: The approach uncovered specific gene-regulatory mechanisms at disease loci with potential therapeutic relevance, such as \\(WARS1\\) in hypertension, \\(IL7R\\) in dermatitis, and \\(IFNAR2\\) in COVID-19$.\nPublic Resource: The findings provide a comprehensive, open-access resource on the shared genetic etiology across transcriptional phenotypes, molecular traits, and health outcomes (https://IntervalRNA.org.uk).",
    "crumbs": [
      "genetics",
      "Papers",
      "The contribution of genetic determinants of blood gene expression and splicing to molecular phenotypes and health outcomes"
    ]
  },
  {
    "objectID": "genetics/wu_2023_37601976.html",
    "href": "genetics/wu_2023_37601976.html",
    "title": "Joint analysis of GWAS and multi-omics QTL summary statistics reveals a large fraction of GWAS signals shared with molecular phenotypes",
    "section": "",
    "text": "PubMed: 37601976 DOI: 10.1016/j.xgen.2023.100344 Overview generated by: Gemini 2.5 Flash, 27/11/2025",
    "crumbs": [
      "genetics",
      "Papers",
      "Joint analysis of GWAS and multi-omics QTL summary statistics reveals a large fraction of GWAS signals shared with molecular phenotypes"
    ]
  },
  {
    "objectID": "genetics/wu_2023_37601976.html#background-and-objective",
    "href": "genetics/wu_2023_37601976.html#background-and-objective",
    "title": "Joint analysis of GWAS and multi-omics QTL summary statistics reveals a large fraction of GWAS signals shared with molecular phenotypes",
    "section": "Background and Objective",
    "text": "Background and Objective\nGenome-wide association studies (GWAS) have successfully identified thousands of genetic loci associated with complex human traits and diseases. However, the precise molecular mechanisms by which these non-coding genetic variants exert their effects—often by regulating gene expression or other molecular phenotypes—remain largely unknown. Quantitative Trait Loci (QTL) studies provide molecular data (e.g., gene expression, methylation) but are often limited by sample size.\nThe objective of this study was to develop a new method, OPERA (Overlap-Based Partitioned Estimation and Regression Analysis), to integrate summary statistics from GWAS and various multi-omics QTL (xQTL) studies to: 1. Quantify the proportion of GWAS signals that are shared with, and likely mediated by, specific molecular phenotypes. 2. Improve the power for fine-mapping and gene discovery for complex traits.",
    "crumbs": [
      "genetics",
      "Papers",
      "Joint analysis of GWAS and multi-omics QTL summary statistics reveals a large fraction of GWAS signals shared with molecular phenotypes"
    ]
  },
  {
    "objectID": "genetics/wu_2023_37601976.html#methods-the-opera-framework",
    "href": "genetics/wu_2023_37601976.html#methods-the-opera-framework",
    "title": "Joint analysis of GWAS and multi-omics QTL summary statistics reveals a large fraction of GWAS signals shared with molecular phenotypes",
    "section": "Methods: The OPERA Framework",
    "text": "Methods: The OPERA Framework\n\nData Integration\nOPERA is a computational method that jointly analyzes GWAS summary statistics and multi-omics xQTL summary statistics (including eQTLs for gene expression, pQTLs for protein levels, sQTLs for splicing, etc.).\n\n\nKey Innovation: Shared Genetic Variance\nThe core of OPERA is its ability to partition the heritability of a complex trait into components explained by genetic variants that are shared with different molecular phenotypes (i.e., those that are QTLs for specific molecular traits). This partitioning allows the study to estimate the proportion of GWAS signals (or heritability) that is mediated through each molecular phenotype. OPERA is robust to issues like linkage disequilibrium (LD) and confounding.\n\n\nApplication\nThe method was applied to 11 complex human traits from GWAS, integrated with xQTL data across 13 different molecular phenotypes (e.g., gene expression, DNA methylation, histone modifications) from various human tissues and cell types.",
    "crumbs": [
      "genetics",
      "Papers",
      "Joint analysis of GWAS and multi-omics QTL summary statistics reveals a large fraction of GWAS signals shared with molecular phenotypes"
    ]
  },
  {
    "objectID": "genetics/wu_2023_37601976.html#key-findings",
    "href": "genetics/wu_2023_37601976.html#key-findings",
    "title": "Joint analysis of GWAS and multi-omics QTL summary statistics reveals a large fraction of GWAS signals shared with molecular phenotypes",
    "section": "Key Findings",
    "text": "Key Findings\n\nLarge Fraction of Shared Signals\nThe primary and most significant finding was that, on average, approximately 50% of the genetic signals identified in GWAS are shared with at least one molecular phenotype (xQTL). This provides strong statistical evidence that a substantial portion of complex trait heritability is mediated by genetic regulatory effects on molecular traits.\n\neQTLs are Major Mediators: Among the molecular phenotypes studied, expression QTLs (eQTLs), which regulate gene expression, were the most significant molecular mediators, accounting for the largest shared fraction of GWAS signals.\n\n\n\nEnhanced Discovery Power\nBy jointly analyzing the data, OPERA achieved: * Identification of novel genes/variants: The joint analysis led to the discovery of 89 novel genes for the 11 complex traits studied, primarily through more effective fine-mapping in previously identified GWAS loci. * Improved fine-mapping: The ability to integrate the xQTL data dramatically enhanced the precision of identifying the likely causal variant within a GWAS locus.",
    "crumbs": [
      "genetics",
      "Papers",
      "Joint analysis of GWAS and multi-omics QTL summary statistics reveals a large fraction of GWAS signals shared with molecular phenotypes"
    ]
  },
  {
    "objectID": "genetics/wu_2023_37601976.html#conclusions-and-significance",
    "href": "genetics/wu_2023_37601976.html#conclusions-and-significance",
    "title": "Joint analysis of GWAS and multi-omics QTL summary statistics reveals a large fraction of GWAS signals shared with molecular phenotypes",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nThe OPERA framework successfully demonstrated that a large fraction of the genetic basis of complex traits is shared with molecular phenotypes, confirming that these molecular traits (particularly gene expression) are critical intermediate steps between genetic variants and disease risk.\nThis integrated approach provides a powerful tool for converting GWAS signals from abstract associations into biologically actionable regulatory mechanisms, aiding in the discovery of novel therapeutic targets.",
    "crumbs": [
      "genetics",
      "Papers",
      "Joint analysis of GWAS and multi-omics QTL summary statistics reveals a large fraction of GWAS signals shared with molecular phenotypes"
    ]
  },
  {
    "objectID": "genetics/coral_2023_36703017.html",
    "href": "genetics/coral_2023_36703017.html",
    "title": "A phenome-wide comparative analysis of genetic discordance between obesity and type 2 diabetes",
    "section": "",
    "text": "PubMed: 36703017 DOI: 10.1038/s42255-022-00731-5 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "genetics",
      "Papers",
      "A phenome-wide comparative analysis of genetic discordance between obesity and type 2 diabetes"
    ]
  },
  {
    "objectID": "genetics/coral_2023_36703017.html#key-findings-two-genetic-obesity-profiles",
    "href": "genetics/coral_2023_36703017.html#key-findings-two-genetic-obesity-profiles",
    "title": "A phenome-wide comparative analysis of genetic discordance between obesity and type 2 diabetes",
    "section": "Key Findings: Two Genetic Obesity Profiles",
    "text": "Key Findings: Two Genetic Obesity Profiles\nThis study addresses the high degree of heterogeneity in the relationship between obesity and type 2 diabetes (T2D) by using a genetic-driven approach to define two distinct obesity profiles. These profiles convey highly concordant and discordant diabetogenic effects, indicating that the genetic mechanism of obesity strongly influences its specific consequences for metabolic health.",
    "crumbs": [
      "genetics",
      "Papers",
      "A phenome-wide comparative analysis of genetic discordance between obesity and type 2 diabetes"
    ]
  },
  {
    "objectID": "genetics/coral_2023_36703017.html#methods-and-study-design",
    "href": "genetics/coral_2023_36703017.html#methods-and-study-design",
    "title": "A phenome-wide comparative analysis of genetic discordance between obesity and type 2 diabetes",
    "section": "Methods and Study Design",
    "text": "Methods and Study Design\nThe researchers employed a genetic-driven approach to identify the two obesity profiles. They then conducted a phenome-wide comparative analysis to annotate and compare the genetic association signals for these profiles across various clinical and molecular phenotypic layers.\n\nData Sources: The analysis utilized individual-level data from large biobanks, specifically the UK Biobank and BioVU.\nEthics: The study adhered to the ethical principles of the Declaration of Helsinki, with both data sources having received the necessary institutional ethics approvals, and all participants providing written informed consent.",
    "crumbs": [
      "genetics",
      "Papers",
      "A phenome-wide comparative analysis of genetic discordance between obesity and type 2 diabetes"
    ]
  },
  {
    "objectID": "genetics/coral_2023_36703017.html#results-phenotypic-differences",
    "href": "genetics/coral_2023_36703017.html#results-phenotypic-differences",
    "title": "A phenome-wide comparative analysis of genetic discordance between obesity and type 2 diabetes",
    "section": "Results: Phenotypic Differences",
    "text": "Results: Phenotypic Differences\nThe comparative analysis identified key differences between the two obesity profiles across a wide spectrum of traits, illustrating the diverse pathways through which obesity can influence health.\n\nTrait Discordance\nSignificant differences were identified in various clinical and molecular traits, including: * Cardiovascular Mortality: The profiles convey different risks for cardiovascular outcomes. * Fat Distribution: Variations were observed in how fat is distributed in the body. * Metabolism: Key differences were found in liver metabolism, blood pressure, and specific lipid fractions. * Molecular Traits: Differences were noted in blood levels of proteins involved in extracellular matrix remodelling. * Microbiome: Marginal differences were found in the abundance of Bacteroidetes and Firmicutes bacteria in the gut.\n\n\nCausal Mechanisms\nInstrumental analyses indicated prominent causal roles for certain risk factors in driving the discordance between the profiles: * Waist-to-Hip Ratio (WHR): This measure of central adiposity plays a key causal role. * Blood Pressure: Elevated blood pressure is a prominent causal factor. * Cholesterol Content of High-Density Lipoprotein (HDL): The genetic influence on HDL cholesterol content is also a significant causal differentiator.",
    "crumbs": [
      "genetics",
      "Papers",
      "A phenome-wide comparative analysis of genetic discordance between obesity and type 2 diabetes"
    ]
  },
  {
    "objectID": "genetics/coral_2023_36703017.html#conclusions",
    "href": "genetics/coral_2023_36703017.html#conclusions",
    "title": "A phenome-wide comparative analysis of genetic discordance between obesity and type 2 diabetes",
    "section": "Conclusions",
    "text": "Conclusions\nThe research defines a genetic basis for the heterogeneity observed in the relationship between obesity and T2D. By identifying obesity profiles with distinct diabetogenic effects and differential impacts on cardiovascular and metabolic health, the study supports a more refined, mechanism-based approach to understanding and treating T2D risk. This paves the way for a precision medicine framework where genetic profiles could inform specific intervention strategies.",
    "crumbs": [
      "genetics",
      "Papers",
      "A phenome-wide comparative analysis of genetic discordance between obesity and type 2 diabetes"
    ]
  },
  {
    "objectID": "genetics/timpson_2017_29225335.html",
    "href": "genetics/timpson_2017_29225335.html",
    "title": "Genetic architecture: the shape of the genetic contribution to human traits and disease",
    "section": "",
    "text": "PubMed: 29225335\nDOI: 10.1038/nrg.2017.101\nOverview generated by: Gemini 2.5 Flash, 26/11/2025",
    "crumbs": [
      "genetics",
      "Papers",
      "Genetic architecture: the shape of the genetic contribution to human traits and disease"
    ]
  },
  {
    "objectID": "genetics/timpson_2017_29225335.html#key-findings",
    "href": "genetics/timpson_2017_29225335.html#key-findings",
    "title": "Genetic architecture: the shape of the genetic contribution to human traits and disease",
    "section": "Key Findings",
    "text": "Key Findings\nThis comprehensive review synthesizes the field’s understanding of genetic architecture—the characteristics of genetic variation responsible for heritable phenotypic variability—in the era following the advent of large-scale Genome-Wide Association Studies (GWAS) and Next-Generation Sequencing (NGS). The authors systematically define the components of genetic architecture and explain how recent technological advances have begun to reveal the complex interplay of factors contributing to human traits and diseases.\n\nCore Components of Genetic Architecture\nThe genetic architecture of any complex trait is defined by four interacting components, which the review explores in depth:\n\nNumber of Causal Variants: The sheer count of genetic variants (SNPs, indels, SVs) that collectively affect the trait. Most complex traits are highly polygenic, involving thousands of variants.\nEffect Size Distribution: The magnitude of the effect that each variant contributes to the phenotype. GWAS has revealed that many common variants have small, additive effects, while rare variants often have large effects.\nAllele Frequency Spectrum: The distribution of causal variant frequencies in the population. Common traits are often influenced by variants across the entire frequency spectrum, supporting the ‘common disease, common variant’ and ‘common disease, rare variant’ hypotheses in tandem.\nInteractions: The complexity added by non-additive relationships:\n\nAllelic Interactions: Dominance (interaction between alleles at the same locus).\nLocus Interactions: Epistasis (interaction between alleles at different loci).\nEnvironmental Interactions: Gene-by-Environment (GxE) interaction.",
    "crumbs": [
      "genetics",
      "Papers",
      "Genetic architecture: the shape of the genetic contribution to human traits and disease"
    ]
  },
  {
    "objectID": "genetics/timpson_2017_29225335.html#impact-of-modern-genomic-technologies-on-architecture-discovery",
    "href": "genetics/timpson_2017_29225335.html#impact-of-modern-genomic-technologies-on-architecture-discovery",
    "title": "Genetic architecture: the shape of the genetic contribution to human traits and disease",
    "section": "Impact of Modern Genomic Technologies on Architecture Discovery",
    "text": "Impact of Modern Genomic Technologies on Architecture Discovery\nThe review highlights how different technologies have been instrumental in characterizing specific aspects of the genetic architecture:\n\nGWAS and Common Variant Architecture\n\nGWAS Success: GWAS has been highly successful in identifying thousands of common, low-effect variants for hundreds of traits, confirming the extreme polygenicity of complex traits.\nMissing Heritability: The review addresses the historical problem of “missing heritability”—the gap between heritability estimated from twin/family studies (broad-sense heritability) and that explained by all detected common SNPs (SNP-heritability). Explanations include:\n\nThe contribution of rare variants missed by GWAS arrays.\nThe residual influence of non-additive effects (dominance and epistasis) captured by family studies but not fully by linear GWAS models.\nThe contribution of structural variation and gene-environment interactions.\n\nLocus Heterogeneity: GWAS often reveals multiple independent associated signals within the same locus, indicating allelic series or complex local regulation.\n\n\n\nNext-Generation Sequencing (NGS) and Rare Variants\n\nSequencing Role: NGS studies (e.g., whole-exome sequencing, whole-genome sequencing) are essential for characterizing the role of rare variants.\nBurden Tests: These tests aggregate the effects of multiple rare variants within a single gene or region. The review notes that rare, high-penetrance variants often reside in genes under strong negative selection, explaining why their overall contribution to population variance (though individually large) may be limited.\nClinical Relevance: Rare variants are crucial for understanding Mendelian disease and for identifying genes with large effects that are strong candidates for drug targets.",
    "crumbs": [
      "genetics",
      "Papers",
      "Genetic architecture: the shape of the genetic contribution to human traits and disease"
    ]
  },
  {
    "objectID": "genetics/timpson_2017_29225335.html#complexity-of-genetic-effects",
    "href": "genetics/timpson_2017_29225335.html#complexity-of-genetic-effects",
    "title": "Genetic architecture: the shape of the genetic contribution to human traits and disease",
    "section": "Complexity of Genetic Effects",
    "text": "Complexity of Genetic Effects\n\nPleiotropy and Shared Genetic Etiology\n\nWidespread Pleiotropy: The authors emphasize that pleiotropy (a single genetic variant affecting multiple distinct traits) is the norm, not the exception, for common variants identified by GWAS. This is supported by analyses showing extensive genetic correlation between traits.\nConfounding: Pleiotropy complicates causal inference. The review discusses how techniques like Mendelian Randomization (MR) are used to distinguish true causal effects from horizontal pleiotropy (a single variant affecting multiple outcomes through different pathways).\n\n\n\nGene-Environment (GxE) Interactions\n\nDefinition: GxE occurs when the effect of a genetic variant on a phenotype depends on the individual’s environment (e.g., diet, smoking, stress).\nDetection Challenge: GxE interactions are notoriously difficult to detect and estimate accurately due to requiring large samples with precise environmental measures. The review notes that population-based cohorts like the UK Biobank are vital for making progress in this area.",
    "crumbs": [
      "genetics",
      "Papers",
      "Genetic architecture: the shape of the genetic contribution to human traits and disease"
    ]
  },
  {
    "objectID": "genetics/timpson_2017_29225335.html#future-directions-and-clinical-goals",
    "href": "genetics/timpson_2017_29225335.html#future-directions-and-clinical-goals",
    "title": "Genetic architecture: the shape of the genetic contribution to human traits and disease",
    "section": "Future Directions and Clinical Goals",
    "text": "Future Directions and Clinical Goals\nThe review concludes by outlining the necessary steps to fully characterize genetic architecture and achieve the field’s clinical goals:\n\nComprehensive Mapping: Moving from association studies to causal variant identification, focusing on non-coding variants and improving fine-mapping methods.\nAccounting for Non-Additivity: Developing statistical methods that are better powered to detect and estimate dominance and epistatic effects in large cohorts.\nIntegrating Environment: Robustly incorporating environmental exposure data into models to quantify the contribution of GxE interactions and improve personalized risk prediction.\nClinical Translation: Leveraging the understanding of genetic architecture to improve disease screening, diagnosis, prognosis, and therapeutic development. This includes prioritizing genes for drug development based on the magnitude and specificity of their genetic effects.",
    "crumbs": [
      "genetics",
      "Papers",
      "Genetic architecture: the shape of the genetic contribution to human traits and disease"
    ]
  },
  {
    "objectID": "genetics/franks_2017_28481885.html",
    "href": "genetics/franks_2017_28481885.html",
    "title": "Post-transcriptional regulation across human tissues",
    "section": "",
    "text": "PubMed: 28481885 DOI: 10.1371/journal.pcbi.1005535 Overview generated by: Gemini 2.5 Flash, 10/12/2025",
    "crumbs": [
      "genetics",
      "Papers",
      "Post-transcriptional regulation across human tissues"
    ]
  },
  {
    "objectID": "genetics/franks_2017_28481885.html#research-goal-and-study-design",
    "href": "genetics/franks_2017_28481885.html#research-goal-and-study-design",
    "title": "Post-transcriptional regulation across human tissues",
    "section": "Research Goal and Study Design",
    "text": "Research Goal and Study Design\nThe primary goal of this research was to estimate the relative contributions of transcriptional and post-transcriptional regulation in shaping tissue-type-specific proteomes across human tissues. The study addressed the long-standing contestation over whether protein levels are primarily set by mRNA levels or by other regulatory mechanisms.\n\nDistinguishing Variability Sources\nA core methodological distinction was made between two orthogonal sources of protein variability in the data, which often become conflated in correlation analyses: 1. Mean-Level Variability: The differences in the mean abundance of different proteins (e.g., highly abundant ribosomal proteins vs. less abundant signaling proteins). Scaled mRNA levels were found to account for most of this variability. 2. Across-Tissues Variability: The physiological differences in the abundance of the same protein across different tissue types. This variability, though smaller in magnitude, is critical for defining tissue identity.\n\n\nMethods and Data\nThe study performed a statistical analysis using large cohorts of mRNA (RNA-seq) and protein (shotgun mass spectrometry) data measured across 12 different human tissues.\n\nData Reliability Assessment: The authors rigorously estimated the reliability of relative mRNA and protein quantification. They found that low reliability, especially across studies, limited the accurate quantification of regulatory mechanisms for individual proteins, indicating that much of the noise was study-dependent.\nConsensus Dataset: To improve data quality, a consensus protein dataset was generated by appropriately combining data from independent mass spectrometry studies, resulting in estimates with increased reliability.\nProtein-to-mRNA (PTR) Ratios: The relative protein-to-mRNA ratio (rPTR) was used as a measure of post-transcriptional regulation to quantify the variability of functional gene sets across tissues.",
    "crumbs": [
      "genetics",
      "Papers",
      "Post-transcriptional regulation across human tissues"
    ]
  },
  {
    "objectID": "genetics/franks_2017_28481885.html#key-findings-and-simpsons-paradox",
    "href": "genetics/franks_2017_28481885.html#key-findings-and-simpsons-paradox",
    "title": "Post-transcriptional regulation across human tissues",
    "section": "Key Findings and Simpson’s Paradox",
    "text": "Key Findings and Simpson’s Paradox\nThe analysis highlighted a crucial statistical nuance, demonstrating how misleading total correlation values can be in this context.\n\nThe Simpson’s Paradox Illustration\n\nThe overall correlation (\\(R_T\\)) between scaled mRNA and absolute protein levels across all genes and tissues was high (\\(R_T^2 \\approx 0.70\\)).\nHowever, this correlation was primarily driven by the large mean-level variability between different proteins.\nWhen examining the across-tissues variability for any single gene (within-gene correlation, \\(R_P\\)), the correlation was often low or near zero.\nThis discrepancy is an example of Simpson’s paradox, where a large overall trend (high \\(R_T\\)) masks the true, opposite trend within subgroups (low \\(R_P\\) for individual genes across tissues).\n\n\n\nEvidence for Post-transcriptional Regulation\n\nWeak Predictive Power: For any single gene, its protein levels across tissues were found to be poorly predicted by its corresponding mRNA levels, suggesting tissue-specific post-transcriptional regulation.\nFunctional Concertion: The analysis of relative PTR (rPTR) showed substantial across-tissues variability that was functionally concerted and reproducible across independent datasets, which further supports the existence of extensive post-transcriptional control.\nReliability-Corrected Estimates: After correcting for measurement noise and low data reliability, the results indicated that approximately 50% of the across-tissues protein variance could be attributed to transcriptional regulation, and approximately 50% was due to post-transcriptional regulation.",
    "crumbs": [
      "genetics",
      "Papers",
      "Post-transcriptional regulation across human tissues"
    ]
  },
  {
    "objectID": "genetics/franks_2017_28481885.html#conclusions-and-recommendations",
    "href": "genetics/franks_2017_28481885.html#conclusions-and-recommendations",
    "title": "Post-transcriptional regulation across human tissues",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study concludes that post-transcriptional regulation is a significant contributor to shaping tissue-type-specific proteomes in humans.\n\nThe results caution researchers against estimating protein fold-changes from mRNA fold-changes between different cell types.\nIt is critical to avoid conflating different sources of variability; the high correlation between absolute protein and mRNA levels should not be used to infer the degree of post-transcriptional regulation of a specific protein across different tissues.\nThe work underscores the fact that cell-type differentiation and commitment involve substantial post-transcriptional remodeling.",
    "crumbs": [
      "genetics",
      "Papers",
      "Post-transcriptional regulation across human tissues"
    ]
  },
  {
    "objectID": "ml/bobb_2015_25532525.html",
    "href": "ml/bobb_2015_25532525.html",
    "title": "Bayesian kernel machine regression for estimating the health effects of multi-pollutant mixtures",
    "section": "",
    "text": "PubMed: 25532525 DOI: 10.1093/biostatistics/kxu058 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "ml",
      "Papers",
      "Bayesian kernel machine regression for estimating the health effects of multi-pollutant mixtures"
    ]
  },
  {
    "objectID": "ml/bobb_2015_25532525.html#key-findings-bayesian-kernel-machine-regression-bkmr",
    "href": "ml/bobb_2015_25532525.html#key-findings-bayesian-kernel-machine-regression-bkmr",
    "title": "Bayesian kernel machine regression for estimating the health effects of multi-pollutant mixtures",
    "section": "Key Findings: Bayesian Kernel Machine Regression (BKMR)",
    "text": "Key Findings: Bayesian Kernel Machine Regression (BKMR)\nThis paper introduces and validates Bayesian Kernel Machine Regression (BKMR) as a flexible and powerful statistical method designed to estimate the complex, non-linear, and interactive health effects of multi-pollutant mixtures (e.g., mixtures of air pollutants, heavy metals, or environmental chemicals).\n\nAddressing Mixture Complexity: BKMR successfully addresses key challenges in environmental mixture analysis:\n\nNon-linearity: It can capture non-linear exposure–response relationships without requiring the pre-specification of functional forms.\nInteractions: It can estimate complex, high-order interactions between multiple pollutants.\nHigh-Dimensionality and Collinearity: It is robust to the high-dimensionality and high correlation (collinearity) that often exists among pollutants in real-world mixtures, a challenge that cripples traditional linear models.\n\nPollutant Importance: The method allows for the assessment of the relative importance of individual pollutants within the mixture (e.g., using Posterior Inclusion Probabilities (PIPs)) for identifying which component is the main driver of the health outcome.\nEstimation of Health Effects: BKMR provides flexible estimates of the health response surface, including:\n\nOverall Mixture Effect: The effect of the entire mixture when all components are held at a certain level (e.g., the median).\nUnivariate Effects: The change in the outcome associated with varying one pollutant while holding all others fixed.\nBivariate Effects: The interactive effect of two pollutants on the outcome.",
    "crumbs": [
      "ml",
      "Papers",
      "Bayesian kernel machine regression for estimating the health effects of multi-pollutant mixtures"
    ]
  },
  {
    "objectID": "ml/bobb_2015_25532525.html#study-design-and-methods",
    "href": "ml/bobb_2015_25532525.html#study-design-and-methods",
    "title": "Bayesian kernel machine regression for estimating the health effects of multi-pollutant mixtures",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\n\nMethodology\nThe authors adapted the core concepts of Kernel Machine Regression (KMR) into a Bayesian framework (BKMR), which provides several benefits, including automatic variable selection and quantifying uncertainty in estimates through posterior distributions.\n\nNon-parametric Regression: BKMR models the relationship between the health outcome (\\(Y\\)) and the mixture of pollutants (\\(Z\\)) using a flexible, non-parametric function \\(h(Z)\\): \\[Y = \\beta X + h(Z) + \\epsilon\\] where \\(\\beta X\\) represents linear effects of covariates, and \\(h(Z)\\) is the core kernel machine function that captures the non-linear and interactive effects of the mixture.\nKernel Function: The function \\(h(Z)\\) is represented as a linear combination of a set of kernel functions (specifically, the Gaussian kernel was primarily used) that quantify the similarity between observed pollutant profiles. This allows the model to “smooth” the response surface and capture non-linearities.\nBayesian Variable Selection: A hierarchical Bayesian model structure is implemented, including a Bayesian variable selection component (e.g., a latent indicator variable \\(I_k\\) for each pollutant \\(k\\)) to automatically determine which pollutants are important for inclusion in the kernel function \\(h(Z)\\). This yields Posterior Inclusion Probabilities (PIPs), a key metric for evaluating pollutant importance.\n\n\n\nData and Application\n\nSimulation Studies: Extensive simulation studies were performed to compare BKMR with standard methods like Generalized Additive Models (GAM) and Lasso regression, especially under scenarios of non-linearity, interaction, and collinearity. BKMR consistently outperformed competing methods in estimating the true exposure-response function and identifying key pollutants.\nReal-World Data Application (Air Pollution): The method was applied to a real-world environmental epidemiology problem using data from the Greater Boston Area to assess the health effects of a mixture of air pollutants (e.g., PM2.5 components, black carbon, ozone) on a specific health outcome (e.g., lung function or mortality). This demonstrated the model’s ability to handle high collinearity among pollutants.",
    "crumbs": [
      "ml",
      "Papers",
      "Bayesian kernel machine regression for estimating the health effects of multi-pollutant mixtures"
    ]
  },
  {
    "objectID": "ml/bobb_2015_25532525.html#conclusions-and-recommendations",
    "href": "ml/bobb_2015_25532525.html#conclusions-and-recommendations",
    "title": "Bayesian kernel machine regression for estimating the health effects of multi-pollutant mixtures",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study establishes BKMR as a crucial methodological advance for environmental epidemiology, providing the flexibility needed to accurately model the effects of complex environmental mixtures.\n\nMethod of Choice for Mixtures: BKMR is recommended as a preferred method for situations where non-linearity, interactions, and high correlations among exposures are anticipated, which is common in environmental and nutritional mixtures.\nFuture Directions: The authors suggest future research should focus on extending BKMR to:\n\nHandle time-varying exposures in longitudinal studies.\nIncorporate spatial misalignment or measurement error in exposure data.\nApply the method to even larger datasets and a wider variety of multi-exposure problems.",
    "crumbs": [
      "ml",
      "Papers",
      "Bayesian kernel machine regression for estimating the health effects of multi-pollutant mixtures"
    ]
  },
  {
    "objectID": "cancer/index.html",
    "href": "cancer/index.html",
    "title": "cancer",
    "section": "",
    "text": "Deep profiling of gene expression across 18 human cancers\n\n\n\nFramework: This paper introduces DeepProfile, an unsupervised deep-learning model using an ensemble of Variational Autoencoders (VAEs) to create robust and biologically interpretable low-dimensional latent spaces from 50,211 transcriptomes across 18 human cancers.\nKey Biological Findings: Universal latent variables across all 18 cancers are driven by genes controlling immune cell activation, while cancer-specific variables define molecular subtypes.\nClinical Associations: DeepProfile linked gene expression to clinical outcomes, finding that Tumour Mutation Burden (TMB) is associated with cell-cycle pathways, and patient survival correlates with DNA-mismatch repair and MHC class II antigen presentation pathway activity.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nGastric cancer: from biomarkers to functional precision medicine\n\n\n\nCurrent Landscape: Treatment for advanced gastric cancer is rapidly evolving beyond chemotherapy, with approved targeted agents including Trastuzumab (HER2), Zolbetuximab (CLDN18.2), and Pembrolizumab/Nivolumab (PD-L1).\nEmerging Targets: Key areas of development include novel Antibody-Drug Conjugates (ADCs), the anti-FGFR2b antibody Bemarituzumab, and bispecific antibodies to overcome resistance mechanisms and improve response rates.\nPrecision Medicine Goal: The future of GC therapy relies on integrating multi-omics analysis (biomarkers) with functional drug testing on patient-derived models (organoids) to enable individualized treatment selection and improve patient survival.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "cancer",
      "Papers"
    ]
  },
  {
    "objectID": "finemap-colocalisation/index.html",
    "href": "finemap-colocalisation/index.html",
    "title": "finemapping/colocalisation",
    "section": "",
    "text": "A frequentist test of proportional colocalization after selecting relevant genetic variants\n\n\n\nObjective: This paper introduced prop-coloc-cond, a novel frequentist test of proportional colocalization designed to assess whether two traits share the same causal genetic variants in a specific genomic region.\nMethodological Advance: The key innovation is that the test formally accounts for the uncertainty introduced by selecting the relevant genetic variants (e.g., through fine-mapping) before testing for proportionality, which ensures accurate Type I error control.\nRole in Research: This test provides a valuable complement to existing Bayesian colocalization methods, offering robust evidence based on different statistical assumptions, particularly useful when Bayesian results are inconclusive or sensitive to prior choices.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "finemap-colocalisation",
      "Papers"
    ]
  },
  {
    "objectID": "pgs/ding_2023_37198491.html",
    "href": "pgs/ding_2023_37198491.html",
    "title": "Polygenic scoring accuracy varies across the genetic ancestry continuum",
    "section": "",
    "text": "PubMed: 37198491 DOI: 10.1038/s41586-023-06079-4 Overview generated by: Gemini 2.5 Flash, 26/11/2025",
    "crumbs": [
      "PGS",
      "Papers",
      "Polygenic scoring accuracy varies across the genetic ancestry continuum"
    ]
  },
  {
    "objectID": "pgs/ding_2023_37198491.html#key-findings-the-genetic-distance-penalty-on-pgs-accuracy",
    "href": "pgs/ding_2023_37198491.html#key-findings-the-genetic-distance-penalty-on-pgs-accuracy",
    "title": "Polygenic scoring accuracy varies across the genetic ancestry continuum",
    "section": "Key Findings: The Genetic Distance Penalty on PGS Accuracy",
    "text": "Key Findings: The Genetic Distance Penalty on PGS Accuracy\nThis study provides a critical, individual-level assessment of Polygenic Score (PGS) portability, which is necessary for the equitable clinical application of genetic risk prediction. The authors argue that assessing PGS performance using traditional discrete genetic ancestry clusters (e.g., European, African) obscures crucial inter-individual variation and biases estimates. They introduce a framework that evaluates accuracy along a genetic ancestry continuum using a precise metric: Genetic Distance (GD).\n\nCore Discovery: Continuous and Steep Decay of Accuracy\nThe central finding is the demonstration that PGS accuracy decreases individual-to-individual along the continuum of genetic ancestries in a highly predictable, linear fashion .\n\nMetric Definition: Genetic Distance (GD) is defined as the distance of a target individual’s genotype (e.g., PCA projection) from the population used to train the PGS model. The higher the GD, the more genetically dissimilar the individual is from the training set.\nQuantification of Decay: Across a large set of 84 complex traits and diseases, the average individual-level PGS accuracy showed an extremely powerful negative correlation with GD, with a Pearson correlation coefficient of -0.95. This near-perfect correlation highlights that the individual’s genetic background relative to the training population is the primary determinant of score performance.\nUbiquity of Variation: This decreasing trend was observed in all populations considered, including within traditionally labeled ‘homogeneous’ genetic ancestry groups (e.g., European ancestry in UK Biobank). This shows that sub-ancestry variation within a single continent still results in a measurable loss of accuracy based on GD.\n\n\n\nDemonstrating Inequity and Systematic Bias\nThe study leveraged data from the UK Biobank (UKBB, training set, predominantly White British) and the diverse Los Angeles biobank (ATLAS, testing set) to quantify the transferability gap.\n\nIntra-European Penalty: When applying UKBB-trained models to individuals of European ancestry in ATLAS, those in the furthest GD decile experienced a significant 14% lower accuracy relative to those in the closest decile.\nCross-Ancestry Disparity: The results reveal a severe “distance penalty” for non-European groups. Individuals of Hispanic/Latino American ancestry who are genetically closest (lowest GD decile) to the training data showed similar PGS performance to the European-ancestry individuals who are furthest away (highest GD decile). For the most distant Hispanic/Latino individuals, accuracy was substantially lower, overlapping with that of African American participants.\nBias in Risk Estimates: Crucially, GD was found to be significantly correlated with the PGS estimates themselves for 82 of 84 traits. This means the systematic bias due to ancestry distance does not just affect the accuracy (\\(R^2\\)) but also the magnitude of the predicted risk, potentially leading to widespread miscalibration and inequitable risk stratification.\n\n\n\nConclusion and Call to Action\nThe authors conclude that relying on aggregate population-level metrics (\\(\\text{e.g., } R^2\\)) obscures this vital individual-level variation and hinders efforts toward health equity. They urge researchers to abandon the use of discrete genetic ancestry clusters in favor of continuous metrics (like GD) to better characterize and address performance disparities, ensuring more reliable and equitable application of PGSs in personalized medicine.",
    "crumbs": [
      "PGS",
      "Papers",
      "Polygenic scoring accuracy varies across the genetic ancestry continuum"
    ]
  },
  {
    "objectID": "pgs/index.html",
    "href": "pgs/index.html",
    "title": "polygenic scores",
    "section": "",
    "text": "Genetic prediction of complex traits with polygenic scores: A statistical review\n\n\n\nThis statistical review comprehensively analyzes 46 methods for Polygenic Score (PGS) construction, unifying most of them under a multiple linear regression framework to clarify their assumptions regarding effect size distribution and Linkage Disequilibrium (LD).\nThe review concludes that optimal PGS performance (accuracy) is highly dependent on the trait’s genetic architecture and is significantly improved by using Bayesian/Regularization methods (e.g., LDpred, PRS-CS) that explicitly model LD and incorporate informed prior distributions for SNP effects.\nA critical challenge highlighted is the significant loss of transferability across ancestral populations, underscoring the need for more diverse training data and methods that better address ancestral heterogeneity and incorporate non-additive and Gene-by-Environment (GxE) effects.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nIncorporating functional priors improves polygenic prediction accuracy in UK Biobank and 23andMe data sets\n\n\n\nGoal: To introduce LDpred-funct, a polygenic prediction method that uses trait-specific functional priors to enhance prediction accuracy.\nMethod: LDpred-funct incorporates information from the baseline-LD model (including coding, regulatory, and conserved annotations) to estimate posterior mean causal effect sizes, followed by a cross-validation regularization step to account for genetic sparsity.\nFinding: The method achieved a +4.6% relative improvement in average prediction \\(R^2\\) across 21 UK Biobank traits compared to the best non-functional method. This demonstrates that leveraging the functional architecture of the genome leads to more accurate Polygenic Risk Scores (PRS).\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nGenome-wide risk prediction of common diseases across ancestries in one million people\n\n\n\nThis large-scale study evaluated the cross-ancestry transferability of Polygenic Risk Scores (PRSs) for four common diseases (CAD, T2D, breast, and prostate cancer) using data from six biobanks and over one million individuals of diverse global ancestries.\nThe analysis found that PRS transferability was high and robust across different populations and substructures of European ancestry, but was significantly lower for individuals of African, South Asian, and East Asian ancestry.\nThe poor transferability, which was most pronounced in African ancestry individuals, highlights the critical issue of ancestral bias in genomic research and the potential for current PRS implementation to exacerbate health disparities until more diverse training data are available.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nDefining type 2 diabetes polygenic risk scores through colocalization and network-based clustering of metabolic trait genetic associations\n\n\n\nCore Principle: This study partitioned the genetic heterogeneity of Type 2 Diabetes (T2D) using a novel colocalization-first approach followed by network-based clustering of T2D and 20 related metabolic traits across 243 loci.\nKey Finding: The method identified five distinct T2D biological pathways (Obesity, Lipodystrophic insulin resistance, Liver/lipid metabolism, Hepatic glucose metabolism, and Beta-cell dysfunction), successfully isolating genetically distinct disease mechanisms.\nClinical Significance: Partitioned Polygenic Risk Scores (PRSs) showed heterogeneous clinical associations in a validation cohort (n=21,742 T2D individuals); notably, the Lipodystrophic insulin resistance PRS and Beta-cell dysfunction PRS were causally associated with lower BMI, providing genetic validation for the clinically important “lean diabetes” sub-type.\nMethodological Advance: By integrating colocalization and Mendelian Randomization, the framework provided stronger inferences on the causality and directionality of the genetic associations, which is essential for translating genetic discoveries into targeted T2D treatments.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nPolygenic scoring accuracy varies across the genetic ancestry continuum\n\n\n\nCore Principle: This landmark study challenged the traditional use of discrete ancestry groups for polygenic scores (PGSs), proposing a framework that evaluates individual-level PGS accuracy based on Genetic Distance (GD) from the GWAS training population.\nKey Finding: They demonstrated that individual-level PGS accuracy experiences a continuous, steep decay as GD from the training data increases, with an average Pearson correlation of R = -0.95 across 84 complex traits, confirming that performance loss is a predictable function of genetic dissimilarity.\nHealth Equity Implication: The study quantified a major health disparity, showing that the genetically closest individuals of non-European ancestry (e.g., Hispanic/Latino American) had PGS accuracy comparable to the most distant European-ancestry individuals, highlighting the severe and systematic bias due to lack of diversity in training cohorts.\nRecommendation: The authors advocate for moving beyond discrete ancestry labels and using continuous metrics like GD to characterize and correct for performance disparities, thus ensuring more equitable clinical translation of PGSs.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "PGS",
      "Papers"
    ]
  },
  {
    "objectID": "pgs/marquezluna_2021_34663819.html",
    "href": "pgs/marquezluna_2021_34663819.html",
    "title": "Incorporating functional priors improves polygenic prediction accuracy in UK Biobank and 23andMe data sets",
    "section": "",
    "text": "PubMed: 34663819 DOI: 10.1038/s41467-021-25171-9 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "PGS",
      "Papers",
      "Incorporating functional priors improves polygenic prediction accuracy in UK Biobank and 23andMe data sets"
    ]
  },
  {
    "objectID": "pgs/marquezluna_2021_34663819.html#key-findings-ldpred-funct-for-pgs",
    "href": "pgs/marquezluna_2021_34663819.html#key-findings-ldpred-funct-for-pgs",
    "title": "Incorporating functional priors improves polygenic prediction accuracy in UK Biobank and 23andMe data sets",
    "section": "Key Findings: LDpred-funct for PGS",
    "text": "Key Findings: LDpred-funct for PGS\nThe study introduces and validates LDpred-funct, a new method for polygenic prediction that leverages trait-specific functional priors to increase prediction accuracy.\n\nPrediction Accuracy Improvement: When applied to 21 highly heritable traits in the UK Biobank, LDpred-funct attained a +4.6% relative improvement in average prediction accuracy (average prediction \\(R^2 = 0.144\\)) compared to SBayesR, which was identified as the best performing method that does not incorporate functional information.\nHighest Accuracy: LDpred-funct achieved the highest \\(R^2\\) of 0.413 for height in the UK Biobank. Meta-analyzing training data for height from UK Biobank and 23andMe cohorts (\\(N=1107K\\)) further increased the prediction \\(R^2\\) to 0.431.\nComparison: LDpred-funct was found to have substantially higher prediction accuracy than other comparable functional and non-functional methods, including P+T-funct-LASSO and AnnoPred, in most settings.",
    "crumbs": [
      "PGS",
      "Papers",
      "Incorporating functional priors improves polygenic prediction accuracy in UK Biobank and 23andMe data sets"
    ]
  },
  {
    "objectID": "pgs/marquezluna_2021_34663819.html#methods-leveraging-functional-priors",
    "href": "pgs/marquezluna_2021_34663819.html#methods-leveraging-functional-priors",
    "title": "Incorporating functional priors improves polygenic prediction accuracy in UK Biobank and 23andMe data sets",
    "section": "Methods: Leveraging Functional Priors",
    "text": "Methods: Leveraging Functional Priors\nLDpred-funct is built on the principle that genetic variants in functional regions of the genome are enriched for complex trait heritability, and this information can be leveraged to improve Polygenic Risk Scores (PRS).\n\nPriors Model: The method fits functional priors using the established baseline-LD model. This model includes various annotations such as coding, conserved, regulatory, and LD-related annotations.\nCalculation: It first analytically estimates posterior mean causal effect sizes while accounting for the functional priors and linkage disequilibrium (LD) between variants.\nRegularization: It then uses cross-validation to regularize these causal effect size estimates in bins of different magnitudes, which is specifically designed to improve prediction accuracy for traits with sparse genetic architectures (fewer causal variants).\nData Sets: The models were tested using 21 highly heritable traits across the UK Biobank (average \\(N=373K\\) training data) and through meta-analysis of height using data from both UK Biobank and 23andMe cohorts.",
    "crumbs": [
      "PGS",
      "Papers",
      "Incorporating functional priors improves polygenic prediction accuracy in UK Biobank and 23andMe data sets"
    ]
  },
  {
    "objectID": "pgs/marquezluna_2021_34663819.html#conclusions-and-significance",
    "href": "pgs/marquezluna_2021_34663819.html#conclusions-and-significance",
    "title": "Incorporating functional priors improves polygenic prediction accuracy in UK Biobank and 23andMe data sets",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nThe study concludes that incorporating functional priors into polygenic prediction methods significantly improves accuracy, confirming the importance of the functional architecture of complex traits. The LDpred-funct method offers a powerful and scalable way to integrate this functional information into the prediction of Polygenic Risk Scores.",
    "crumbs": [
      "PGS",
      "Papers",
      "Incorporating functional priors improves polygenic prediction accuracy in UK Biobank and 23andMe data sets"
    ]
  },
  {
    "objectID": "index.html#proteomics",
    "href": "index.html#proteomics",
    "title": "",
    "section": "proteomics",
    "text": "proteomics"
  },
  {
    "objectID": "index.html#metabolomics",
    "href": "index.html#metabolomics",
    "title": "",
    "section": "metabolomics",
    "text": "metabolomics"
  },
  {
    "objectID": "index.html#multi-omics",
    "href": "index.html#multi-omics",
    "title": "",
    "section": "multi-omics",
    "text": "multi-omics"
  },
  {
    "objectID": "index.html#statistics",
    "href": "index.html#statistics",
    "title": "",
    "section": "statistics",
    "text": "statistics"
  },
  {
    "objectID": "index.html#mr",
    "href": "index.html#mr",
    "title": "",
    "section": "MR",
    "text": "MR"
  },
  {
    "objectID": "mr/thornton_2025_39565278.html",
    "href": "mr/thornton_2025_39565278.html",
    "title": "Brain multi-omic Mendelian randomisation to identify novel drug targets for gliomagenesis",
    "section": "",
    "text": "PubMed: 39565278 DOI: 10.1093/hmg/ddae168 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "MR",
      "Papers",
      "Brain multi-omic Mendelian randomisation to identify novel drug targets for gliomagenesis"
    ]
  },
  {
    "objectID": "mr/thornton_2025_39565278.html#key-findings-multi-omic-prioritization-of-glioma-drug-targets",
    "href": "mr/thornton_2025_39565278.html#key-findings-multi-omic-prioritization-of-glioma-drug-targets",
    "title": "Brain multi-omic Mendelian randomisation to identify novel drug targets for gliomagenesis",
    "section": "Key Findings: Multi-Omic Prioritization of Glioma Drug Targets",
    "text": "Key Findings: Multi-Omic Prioritization of Glioma Drug Targets\nThis study utilized a novel multi-omic Mendelian randomization (MR) approach, integrating genetic, transcriptomic (gene expression), and proteomic (protein expression) data specifically from human brain tissue, to prioritize genes and proteins causally implicated in the development of glioma and its subtypes (glioblastoma, GBM).\n\nPrioritized Causal Genes (Transcriptome): The MR analysis using brain-specific expression data (eQTLs) identified 25 genes whose genetically predicted expression was causally associated with overall glioma risk, including several novel candidate genes.\nPrioritized Causal Proteins (Proteome): A key advancement was the use of brain-specific proteomic data (pQTLs), which identified 13 proteins whose genetically predicted levels were causally associated with glioma risk. The most compelling findings included:\n\nIncreased risk of GBM associated with genetically predicted lower levels of the protein FAM178B (Odds Ratio (OR) of 0.81 per 1 SD decrease, \\(P=1.92 \\times 10^{-6}\\)).\nIncreased risk of GBM associated with genetically predicted higher levels of the protein MDM4 (OR of 1.15 per 1 SD increase, \\(P=2.87 \\times 10^{-5}\\)).\n\nIntegration of Omics: The multi-omic approach successfully validated previously known targets (e.g., TP53, MDM4) and identified novel mechanisms. The strongest and most robust evidence for potential drug targets were the proteins FAM178B and MDM4, which showed consistent association with glioma subtypes.\nTissue Specificity Confirmed: The results confirmed the importance of brain-specific molecular data, as the effects observed were often specific to brain tissue and not replicated when using non-brain tissues (e.g., whole blood).",
    "crumbs": [
      "MR",
      "Papers",
      "Brain multi-omic Mendelian randomisation to identify novel drug targets for gliomagenesis"
    ]
  },
  {
    "objectID": "mr/thornton_2025_39565278.html#study-design-and-methods",
    "href": "mr/thornton_2025_39565278.html#study-design-and-methods",
    "title": "Brain multi-omic Mendelian randomisation to identify novel drug targets for gliomagenesis",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\n\nStudy Design\nThis was a two-sample multi-omic Mendelian randomization (MR) study. It leveraged two distinct sets of instrumental variables: one for gene expression and one for protein expression, both derived from brain tissue, to infer causal relationships with glioma risk.\n\n\nData Sources and Instrumental Variables\n\nOutcome GWAS Data (Glioma Risk): Summary statistics were used for overall glioma risk and the specific subtypes glioblastoma (GBM) and non-GBM glioma, sourced from large-scale consortia (up to 7,400 cases).\nExposure Data (Brain Multi-omics):\n\nTranscriptomics (Gene Expression): Expression quantitative trait loci (eQTLs) were used from the GTEx consortium and other brain-specific datasets for gene expression in various brain regions.\nProteomics (Protein Expression): Protein quantitative trait loci (pQTLs) were used from two key human brain-derived proteomic datasets, providing genetic instruments for protein levels.\n\nInstrument Selection: Genetic variants (SNPs) acting as both eQTLs or pQTLs were selected. Strict quality control, including linkage disequilibrium (LD) clumping, was applied to ensure the independence of the instruments.\n\n\n\nStatistical Analysis\n\nPrimary MR Method: The Inverse-Variance Weighted (IVW) method was used to calculate the causal effect estimates (Odds Ratios, ORs).\nSensitivity Analyses: The following robust MR methods were applied to assess the validity of the instruments and detect pleiotropy:\n\nMR-Egger regression (to test for balanced pleiotropy).\nWeighted Median and Weighted Mode estimators (to provide consistent estimates even with invalid instruments).\nMR-PRESSO (to detect and adjust for horizontal pleiotropy outliers).\n\nColocalization Analysis: Bayesian colocalization (using the moloc method) was performed to ensure that the same causal variant drove both the molecular exposure (expression or protein level) and the disease outcome (glioma risk), strengthening the causal evidence.\nDrug Target Prioritization: The results were systematically compared against existing databases (DGIdb, Open Targets) to prioritize genes and proteins that are known to be targetable by existing drugs or are considered strong drug candidates.",
    "crumbs": [
      "MR",
      "Papers",
      "Brain multi-omic Mendelian randomisation to identify novel drug targets for gliomagenesis"
    ]
  },
  {
    "objectID": "mr/thornton_2025_39565278.html#conclusions-and-recommendations",
    "href": "mr/thornton_2025_39565278.html#conclusions-and-recommendations",
    "title": "Brain multi-omic Mendelian randomisation to identify novel drug targets for gliomagenesis",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe multi-omic MR pipeline successfully identified and prioritized several brain-specific molecular targets for gliomagenesis, offering substantial advantages over relying solely on genetic or expression data.\n\nDrug Development Focus: The strong evidence for a causal role of the proteins FAM178B and MDM4 in GBM risk, based on brain-derived proteomic data, provides highly compelling candidates for drug development and repurposing efforts.\nMethodological Advance: The study strongly recommends the use of brain-specific pQTL data in future MR studies of neurological diseases, as protein levels are often a more direct and clinically relevant measure of biological function than mRNA expression.\nNeed for Functional Studies: While the MR analysis provides strong evidence for a causal link, the authors stress that follow-up functional validation studies are necessary to fully elucidate the exact molecular mechanisms by which these prioritized targets influence tumor risk.",
    "crumbs": [
      "MR",
      "Papers",
      "Brain multi-omic Mendelian randomisation to identify novel drug targets for gliomagenesis"
    ]
  },
  {
    "objectID": "mr/hemani_2017_29149188.html",
    "href": "mr/hemani_2017_29149188.html",
    "title": "Orienting the causal relationship between imprecisely measured traits using GWAS summary data",
    "section": "",
    "text": "PubMed: 29149188 DOI: 10.1371/journal.pgen.1007081 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "MR",
      "Papers",
      "Orienting the causal relationship between imprecisely measured traits using GWAS summary data"
    ]
  },
  {
    "objectID": "mr/hemani_2017_29149188.html#key-findings-causal-direction-between-traits",
    "href": "mr/hemani_2017_29149188.html#key-findings-causal-direction-between-traits",
    "title": "Orienting the causal relationship between imprecisely measured traits using GWAS summary data",
    "section": "Key Findings: Causal Direction between Traits",
    "text": "Key Findings: Causal Direction between Traits\nThis paper addresses the common challenge in Mendelian Randomization (MR) and other observational studies: determining the direction of the causal relationship between two highly correlated traits (\\(X\\) and \\(Y\\)). The authors propose a novel method using GWAS summary data to test for the existence and direction of a causal effect, even when the traits are measured with error. The key finding is that under certain conditions, the causal direction can be inferred by comparing the strength of the genetic associations with the two traits.",
    "crumbs": [
      "MR",
      "Papers",
      "Orienting the causal relationship between imprecisely measured traits using GWAS summary data"
    ]
  },
  {
    "objectID": "mr/hemani_2017_29149188.html#methods-mr-directionality-methods",
    "href": "mr/hemani_2017_29149188.html#methods-mr-directionality-methods",
    "title": "Orienting the causal relationship between imprecisely measured traits using GWAS summary data",
    "section": "Methods: MR Directionality Methods",
    "text": "Methods: MR Directionality Methods\nThe study proposes two primary methods for causal directionality testing, both rooted in the structure of MR and the concept of pleiotropy (where one gene affects multiple traits):\n\nComparison of Variance Explained (\\(R^2\\)): This method suggests that if trait \\(X\\) is the cause of trait \\(Y\\), the proportion of variance explained in \\(X\\) by the genetic instruments (\\(R^2_X\\)) should be greater than the proportion of variance explained in \\(Y\\) by those same instruments (\\(R^2_Y\\)). This comparison allows for the testing of a directional hypothesis.\nSteiger Filtering: This method, which has become a foundational tool in MR, formally tests the assumption that the genetic instrument influences the exposure (\\(X\\)) more strongly than it influences the outcome (\\(Y\\)). This test is used to filter out genetic variants that appear to be stronger instruments for the outcome than for the exposure, suggesting a potential reverse causation (i.e., \\(Y\\) causes \\(X\\)) or unmodeled pleiotropy. Steiger filtering is particularly robust because it uses GWAS summary data and accounts for the fact that many traits are measured with error (imprecise measurement).",
    "crumbs": [
      "MR",
      "Papers",
      "Orienting the causal relationship between imprecisely measured traits using GWAS summary data"
    ]
  },
  {
    "objectID": "mr/hemani_2017_29149188.html#results-application-and-robustness",
    "href": "mr/hemani_2017_29149188.html#results-application-and-robustness",
    "title": "Orienting the causal relationship between imprecisely measured traits using GWAS summary data",
    "section": "Results: Application and Robustness",
    "text": "Results: Application and Robustness\nThe methods were tested using simulated data and applied to real-world traits with known causal direction.\n\nSimulation Performance: Simulations demonstrated that the Steiger filtering approach was effective in correctly orienting the causal relationship under various scenarios, including the presence of measurement error in both traits.\nReal-World Application: When applied to the causal relationship between height and educational attainment—where the causal direction is known to be primarily from height to educational attainment due to confounding factors—the methods successfully identified the correct direction.",
    "crumbs": [
      "MR",
      "Papers",
      "Orienting the causal relationship between imprecisely measured traits using GWAS summary data"
    ]
  },
  {
    "objectID": "mr/hemani_2017_29149188.html#conclusions-and-recommendations",
    "href": "mr/hemani_2017_29149188.html#conclusions-and-recommendations",
    "title": "Orienting the causal relationship between imprecisely measured traits using GWAS summary data",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe paper provides robust statistical methods for determining the causal direction between two traits using readily available GWAS summary statistics.\n\nUtility in MR: The methods are critical for two-sample MR, where the causal direction is often assumed but may be incorrect. Steiger filtering has become a standard check to validate that the genetic instrument primarily targets the intended exposure.\nImprecise Measurement: A major strength is the ability to account for the impact of imprecise or confounded measurement of traits, a pervasive issue in observational epidemiology.\nCausal Inference: The approach adds rigor to causal inference by allowing researchers to test, rather than simply assume, the direction of the relationship, greatly reducing the risk of making reverse causal inferences.",
    "crumbs": [
      "MR",
      "Papers",
      "Orienting the causal relationship between imprecisely measured traits using GWAS summary data"
    ]
  },
  {
    "objectID": "mr/coscia_2022_35639294.html",
    "href": "mr/coscia_2022_35639294.html",
    "title": "Avoiding collider bias in Mendelian randomization when performing stratified analyses",
    "section": "",
    "text": "PubMed: 35639294 DOI: 10.1007/s10654-022-00879-0 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "MR",
      "Papers",
      "Avoiding collider bias in Mendelian randomization when performing stratified analyses"
    ]
  },
  {
    "objectID": "mr/coscia_2022_35639294.html#key-findings-correcting-collider-bias-in-mr-stratification",
    "href": "mr/coscia_2022_35639294.html#key-findings-correcting-collider-bias-in-mr-stratification",
    "title": "Avoiding collider bias in Mendelian randomization when performing stratified analyses",
    "section": "Key Findings: Correcting Collider Bias in MR Stratification",
    "text": "Key Findings: Correcting Collider Bias in MR Stratification\nThis paper addresses a critical methodological problem in Mendelian Randomization (MR): the bias introduced when performing stratified analyses by a variable that is a collider—a variable influenced by both the risk factor (exposure) and the outcome. The authors propose a novel, robust method to conduct MR stratification while avoiding collider bias.\n\nThe Problem of Collider Bias in MR: A naive MR analysis performed within strata of the population defined by a collider (e.g., performing MR for the effect of Smoking on Heart Disease, stratified by a variable influenced by both Smoking and Heart Disease) induces collider bias, leading to invalid causal estimates.\nThe Solution: Residual Collider Stratification: The proposed solution is to construct a new variable called the residual collider. This is calculated as the residual from a regression of the original collider on the genetic instrument(s) for the exposure.\n\nMechanism: Stratifying the population by quantiles of this residual collider ensures that the genetic instrument remains independent of the conditioning variable, thereby preserving the core assumption of MR and avoiding the introduction of collider bias.\n\nInterpretation of Stratified Estimates: Estimates stratified on the residual collider are shown to have an interpretation that is equivalent to the estimates stratified on the original collider, allowing researchers to still investigate heterogeneity of the causal effect across different population subgroups defined by the collider, but in a valid manner.\nSimulation Validation: Extensive simulation studies demonstrated that:\n\nNaive MR stratification by a collider led to substantial and consistent bias.\nStratification by the proposed residual collider successfully removed the collider bias and produced unbiased causal estimates across various scenarios, including different strengths of the instrument and different characteristics of the collider variable.",
    "crumbs": [
      "MR",
      "Papers",
      "Avoiding collider bias in Mendelian randomization when performing stratified analyses"
    ]
  },
  {
    "objectID": "mr/coscia_2022_35639294.html#study-design-and-methods",
    "href": "mr/coscia_2022_35639294.html#study-design-and-methods",
    "title": "Avoiding collider bias in Mendelian randomization when performing stratified analyses",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\n\nStudy Design\nThis was a methodological study using simulation and real-world data application to illustrate the introduction of collider bias in stratified MR analyses and validate a novel bias-correction approach.\n\n\nMethodology: Residual Collider\n\nDefine Collider (\\(C\\)): The variable used for stratification is a collider, influenced by the exposure (\\(X\\)) and the outcome (\\(Y\\)).\nDefine Genetic Instrument (\\(G\\)): The IVs used in the MR analysis.\nCalculate Residual Collider (\\(\\tilde{C}\\)): The key step is a regression of the collider \\(C\\) on the genetic instrument \\(G\\): \\[C = \\gamma G + \\tilde{C}\\] where \\(\\tilde{C}\\) is the residual collider, which is, by definition, uncorrelated with the instrument \\(G\\).\nStratified MR: The final MR analysis (e.g., IVW method) is then performed within strata (e.g., quartiles or quintiles) defined by \\(\\tilde{C}\\).\n\n\n\nData Application\n\nReal-World Application: The method was applied to a real-world example investigating the causal effect of educational attainment on a health outcome (e.g., cognitive function) using the UK Biobank data, stratified by a post-exposure factor like employment status (a potential collider). This demonstrated the practical utility of the residual collider approach.",
    "crumbs": [
      "MR",
      "Papers",
      "Avoiding collider bias in Mendelian randomization when performing stratified analyses"
    ]
  },
  {
    "objectID": "mr/coscia_2022_35639294.html#conclusions-and-recommendations",
    "href": "mr/coscia_2022_35639294.html#conclusions-and-recommendations",
    "title": "Avoiding collider bias in Mendelian randomization when performing stratified analyses",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study concludes that researchers can validly investigate the heterogeneity of causal effects across strata of a population in MR studies, provided they use the novel residual collider method.\n\nGuidance for MR Practice: The authors strongly recommend that researchers avoid naive MR stratification by a collider and instead implement the residual collider approach when studying effect heterogeneity related to a post-exposure factor.\nGeneralizability: The methodology is flexible and can be applied across various MR methods and scenarios where a valid instrument is available, making it a broadly useful tool for causal inference in epidemiology.\nFuture Work: Further research is suggested to extend the residual collider approach to non-linear MR models and scenarios involving multiple stratifying variables.",
    "crumbs": [
      "MR",
      "Papers",
      "Avoiding collider bias in Mendelian randomization when performing stratified analyses"
    ]
  },
  {
    "objectID": "mr/leyden_2025_40375348.html",
    "href": "mr/leyden_2025_40375348.html",
    "title": "Distinct pathway-based effects of blood pressure and body mass index on cardiovascular traits: comparison of novel Mendelian randomization approaches",
    "section": "",
    "text": "PubMed: 40375348 DOI: 10.1186/s13073-025-01472-2 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "MR",
      "Papers",
      "Distinct pathway-based effects of blood pressure and body mass index on cardiovascular traits: comparison of novel Mendelian randomization approaches"
    ]
  },
  {
    "objectID": "mr/leyden_2025_40375348.html#key-findings-comparing-mr-partitioning-approaches",
    "href": "mr/leyden_2025_40375348.html#key-findings-comparing-mr-partitioning-approaches",
    "title": "Distinct pathway-based effects of blood pressure and body mass index on cardiovascular traits: comparison of novel Mendelian randomization approaches",
    "section": "Key Findings: Comparing MR Partitioning Approaches",
    "text": "Key Findings: Comparing MR Partitioning Approaches\nThis study introduces a novel pathway-partitioned Mendelian randomization (MR) approach and compares its performance and findings against an existing tissue-partitioned MR approach to dissect the heterogeneous genetic effects of Blood Pressure (BP) and Body Mass Index (BMI) on downstream cardiovascular traits.\n\nPathway-Partitioned MR is Effective: The novel pathway-partitioned approach successfully isolated sets of genetic instruments based on their proximity to genes implicated in Mendelian disorders of the renal system or vasculature (for BP), or mental health or metabolic disorders (for BMI).\nDistinct Causal Pathways for BP:\n\nRenal Pathway: Genetic instruments related to the renal system were significantly and strongly associated with a causal risk of Coronary Artery Disease (CAD).\nVascular Pathway: Genetic instruments related to the vasculature were significantly and strongly associated with a causal risk of Stroke.\n\nConfirmation of BMI Pathways: The pathway-partitioned MR for BMI (Mental Health vs. Metabolic disorders) largely confirmed the findings of the previously developed tissue-partitioned approach (Brain vs. Adipose tissue). Both methodologies consistently showed that genetic variants related to central/appetite regulation (Mental Health/Brain-mediated) had a stronger causal effect on Type 2 Diabetes Mellitus (T2DM) risk compared to the metabolic/peripheral pathways.\nComplementary Methods: The study concludes that both the pathway-partitioned and tissue-partitioned MR methods are valid, complementary tools for disentangling the genetic heterogeneity of complex traits, offering more granular insights into disease mechanisms than conventional MR which uses a single, aggregated instrument set.",
    "crumbs": [
      "MR",
      "Papers",
      "Distinct pathway-based effects of blood pressure and body mass index on cardiovascular traits: comparison of novel Mendelian randomization approaches"
    ]
  },
  {
    "objectID": "mr/leyden_2025_40375348.html#study-design-and-methods",
    "href": "mr/leyden_2025_40375348.html#study-design-and-methods",
    "title": "Distinct pathway-based effects of blood pressure and body mass index on cardiovascular traits: comparison of novel Mendelian randomization approaches",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\n\nStudy Design\nThe study employed a two-sample Mendelian randomization (MR) design, introducing a new method for partitioning genetic instrumental variables (IVs) and comparing it against a previous method (tissue-partitioned MR). The analysis was conducted using both individual-level (UK Biobank) and summary-level MR methodologies.\n\n\nInstrument Partitioning Approaches\n\nPathway-Partitioned MR (Novel Method):\n\nMechanism: SNPs were partitioned based on their proximity to genes associated with specific Mendelian diseases known to affect the trait.\nBP Partitioning: IVs were grouped into Renal System (e.g., genes causing monogenic hypertension/kidney disease) and Vasculature (e.g., genes causing monogenic vascular disorders).\nBMI Partitioning: IVs were grouped into Mental Health (genes implicated in psychiatric/neurodevelopmental disorders, often affecting appetite) and Metabolic disorders (genes linked to monogenic obesity/metabolic conditions).\n\nTissue-Partitioned MR (Existing Method):\n\nMechanism: SNPs were partitioned based on genetic colocalization and causal association with gene expression in contrasting tissues.\nBMI Partitioning: IVs were grouped into Brain-mediated and Adipose-mediated sets, mirroring the previous study by the same authors.\n\n\n\n\nData Sources and Outcomes\n\nExposure GWAS: Summary statistics for BP and BMI were sourced from large, publicly available consortia (e.g., UK Biobank, GIANT).\nOutcome GWAS: Cardiometabolic outcomes included CAD, Stroke (all subtypes), and T2DM.\n\n\n\nStatistical Analysis\n\nMR Methods: The Inverse-Variance Weighted (IVW) method was the primary technique for estimating causal effects within each partitioned gene set. This was supplemented by sensitivity analyses (e.g., MR-Egger, Weighted Median).\nComparison: The causal effect estimates (e.g., Odds Ratios, ORs) derived from the different pathway/tissue partitions were compared using formal statistical tests to determine if the effect estimates were significantly different from each other.",
    "crumbs": [
      "MR",
      "Papers",
      "Distinct pathway-based effects of blood pressure and body mass index on cardiovascular traits: comparison of novel Mendelian randomization approaches"
    ]
  },
  {
    "objectID": "mr/leyden_2025_40375348.html#conclusions-and-recommendations",
    "href": "mr/leyden_2025_40375348.html#conclusions-and-recommendations",
    "title": "Distinct pathway-based effects of blood pressure and body mass index on cardiovascular traits: comparison of novel Mendelian randomization approaches",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThis research validates the use of partitioning complex trait instruments into biologically meaningful subgroups to gain clearer etiological insights.\n\nEnhanced Mechanistic Detail: By distinguishing between genetic variants acting on the renal system versus the vasculature for BP, the study provides targets for both CAD and Stroke prevention that were not possible with an aggregated BP instrument.\nTherapeutic Prioritization: The consistent finding across both partitioning methods for BMI—that appetite-regulating pathways have a disproportionate effect on T2DM risk—strongly supports therapeutic targeting of central regulation for T2DM prevention in at-risk individuals.\nFuture Application: The authors propose that the pathway-partitioned approach, being data-driven and leveraging established Mendelian disease classifications, offers a valuable, mechanistic complement to tissue-based MR for dissecting the genetic architecture of other complex diseases.",
    "crumbs": [
      "MR",
      "Papers",
      "Distinct pathway-based effects of blood pressure and body mass index on cardiovascular traits: comparison of novel Mendelian randomization approaches"
    ]
  },
  {
    "objectID": "mr/index.html",
    "href": "mr/index.html",
    "title": "Mendelian randomisation",
    "section": "",
    "text": "Purpose: This paper reviews critical methodological challenges in Mendelian Randomization (MR) when applied to complex exposures derived from multi-omics data.\nKey Challenge: Pleiotropy: The authors advocate for using a biologically motivated strategy for selecting genetic instrumental variables (IVs) to avoid pleiotropy, such as explicitly excluding confounding genetic regions (e.g., the IL6R locus when using IL-6 levels).\nData Quality: The review highlights the importance of the measurement technique for the exposure (e.g., avoiding low-resolution 16S sequencing for microbiome or techniques that cannot separate lipid isomers) as data quality issues can invalidate MR assumptions and introduce pleiotropy.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\n\n\nObjective: This study used a two-sample Mendelian randomization (MR) and colocalization approach to identify genes whose tissue-specific expression is causally associated with the risk of glioma and its subtypes (glioblastoma and non-GBM gliomas).\nKey Findings: MR analysis provided evidence for a causal link between the expression of 12 genes and glioma risk, including three novel genes: RETREG2/FAM134A, FAM178B, and MVB12B.\nTissue Specificity: The results confirmed that effects are often tissue-dependent, such as the association for MDM4 being specific to brain tissue, emphasizing the necessity of using relevant eQTL data.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\n\n\nObjective: This MR study partitioned BMI-associated genetic variants into two distinct groups based on their association with gene expression in subcutaneous adipose tissue (metabolic) or brain tissue (appetite) to assess their differential effects on cardiometabolic risk.\nKey Finding: The brain-mediated genetic pathway (appetite regulation) for higher BMI was found to be significantly more strongly associated with increased risk of Type 2 Diabetes Mellitus (T2DM) compared to the adipose-mediated pathway.\nShared Effects: Both tissue-specific pathways showed similar detrimental effects on blood lipids (LDL-C, triglycerides) and blood pressure, suggesting these are shared downstream consequences of increased BMI regardless of the initiating genetic mechanism.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\n\n\nCore Method: The study developed a multi-step analysis pipeline integrating eQTL-wide association, two-sample Mendelian randomization (MR), and multiple-trait colocalization to investigate the causal pathway from genetic variants to cardiovascular traits.\nKey Findings: MR analysis provided evidence for tissue-specific genetic effects on traits like BMI (ADCY3) and cholesterol (FADS1 and SORT1 in liver tissue), demonstrating that the biological context of gene expression is critical.\nMechanism Elucidation: Colocalization analyses suggested that DNA methylation may also play a role alongside gene expression, particularly at the FADS1/TMEM258 locus, and the pipeline identified a total of 233 candidate association signals for future functional studies.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\n\n\nCore Argument: The paper analyzes the interpretation difficulties of Instrumental Variable (IV) estimates that rely on the monotonicity assumption, particularly when the instrument is non-causal (e.g., in many Mendelian Randomization studies).\nEffect Identified: IV methods typically identify the Local Average Treatment Effect (LATE), which applies only to the subgroup of compliers—those whose treatment status is modified by the instrument.\nPolicy Implications: The LATE is often not ideal for informing clinical or policy decision-making because the complier subgroup is often ill-defined or not the target of the intervention, requiring researchers to be cautious about generalization.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\n\n\nObjective: This study used tissue-partitioned Mendelian randomization (MR) to separate the genetic effects of BMI into brain-mediated (appetite) and adipose-mediated (metabolic) pathways to assess their independent causal impact on seven site-specific cancer risks.\nKey Finding (Lung Cancer): The brain-mediated BMI variants were identified as the predominant causal driver of lung cancer risk (OR: 1.17), an effect which was shown to be highly correlated with an increase in cigarettes per day, suggesting a shared genetic mechanism acting via addictive behaviors.\nKey Finding (Colorectal Cancer): A non-significant trend suggested that the adipose-mediated BMI variants were more strongly associated with colorectal cancer risk, supporting the role of metabolic consequences of adiposity in this disease.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\n\n\nCore Problem: This study addresses weak instrument bias in Mendelian Randomization (MR), which occurs when genetic variants are only weakly associated with the exposure phenotype.\nBias Direction: When instruments are weak, the MR causal estimate is biased toward the confounded observational association (i.e., the same bias that conventional observational studies suffer from).\nGuideline: To minimize this bias, the paper recommends that genetic instruments used in MR studies should have a collective F-statistic greater than 10 in the exposure sample.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\n\n\nCore Problem: This study demonstrates that using GWAS summary statistics that have been adjusted for heritable covariables (e.g., adjusting BMI for height) can introduce bias into Two-Sample Mendelian Randomization (2SMR) estimates.\nMechanism: Adjusting the exposure GWAS for a heritable covariable makes the resulting MR estimate prone to bias toward the null (zero), as the genetic variant is no longer a valid instrument for the total effect of the unadjusted exposure.\nRecommendation: Researchers should prioritize using unadjusted GWAS summary statistics to estimate the total causal effect of the exposure and must be transparent about any adjustments made to the summary data used.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\n\n\nCore Concern: The paper addresses the decline in quality of Mendelian Randomization (MR) research due to an overwhelming number of low-value Two-Sample MR (2SMR) papers and the premature, unvalidated application of highly complex MR methods.\nRecommendations for Editors: Editors are advised to set a higher bar, specifically to reject papers that report only standard 2SMR findings without additional supporting evidence, methodological novelty, or validation.\nReviewer Focus: Reviewers must enforce rigorous quality control, demanding comprehensive sensitivity analyses (e.g., MR-Egger, Steiger filtering) and ensuring that findings, especially those from complex non-linear MR, are biologically plausible and properly validated.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\n\n\nObjective: This study employed drug target Mendelian randomization (MR) to emulate the effects of five different classes of lipid-modifying therapies and characterize their resulting causal impact on 229 circulating metabolic traits.\nKey Findings: The analysis successfully generated distinct metabolomic signatures for each drug class, confirming known effects (e.g., Statins reducing LDL-C) while providing novel insights into their differential impact on lipoprotein subclasses; for instance, PCSK9 inhibitors showed limited effect on VLDL/triglycerides compared to Statins.\nImplication: The findings validate drug target MR as a valuable tool for predicting the pleiotropic effects of therapies on the metabolome, which can inform drug development, safety assessment, and personalized clinical choice.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\n\n\nObjective: This commentary discusses the increasing relevance and power of instrumental inequalities (IIs)—mathematical constraints derived from the core assumptions of Instrumental Variables (IV)—as a tool for detecting bias in Mendelian Randomization (MR) studies using large-scale Biobank data.\nInstrumental Inequalities: IIs are a set of conditions that must be satisfied if the IV assumptions hold true. If the data violates these inequalities, it proves the instruments are invalid.\nRole in Mega-Biobanks: The authors argue that the large sample sizes of modern Biobanks provide the necessary statistical power to accurately detect subtle violations of the inequalities, making this tool highly effective for falsifying the validity of genetic instruments.\nRecommendation: IIs should be used as a complementary, stringent test alongside standard MR sensitivity analyses to enhance the statistical rigor of causal inference.\n\n\n\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\n\n\nCore Purpose: This review advocates for the systematic use of falsification strategies and statistical tools to assess the plausibility of the unverifiable assumptions underlying Instrumental Variable (IV) analyses, especially in Mendelian Randomization (MR).\nMethods: While the Relevance assumption is statistically testable (e.g., F-statistic), the Exclusion Restriction and Independence assumptions can be tested for violation using methods like Negative Control Outcomes and sensitivity analyses (e.g., MR-Egger regression).\nRecommendation: The paper stresses that epidemiologists should move beyond justifying IV assumptions solely with subject matter knowledge and routinely employ formal statistical checks to improve the robustness and transparency of causal inference.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\n\n\nCore Problem: This comment highlights the methodological difficulties of using Mendelian Randomization (MR) to establish causal links between dietary exposures and health outcomes due to the inherent nature of diet.\nKey Challenges: Challenges include difficulty quantifying intake and reliance on non-representative midlife measurements, the time-varying nature of diet, and its compositional constraints (change in one nutrient affects others).\nMR Limitation: The strong correlation between nutrition and other lifestyle/environmental factors (e.g., physical activity) risks violating core MR assumptions (pleiotropy and independence), potentially leading to biased or misleading causal estimates .\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\n\n\nObjective: This study introduced a novel Contextual Mendelian Randomization (cMR) framework that leverages single-cell RNA sequencing (scRNA-seq) to identify genetic effects on gene expression that are dependent on the cell type and the cell’s state (e.g., inflammatory vs. homeostatic) in the human brain.\nKey Findings: The cMR approach identified hundreds of cell state-dependent eQTLs, and, when applied to neurological diseases, it prioritized novel causal genes missed by bulk MR; for example, linking the expression of LRRC18 and RHOBTB3 in microglia to Alzheimer’s Disease (AD) risk.\nSignificance: By resolving genetic signals to specific cellular contexts, cMR offers highly refined and biologically precise causal estimates, reducing pleiotropy and identifying superior, cell state-specific drug targets for complex brain phenotypes.\n\n\n\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\n\n\nObjective: This Mendelian randomization (MR) study aimed to determine if grouping BMI-associated genetic variants based on differential tissue expression profiles (from GTEx data) could identify distinct causal effects on Type 2 Diabetes Mellitus (T2DM) and Coronary Artery Disease (CAD).\nKey Finding: The study identified 17 tissue-grouped gene sets; however, all sets showed a similar association with increased risks of T2DM and CAD, with effect estimates comparable to those from randomly sampled genetic variants.\nConclusion: The novel approach of clustering BMI genetic instruments by tissue expression did not provide additional insight into the role of specific tissues in the genetic risk for these cardiometabolic diseases.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\n\n\nCore Problem: This study addresses the difficulty of interpreting Mendelian Randomization (MR) results when the exposure (e.g., smoking) is time-varying and changes over an individual’s life course.\nInterpretation: Standard MR estimates for a time-varying exposure should be interpreted as the causal effect of an intervention on the long-term average or cumulative lifetime exposure, not the effect of intervening at a specific moment (e.g., quitting at age 40).\nRecommendation: Researchers must be explicit about this interpretation, as the estimate may not align with the causal effect relevant to clinical or public health policy interventions, necessitating the development of MR methods based on g-methods for time-varying treatments.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\n\n\nObjective: This study introduced a novel pathway-partitioned Mendelian randomization (MR) approach (based on proximity to Mendelian disease genes) and compared its findings to the existing tissue-partitioned MR approach for Blood Pressure (BP) and Body Mass Index (BMI) effects on cardiovascular traits.\nBP Findings: The pathway-partitioned MR showed that renal-system related IVs were significantly associated with Coronary Artery Disease (CAD), while vascular-related IVs were significantly associated with Stroke, demonstrating distinct causal pathways.\nBMI Findings: Both the pathway-partitioned (Mental Health vs. Metabolic) and tissue-partitioned (Brain vs. Adipose) methods for BMI consistently found that genetic variants related to central/appetite regulation had a stronger causal effect on the risk of Type 2 Diabetes Mellitus (T2DM).\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\n\n\nObjective: This study introduced a novel single-cell transcriptome-wide Mendelian randomization (scTWMR) framework to leverage cell-type-specific gene expression data from atherosclerotic plaques and identify causal genes for Atherosclerotic Cardiovascular Disease (ASCVD) risk.\nKey Findings: The approach prioritized 23 causal genes by resolving signals to specific cell types, finding the strongest evidence in immune cells; for example, genetically predicted increased expression of HSH2D in macrophages was causally associated with increased ASCVD risk.\nSignificance: scTWMR provides a level of resolution superior to traditional bulk-tissue MR, enabling the identification of cell-specific drug targets and offering a scalable strategy for uncovering mechanistic details of complex diseases.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\n\n\nObjective: This paper introduced a novel, robust method to conduct Mendelian Randomization (MR) stratified analyses while effectively avoiding the introduction of collider bias, a common pitfall when stratifying by a variable influenced by both the exposure and the outcome.\nMethodology (Residual Collider): The solution is to construct a residual collider (\\(\\tilde{C}\\)) by regressing the original collider (\\(C\\)) on the genetic instrument (\\(G\\)). Stratifying the MR analysis by quantiles of this residual (\\(\\tilde{C}\\)) preserves the independence of the instrument and the stratifying variable, thus ensuring the validity of the causal estimate.\nConclusion: The study validates that stratification by the residual collider provides unbiased causal estimates and allows for the valid investigation of causal effect heterogeneity across population subgroups defined by the collider.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\n\n\nCore Recommendation: For Mendelian Randomisation (MR), a biologically motivated strategy for selecting genetic variants is preferred over a genome-wide approach to increase the plausibility of the No Pleiotropy instrumental variable assumption.\nSelection Methods: Plausible variant selection includes Cis-MR (variants in the coding region of the exposure gene) and using instruments associated with a specific biomarker rather than a broad behavioral proxy.\nValidation: The study advocates for rigorous sensitivity checks such as Multivariable MR (MVMR), positive controls, and negative controls to validate the instruments and dissect causal pathways from confounding or pleiotropy.\n\n\n\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\n\n\nCore Problem: This research addresses how to determine the causal direction between two highly correlated traits (\\(X \\rightarrow Y\\) vs. \\(Y \\rightarrow X\\)) using easily accessible GWAS summary data.\nKey Method (Steiger Filtering): The primary method is Steiger filtering, which tests if the genetic instrument explains more variance in the intended exposure than in the intended outcome. If the reverse is true, it suggests an incorrect causal direction or unmodeled pleiotropy.\nImpact: The method has become a routine, mandatory step in Mendelian Randomization studies to validate that the genetic variants selected are genuine instruments for the exposure and not proxies for the outcome or unmodeled confounders, thus preventing reverse causation bias.\n\n\n\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\n\n\nCore Goal: This paper provides updated guidelines for Mendelian Randomization (MR) investigations, stressing the need for a rigorous, multi-method approach to inferring causality between an exposure and an outcome using genetic variants.\nValidity Checks: The guidelines mandate systematic testing of the core IV assumptions, particularly the Exclusion Restriction (no pleiotropy), using multiple sensitivity analyses such as MR-Egger, Weighted Median/Mode, and Steiger Filtering.\nBest Practices: Recommended practices for Two-Sample MR include proper variant selection (clumping), careful data harmonization, and comprehensive reporting of all sensitivity analysis results, including visual plots, to ensure robustness and transparency.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\n\n\nObjective: This study employed a novel brain multi-omic Mendelian randomization (MR) approach, integrating brain-specific genetic, transcriptomic (eQTLs), and proteomic (pQTLs) data to identify genes and proteins causally associated with glioma risk.\nKey Findings: The analysis identified 25 causal genes and, more specifically, 13 causal proteins. The strongest evidence pointed to two promising drug targets for glioblastoma (GBM): genetically predicted lower levels of the protein FAM178B and higher levels of the protein MDM4 both increased risk.\nSignificance: The work validates the use of brain-specific proteomic data in MR to enhance the prioritization of highly relevant molecular drug targets for neurological cancers.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "MR",
      "Papers"
    ]
  },
  {
    "objectID": "mr/haglund_2025_39794547.html",
    "href": "mr/haglund_2025_39794547.html",
    "title": "Cell state-dependent allelic effects and contextual Mendelian randomization analysis for human brain phenotypes",
    "section": "",
    "text": "PubMed: 39794547 DOI: 10.1038/s41588-024-02050-9 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "MR",
      "Papers",
      "Cell state-dependent allelic effects and contextual Mendelian randomization analysis for human brain phenotypes"
    ]
  },
  {
    "objectID": "mr/haglund_2025_39794547.html#key-findings-cell-state-dependent-gene-regulation-in-the-brain",
    "href": "mr/haglund_2025_39794547.html#key-findings-cell-state-dependent-gene-regulation-in-the-brain",
    "title": "Cell state-dependent allelic effects and contextual Mendelian randomization analysis for human brain phenotypes",
    "section": "Key Findings: Cell State-Dependent Gene Regulation in the Brain",
    "text": "Key Findings: Cell State-Dependent Gene Regulation in the Brain\nThis study introduced and applied a novel Contextual Mendelian Randomization (cMR) framework, which uses single-cell RNA sequencing (scRNA-seq) data to resolve genetic effects on gene expression that are dependent on the cell type and the cell’s state (e.g., activated vs. quiescent). This framework was used to better prioritize causal genes for major human brain phenotypes.\n\nCell State-Dependent Effects Discovered: The study identified hundreds of cell state-dependent expression quantitative trait loci (eQTLs), meaning the genetic influence on a gene’s expression changes significantly depending on the cell’s activation status. This effect was observed, for example, in microglia when comparing their inflammatory state to their homeostatic state.\nContextual MR Prioritizes Novel Causal Genes: Applying the cMR framework to neurological GWAS traits (e.g., Alzheimer’s Disease (AD), Schizophrenia (SCZ)) provided enhanced resolution:\n\nMicroglia and AD: The cMR approach identified a causal role for the genetically regulated expression of LRRC18 and RHOBTB3 in microglia, particularly when the cells are in a disease-relevant state. Traditional bulk-tissue MR failed to detect these associations.\nExcitatory Neurons and SCZ: Causal effects were linked to specific genes in excitatory neurons, such as NKAIN3 and CLCN3, whose expression influences SCZ risk.\n\nAddressing Pleiotropy: By isolating the genetic signal to a specific cell state/type, the cMR framework significantly reduced the likelihood of horizontal pleiotropy (where a genetic variant influences the outcome through a pathway unrelated to the intended exposure). This led to more reliable and biologically precise causal estimates.\nDrug Target Identification: The prioritized genes, such as LRRC18, offer more refined and cell state-specific targets for drug development for complex neurological disorders.",
    "crumbs": [
      "MR",
      "Papers",
      "Cell state-dependent allelic effects and contextual Mendelian randomization analysis for human brain phenotypes"
    ]
  },
  {
    "objectID": "mr/haglund_2025_39794547.html#study-design-and-methods",
    "href": "mr/haglund_2025_39794547.html#study-design-and-methods",
    "title": "Cell state-dependent allelic effects and contextual Mendelian randomization analysis for human brain phenotypes",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\n\nStudy Design\nThis was a two-sample Contextual Mendelian Randomization (cMR) study. It combined publicly available large-scale GWAS summary statistics for brain phenotypes with highly granular, cell state-specific eQTL data derived from human post-mortem brain tissue scRNA-seq.\n\n\nData Sources and Instrumental Variables\n\nOutcome GWAS Data (Brain Phenotypes): Summary statistics for complex neurological and psychiatric traits were utilized, including Alzheimer’s Disease (AD), Parkinson’s Disease (PD), Multiple Sclerosis (MS), Schizophrenia (SCZ), and height.\nExposure Data (Cell State-Dependent eQTLs):\n\nSingle-Cell RNA-seq: ScRNA-seq data from human brain tissue (focusing on the frontal cortex) was analyzed to identify gene expression in various cell types (e.g., neurons, astrocytes, microglia).\nCell State Definition: The study computationally defined and analyzed distinct cell states (e.g., homeostatic vs. inflammatory microglia) within each cell type to identify eQTLs that varied in effect size or direction based on that state.\n\nInstrument Selection: Genetic variants (SNPs) acting as eQTLs were selected. The cMR framework partitioned these instruments based on their strength within specific cell types and states.\n\n\n\nStatistical Analysis\n\nContextual Mendelian Randomization (cMR): The cMR method was based on the standard Inverse-Variance Weighted (IVW) MR approach but utilized IVs whose effects were estimated only from the gene expression of the relevant cell type/state (e.g., inflammatory microglia).\nSensitivity Analyses: Standard robust MR methods, including MR-Egger, were employed to test for violations of the MR assumptions.\nColocalization Analysis: Bayesian colocalization was used to assess whether the same genetic variant influenced both the cell state-dependent gene expression and the disease outcome, thereby validating the genetic mechanism.",
    "crumbs": [
      "MR",
      "Papers",
      "Cell state-dependent allelic effects and contextual Mendelian randomization analysis for human brain phenotypes"
    ]
  },
  {
    "objectID": "mr/haglund_2025_39794547.html#conclusions-and-recommendations",
    "href": "mr/haglund_2025_39794547.html#conclusions-and-recommendations",
    "title": "Cell state-dependent allelic effects and contextual Mendelian randomization analysis for human brain phenotypes",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study successfully demonstrates that treating gene regulation as cell state-dependent provides substantially more refined and biologically relevant causal insights into brain phenotypes than previous methods.\n\nNew Era for MR: The cMR framework marks a significant methodological advance, moving genetic epidemiology toward the resolution of single-cell mechanisms. It allows researchers to pinpoint the precise cell type and activation status where a genetic risk factor exerts its influence.\nTargeted Drug Development: By prioritizing genes like LRRC18 and RHOBTB3 specifically in microglia, the cMR approach provides highly specific therapeutic targets, which is crucial for brain disorders where off-target effects are a major concern.\nFuture Work: The authors advocate for the broad application of the cMR framework to all complex traits where cell heterogeneity is known to be an important factor. They recommend the continued generation of large-scale, high-quality scRNA-seq and single-nucleus data to further refine these cell state-dependent genetic maps.",
    "crumbs": [
      "MR",
      "Papers",
      "Cell state-dependent allelic effects and contextual Mendelian randomization analysis for human brain phenotypes"
    ]
  },
  {
    "objectID": "mr/labrecque_2018_30148040.html",
    "href": "mr/labrecque_2018_30148040.html",
    "title": "Understanding the Assumptions Underlying Instrumental Variable Analyses: a Brief Review of Falsification Strategies and Related Tools",
    "section": "",
    "text": "PubMed: 30148040 DOI: 10.1007/s40471-018-0152-1 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "MR",
      "Papers",
      "Understanding the Assumptions Underlying Instrumental Variable Analyses: a Brief Review of Falsification Strategies and Related Tools"
    ]
  },
  {
    "objectID": "mr/labrecque_2018_30148040.html#purpose-and-scope-of-review",
    "href": "mr/labrecque_2018_30148040.html#purpose-and-scope-of-review",
    "title": "Understanding the Assumptions Underlying Instrumental Variable Analyses: a Brief Review of Falsification Strategies and Related Tools",
    "section": "Purpose and Scope of Review",
    "text": "Purpose and Scope of Review\nInstrumental Variable (IV) methods, including their application in genetic epidemiology (Mendelian Randomization, MR), rely on three core assumptions to yield valid causal estimates. This review addresses the limitation that discussions about these assumptions often rely too heavily on subject matter knowledge. It systematically outlines and advocates for the use of various falsification strategies and related statistical tools to complement subject matter knowledge, test for assumption violations, and quantify potential bias.",
    "crumbs": [
      "MR",
      "Papers",
      "Understanding the Assumptions Underlying Instrumental Variable Analyses: a Brief Review of Falsification Strategies and Related Tools"
    ]
  },
  {
    "objectID": "mr/labrecque_2018_30148040.html#assumptions-of-instrumental-variable-analysis",
    "href": "mr/labrecque_2018_30148040.html#assumptions-of-instrumental-variable-analysis",
    "title": "Understanding the Assumptions Underlying Instrumental Variable Analyses: a Brief Review of Falsification Strategies and Related Tools",
    "section": "Assumptions of Instrumental Variable Analysis",
    "text": "Assumptions of Instrumental Variable Analysis\nThe validity of IV analysis rests on three critical conditions for the instrument (\\(Z\\)), the exposure (\\(X\\)), and the outcome (\\(Y\\)):\n\nRelevance: The instrument (\\(Z\\)) must be associated with the exposure (\\(X\\)). (Statistically verifiable).\nExclusion Restriction: The instrument (\\(Z\\)) must only affect the outcome (\\(Y\\)) through the exposure (\\(X\\)). (Unverifiable).\nIndependence: The instrument (\\(Z\\)) must not be associated with any unmeasured confounders of the exposure-outcome relationship. (Unverifiable).",
    "crumbs": [
      "MR",
      "Papers",
      "Understanding the Assumptions Underlying Instrumental Variable Analyses: a Brief Review of Falsification Strategies and Related Tools"
    ]
  },
  {
    "objectID": "mr/labrecque_2018_30148040.html#falsification-strategies-and-statistical-tools",
    "href": "mr/labrecque_2018_30148040.html#falsification-strategies-and-statistical-tools",
    "title": "Understanding the Assumptions Underlying Instrumental Variable Analyses: a Brief Review of Falsification Strategies and Related Tools",
    "section": "Falsification Strategies and Statistical Tools",
    "text": "Falsification Strategies and Statistical Tools\nThe core of the review focuses on synthesizing and promoting the use of tools that assess the plausibility of the unverifiable assumptions (Exclusion Restriction and Independence).\n\nAssessing Relevance\nThe relevance assumption can be tested statistically, most commonly using the F-statistic. A low F-statistic (typically \\(&lt;10\\)) signals a weak instrument, which leads to bias toward the confounded observational association, and therefore violates the requirement for robust IV inference.\n\n\nAssessing Exclusion Restriction and Independence\nWhile these assumptions cannot be proven true, they can often be refuted or their potential bias quantified: * Negative Control Outcomes: This involves testing the IV estimate on an outcome that is theoretically known not to be affected by the exposure. A statistically significant IV estimate on a negative control outcome strongly suggests a violation of the exclusion restriction (e.g., horizontal pleiotropy in MR). * Sensitivity Analyses: Methods designed to detect violations of the assumptions, such as: * Egger Regression Intercept: Used in two-sample MR, the intercept can test for directional pleiotropy, a major violation of the exclusion restriction. * Multiple Instrument Methods: Comparing results from different sets of instruments or weighted analyses can help identify and mitigate potential bias stemming from a single instrument violating an assumption. * Known-Unknown Comparisons: Comparing the IV-derived causal estimate for an outcome where the true causal effect is already known (e.g., from a randomized trial) can validate the method in a specific context. * Bias Estimation Tools: Various techniques exist to estimate the magnitude or direction of bias that would be expected if the assumptions were violated by a certain amount.",
    "crumbs": [
      "MR",
      "Papers",
      "Understanding the Assumptions Underlying Instrumental Variable Analyses: a Brief Review of Falsification Strategies and Related Tools"
    ]
  },
  {
    "objectID": "mr/labrecque_2018_30148040.html#conclusions-and-recommendations",
    "href": "mr/labrecque_2018_30148040.html#conclusions-and-recommendations",
    "title": "Understanding the Assumptions Underlying Instrumental Variable Analyses: a Brief Review of Falsification Strategies and Related Tools",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe authors conclude that although the core IV assumptions remain unverifiable, the widespread availability of statistical tools and falsification strategies means that researchers should no longer rely solely on subject matter knowledge to justify their IV results. They strongly recommend that epidemiologists systematically apply these formal statistical checks and sensitivity analyses to rigorously test for potential assumption violations and thereby increase the credibility and transparency of causal effects estimated using IV methods.",
    "crumbs": [
      "MR",
      "Papers",
      "Understanding the Assumptions Underlying Instrumental Variable Analyses: a Brief Review of Falsification Strategies and Related Tools"
    ]
  },
  {
    "objectID": "mr/richardson_2022_35213538.html",
    "href": "mr/richardson_2022_35213538.html",
    "title": "Characterising metabolomic signatures of lipid-modifying therapies through drug target Mendelian randomisation",
    "section": "",
    "text": "PubMed: 35213538 DOI: 10.1371/journal.pbio.3001547 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "MR",
      "Papers",
      "Characterising metabolomic signatures of lipid-modifying therapies through drug target Mendelian randomisation"
    ]
  },
  {
    "objectID": "mr/richardson_2022_35213538.html#key-findings-metabolomic-signatures-of-lipid-lowering-drugs",
    "href": "mr/richardson_2022_35213538.html#key-findings-metabolomic-signatures-of-lipid-lowering-drugs",
    "title": "Characterising metabolomic signatures of lipid-modifying therapies through drug target Mendelian randomisation",
    "section": "Key Findings: Metabolomic Signatures of Lipid-Lowering Drugs",
    "text": "Key Findings: Metabolomic Signatures of Lipid-Lowering Drugs\nThis study utilized drug target Mendelian randomization (MR) to emulate the effects of five different classes of lipid-modifying therapies and characterize their resulting causal impact on the circulating metabolome (229 metabolic traits).\n\nDrug Class Signatures Identified: The study successfully characterized the distinct causal metabolomic signatures of the five drug classes, leveraging genetic variants that proxy the function of the drug target gene:\n\nHMGCR inhibitors (Statins): Genetically proxied HMGCR inhibition caused reductions in LDL-C and triglycerides, and importantly, showed significant causal effects on numerous intermediate density lipoprotein (IDL) and very low-density lipoprotein (VLDL) traits.\nPCSK9 inhibitors (PCSK9i): Genetically proxied PCSK9 inhibition primarily reduced LDL-C and total cholesterol but had no significant effect on triglycerides or VLDL traits, highlighting a key distinction from statins.\nNPC1L1 inhibitors (Ezetimibe): Similar to PCSK9i, NPC1L1 inhibition predominantly reduced LDL-C and showed limited effects on triglycerides or VLDL.\nLPL activators (Fibrates/Gemfibrozil): Genetically proxied LPL activation showed the largest causal effect on reducing triglycerides and VLDL traits.\nCETP inhibitors: Genetically proxied CETP inhibition primarily increased HDL-C and total cholesterol.\n\nCausal Impact on Metabolism: The analysis confirmed the established effects of these drug classes on primary lipid fractions (e.g., LDL-C, triglycerides) but provided novel insights into their distinct effects on various metabolite sub-fractions, particularly within the lipoprotein subclasses and amino acid metabolism (e.g., changes in the ratio of polyunsaturated fatty acids to total fatty acids by LPL activation).\nValidation of MR for Drug Targets: The study validated the use of MR to predict the pleiotropic effects of drug targets on a high-dimensional panel of metabolic traits, making it a valuable tool for drug development and safety assessment.",
    "crumbs": [
      "MR",
      "Papers",
      "Characterising metabolomic signatures of lipid-modifying therapies through drug target Mendelian randomisation"
    ]
  },
  {
    "objectID": "mr/richardson_2022_35213538.html#study-design-and-methods",
    "href": "mr/richardson_2022_35213538.html#study-design-and-methods",
    "title": "Characterising metabolomic signatures of lipid-modifying therapies through drug target Mendelian randomisation",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\n\nStudy Design\nThis was a two-sample drug target Mendelian randomization (MR) study. The design used genetic variants located near or within the genes encoding the drug targets as instrumental variables (IVs) to proxy the lifelong effect of the drug on the metabolome.\n\n\nExposure and Outcome Data\n\nExposures (Drug Target Proxies): Genetic variants (SNPs) associated with changes in the concentration of the primary lipid fraction targeted by the drug were used as instrumental variables, effectively mimicking the biological effect of the drug:\n\nHMGCR: Proxied Statins (effect on LDL-C).\nPCSK9: Proxied PCSK9i (effect on LDL-C).\nNPC1L1: Proxied Ezetimibe (effect on LDL-C).\nLPL: Proxied Fibrates/Gemfibrozil (effect on Triglycerides).\nCETP: Proxied CETP inhibitors (effect on HDL-C).\nExposure GWAS data for the primary lipids were sourced from large-scale consortia.\n\nOutcomes (Metabolomic Traits): Summary statistics were used for 229 circulating metabolic traits measured using the high-throughput Nuclear Magnetic Resonance (NMR) spectroscopy platform, predominantly sourced from the UK Biobank (up to \\(n=115,076\\)). These traits included lipoprotein subclasses, amino acids, fatty acids, and glycolysis-related metabolites.\n\n\n\nStatistical Analysis\n\nMR Methods: The Inverse-Variance Weighted (IVW) method was used as the primary MR approach.\nSensitivity Analyses: Robust sensitivity methods were performed to ensure the validity of the causal estimates by addressing potential pleiotropy:\n\nMR-Egger regression.\nWeighted Median and Weighted Mode estimators.\nMultivariable MR (MVMR): Used to confirm that the observed effects were not confounded by the genetic effect on other lipid fractions (e.g., confirming the effect of \\(HMGCR\\) on metabolites was independent of its effect on \\(HDL-C\\)).\n\nFDR Correction: False Discovery Rate (FDR) correction was applied to adjust for multiple testing across the 229 metabolic outcomes for each drug target.",
    "crumbs": [
      "MR",
      "Papers",
      "Characterising metabolomic signatures of lipid-modifying therapies through drug target Mendelian randomisation"
    ]
  },
  {
    "objectID": "mr/richardson_2022_35213538.html#conclusions-and-recommendations",
    "href": "mr/richardson_2022_35213538.html#conclusions-and-recommendations",
    "title": "Characterising metabolomic signatures of lipid-modifying therapies through drug target Mendelian randomisation",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study concludes that drug target MR is a powerful and efficient method for predicting the comprehensive metabolomic signature and potential pleiotropic effects of new and existing therapies.\n\nClinical Utility: The distinct metabolic profiles characterized (e.g., the difference between statin and PCSK9i effects on VLDL/triglycerides) can inform the rational choice of therapy for patients with specific metabolic risk profiles.\nDrug Development: This approach allows for the early assessment of the unintended (pleiotropic) effects of drug targets on a wide range of biological processes, potentially accelerating the identification of both beneficial and adverse effects before large-scale clinical trials.\nFuture Work: The authors recommend applying this drug target MR approach to other classes of cardiovascular drugs and integrating even larger metabolomic datasets to increase the statistical power for detecting smaller, more subtle causal effects.",
    "crumbs": [
      "MR",
      "Papers",
      "Characterising metabolomic signatures of lipid-modifying therapies through drug target Mendelian randomisation"
    ]
  },
  {
    "objectID": "mr/hartwig_2021_33619569.html",
    "href": "mr/hartwig_2021_33619569.html",
    "title": "Bias in two-sample Mendelian randomization when using heritable covariable-adjusted summary associations",
    "section": "",
    "text": "PubMed: 33619569 DOI: 10.1093/ije/dyaa266 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "MR",
      "Papers",
      "Bias in two-sample Mendelian randomization when using heritable covariable-adjusted summary associations"
    ]
  },
  {
    "objectID": "mr/hartwig_2021_33619569.html#key-finding-bias-from-adjusted-gwas-summary-data",
    "href": "mr/hartwig_2021_33619569.html#key-finding-bias-from-adjusted-gwas-summary-data",
    "title": "Bias in two-sample Mendelian randomization when using heritable covariable-adjusted summary associations",
    "section": "Key Finding: Bias from Adjusted GWAS Summary Data",
    "text": "Key Finding: Bias from Adjusted GWAS Summary Data\nThis paper investigates the potential bias introduced into Two-Sample Mendelian Randomization (2SMR) estimates when the summary association results from Genome-Wide Association Studies (GWASs)—used for either the exposure or the outcome—have been adjusted for heritable covariables (e.g., adjusting a Body Mass Index GWAS for height).\nThe key finding is that performing 2SMR using summary statistics adjusted for a heritable covariable can introduce bias, and the direction and magnitude of this bias depend crucially on whether the adjustment was applied to the exposure GWAS, the outcome GWAS, or both, and the underlying causal structure.",
    "crumbs": [
      "MR",
      "Papers",
      "Bias in two-sample Mendelian randomization when using heritable covariable-adjusted summary associations"
    ]
  },
  {
    "objectID": "mr/hartwig_2021_33619569.html#mechanism-of-bias",
    "href": "mr/hartwig_2021_33619569.html#mechanism-of-bias",
    "title": "Bias in two-sample Mendelian randomization when using heritable covariable-adjusted summary associations",
    "section": "Mechanism of Bias",
    "text": "Mechanism of Bias\nStandard MR relies on the assumption that the genetic variant’s effect on the exposure is unconfounded. When a GWAS adjusts for a heritable covariable, the resulting genetic effect estimate (\\(\\hat{\\Gamma}\\)) is no longer the total effect of the variant on the phenotype, but rather the direct effect net of the covariable.\nThe authors show via simulations and theoretical exploration that:\n\nAdjustment in Exposure GWAS Only: If the genetic association with the exposure (\\(\\hat{\\Gamma}_{XZ}\\)) is adjusted for a heritable covariable, the MR estimate is generally biased toward the null (zero). This occurs because the IV estimate now targets a highly conditional and often complex causal effect parameter, which may not be the total effect intended by the MR analysis.\nAdjustment in Outcome GWAS Only: If the genetic association with the outcome (\\(\\hat{\\Gamma}_{YZ}\\)) is adjusted, the resulting 2SMR estimate will be estimating the effect of the exposure on the residual outcome (the part of the outcome not explained by the covariable). This estimate may also be biased relative to the total causal effect of the exposure on the unadjusted outcome.",
    "crumbs": [
      "MR",
      "Papers",
      "Bias in two-sample Mendelian randomization when using heritable covariable-adjusted summary associations"
    ]
  },
  {
    "objectID": "mr/hartwig_2021_33619569.html#recommendations-and-implications-for-mr-practice",
    "href": "mr/hartwig_2021_33619569.html#recommendations-and-implications-for-mr-practice",
    "title": "Bias in two-sample Mendelian randomization when using heritable covariable-adjusted summary associations",
    "section": "Recommendations and Implications for MR Practice",
    "text": "Recommendations and Implications for MR Practice\nThe paper advises extreme caution when using GWAS summary data that has been adjusted for heritable traits:\n\nPrioritize Unadjusted GWAS: The most straightforward approach is to prioritize unadjusted GWAS summary statistics for both the exposure and the outcome when aiming to estimate the total causal effect of the exposure.\nTargeting Direct Effects: If the researcher specifically wants to estimate a direct causal effect (e.g., the effect of BMI on T2D, independent of cholesterol), then adjustments in both the exposure and outcome GWAS (or using multivariable MR) may be appropriate, but the interpretation becomes highly specific and complex.\nTransparency: Researchers must be fully transparent about whether the GWAS data they used was adjusted for heritable covariables, as this deeply affects the interpretation and validity of the final 2SMR causal estimate.",
    "crumbs": [
      "MR",
      "Papers",
      "Bias in two-sample Mendelian randomization when using heritable covariable-adjusted summary associations"
    ]
  },
  {
    "objectID": "mr/hartwig_2021_33619569.html#conclusion",
    "href": "mr/hartwig_2021_33619569.html#conclusion",
    "title": "Bias in two-sample Mendelian randomization when using heritable covariable-adjusted summary associations",
    "section": "Conclusion",
    "text": "Conclusion\nThe study underscores a critical source of potential bias in the widely used 2SMR framework, demonstrating that genetic variants associated with conditional phenotypes are not necessarily valid instruments for estimating the total causal effect of the unadjusted phenotype. Researchers must carefully select GWAS data based on the adjustment status of the summary statistics.",
    "crumbs": [
      "MR",
      "Papers",
      "Bias in two-sample Mendelian randomization when using heritable covariable-adjusted summary associations"
    ]
  },
  {
    "objectID": "mr/leyden_2022_36434155.html",
    "href": "mr/leyden_2022_36434155.html",
    "title": "Disentangling the aetiological pathways between body mass index and site-specific cancer risk using tissue-partitioned Mendelian randomisation",
    "section": "",
    "text": "PubMed: 36434155 DOI: 10.1038/s41416-022-02060-6 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "MR",
      "Papers",
      "Disentangling the aetiological pathways between body mass index and site-specific cancer risk using tissue-partitioned Mendelian randomisation"
    ]
  },
  {
    "objectID": "mr/leyden_2022_36434155.html#key-findings-tissue-specific-bmi-effects-on-cancer",
    "href": "mr/leyden_2022_36434155.html#key-findings-tissue-specific-bmi-effects-on-cancer",
    "title": "Disentangling the aetiological pathways between body mass index and site-specific cancer risk using tissue-partitioned Mendelian randomisation",
    "section": "Key Findings: Tissue-Specific BMI Effects on Cancer",
    "text": "Key Findings: Tissue-Specific BMI Effects on Cancer\nThis study applied tissue-partitioned Mendelian randomization (MR) to separate the genetic effects of BMI mediated by subcutaneous adipose tissue (metabolic) from those mediated by brain tissue (appetite/central regulation) and assessed their differential causal impact on the risk of seven site-specific cancers.\n\nBrain-Mediated BMI Drives Lung Cancer Risk: The most distinct finding was that the brain-tissue-derived BMI variants were the predominant driver of the genetically predicted causal effect of BMI on lung cancer risk (OR: 1.17; 95% CI: 1.01-1.36).\n\nThis effect was strongly supported by a parallel finding: the brain-mediated BMI variants were also robustly associated with an increased number of cigarettes per day (Beta = 0.44; 95% CI: 0.26-0.61). This suggests that the genetic pathway influencing BMI via appetite/central regulation may increase lung cancer risk primarily by promoting addictive behaviors (smoking).\n\nAdipose-Mediated BMI Drives Colorectal Cancer Risk (Marginal Evidence): The adipose-tissue-derived BMI variants showed a stronger, though non-significant at the conventional level, association with colorectal cancer risk (OR: 1.07; 95% CI: 0.99-1.15). This suggests that the metabolic consequences of excess adipose tissue are the more relevant pathway for this cancer type.\nNo Distinct Partitioned Effects on Other Cancers: For the remaining five site-specific cancers investigated (breast, prostate, kidney, ovarian, and endometrial), the causal effects of the brain-mediated and adipose-mediated BMI partitions were statistically indistinguishable. This suggests that either the downstream cancer mechanisms are shared, or the current tissue partitioning is insufficient to separate the relevant pathways for these cancers.",
    "crumbs": [
      "MR",
      "Papers",
      "Disentangling the aetiological pathways between body mass index and site-specific cancer risk using tissue-partitioned Mendelian randomisation"
    ]
  },
  {
    "objectID": "mr/leyden_2022_36434155.html#study-design-and-methods",
    "href": "mr/leyden_2022_36434155.html#study-design-and-methods",
    "title": "Disentangling the aetiological pathways between body mass index and site-specific cancer risk using tissue-partitioned Mendelian randomisation",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\n\nStudy Design\nThis was a two-sample tissue-partitioned Mendelian randomization (MR) study. The methodology leveraged tissue-specific gene expression data to define two distinct sets of genetic instrumental variables (IVs) for BMI, allowing for the comparison of their independent causal effects on cancer risk.\n\n\nData and Genetic Instrument Selection\n\nExposure GWAS (BMI): Summary statistics were used from a large-scale BMI GWAS (GIANT consortium, \\(n \\sim 322,000\\)).\nTissue-Specific Prioritization: The study used a previously developed method to partition BMI IVs based on colocalization with gene expression (eQTLs) in two relevant tissues:\n\nSubcutaneous Adipose Tissue: Used to proxy metabolic/peripheral mechanisms.\nBrain Tissue: Used to proxy appetite/central regulation mechanisms.\n\nInstrument Sets: This resulted in two, largely non-overlapping sets of IVs: the adipose-mediated set and the brain-mediated set.\nOutcome GWAS Data (Cancers): Outcome data were sourced from large international consortia GWAS for seven site-specific cancers: lung, colorectal, breast, prostate, kidney, ovarian, and endometrial cancers. Data for Cigarettes per day (CPD) were also used as an intermediate outcome to test the smoking pathway hypothesis.\n\n\n\nStatistical Analysis\n\nMR Methodology: A Multivariable Mendelian Randomization (MVMR) approach was applied. This method simultaneously models the effects of the brain-mediated and adipose-mediated BMI partitions, which helps to account for the correlation between the two instrumental variable sets, thereby providing the independent causal effect of each pathway.\nPrimary MR Method: The MVMR estimates were derived and the results were reported as Odds Ratios (ORs) per one standard deviation (SD) increase in BMI for each pathway.\nSensitivity Analyses: Standard MR sensitivity analyses (e.g., MR-Egger) were applied to the aggregated BMI instrument set to ensure overall robustness.",
    "crumbs": [
      "MR",
      "Papers",
      "Disentangling the aetiological pathways between body mass index and site-specific cancer risk using tissue-partitioned Mendelian randomisation"
    ]
  },
  {
    "objectID": "mr/leyden_2022_36434155.html#conclusions-and-recommendations",
    "href": "mr/leyden_2022_36434155.html#conclusions-and-recommendations",
    "title": "Disentangling the aetiological pathways between body mass index and site-specific cancer risk using tissue-partitioned Mendelian randomisation",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study successfully demonstrates that the complex genetic effects of BMI can be dissected into biologically meaningful pathways that have distinct, site-specific causal consequences for cancer risk.\n\nSpecific Aetiology for Lung Cancer: The strong evidence linking the brain-mediated/appetite pathway to lung cancer risk, mediated by smoking behavior, provides a specific mechanistic target for prevention. Interventions should focus on the behavioral and neurobiological aspects of appetite and addiction in individuals with high genetic risk for this pathway.\nColorectal Cancer Focus: The trend toward the adipose-mediated pathway driving colorectal cancer risk supports the traditional hypothesis that this cancer type is linked to the metabolic and inflammatory consequences of excess adiposity.\nFuture Directions: The authors suggest that future studies should investigate whether the differential pathways identified for T2DM (previously shown to be brain-mediated) and CAD (shown to be similarly affected by both pathways) could also inform cancer risk mechanisms, further linking cardiometabolic and oncologic outcomes.",
    "crumbs": [
      "MR",
      "Papers",
      "Disentangling the aetiological pathways between body mass index and site-specific cancer risk using tissue-partitioned Mendelian randomisation"
    ]
  },
  {
    "objectID": "mr/taylor_2019_30704512.html",
    "href": "mr/taylor_2019_30704512.html",
    "title": "Prioritizing putative influential genes in cardiovascular disease susceptibility by applying tissue-specific Mendelian randomization",
    "section": "",
    "text": "PubMed: 30704512 DOI: 10.1186/s13073-019-0613-2 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "MR",
      "Papers",
      "Prioritizing putative influential genes in cardiovascular disease susceptibility by applying tissue-specific Mendelian randomization"
    ]
  },
  {
    "objectID": "mr/taylor_2019_30704512.html#key-findings",
    "href": "mr/taylor_2019_30704512.html#key-findings",
    "title": "Prioritizing putative influential genes in cardiovascular disease susceptibility by applying tissue-specific Mendelian randomization",
    "section": "Key Findings",
    "text": "Key Findings\nThe study’s primary finding is the successful development and application of an analysis pipeline that integrates molecular and genetic data to prioritize genes whose tissue-specific expression or DNA methylation status influences cardiovascular disease (CVD) susceptibility.\n\nTissue-Specific Effects Demonstrated (MR): The Mendelian randomization (MR) analysis provided evidence for causal, tissue-specific effects at multiple genetic loci.\n\nSpecific insights were gained at the ADCY3 locus for body mass index (BMI).\nThe FADS1 locus showed effects on cholesterol levels.\nThe SORT1 locus influence on cholesterol and ApoB was found to colocalize specifically in liver tissue, but not whole blood, highlighting the importance of tissue specificity.\n\nDNA Methylation Role: Multiple-trait colocalization (moloc) analyses suggested that changes in DNA methylation at the promoter region upstream of FADS1/TMEM258 may work in conjunction with gene expression to affect cardiovascular trait variation.\nGene Prioritization: Applying the full pipeline genome-wide identified 233 association signals representing promising candidate genes for further functional evaluation.",
    "crumbs": [
      "MR",
      "Papers",
      "Prioritizing putative influential genes in cardiovascular disease susceptibility by applying tissue-specific Mendelian randomization"
    ]
  },
  {
    "objectID": "mr/taylor_2019_30704512.html#study-design-and-methods",
    "href": "mr/taylor_2019_30704512.html#study-design-and-methods",
    "title": "Prioritizing putative influential genes in cardiovascular disease susceptibility by applying tissue-specific Mendelian randomization",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\n\nAnalysis Pipeline\nThe authors developed a five-step analysis pipeline to move from genetic variants to putative causal genes and mechanisms:\n\nExposure and Outcome Association (eQTLWAS): An expression quantitative trait loci-wide association study (eQTLWAS) was performed using cis-eQTLs to uncover genetic variants associated with both nearby gene expression and cardiovascular traits.\nFine-Mapping: Causal variants at associated loci were prioritized using FINEMAP software in the ALSPAC dataset.\nCausal Inference (Tissue-Specific MR): Two-sample Mendelian randomization (MR) using the Wald ratio method was applied. The exposure was gene expression derived from the Genotype-Tissue Expression Project (GTEx) across cardiovascular-relevant tissues (e.g., adipose, liver, heart, artery) and brain areas. The outcome data came from large-scale GWAS consortia for traits like cholesterol, BMI, and triglycerides.\nShared Causal Variant (Multiple-Trait Colocalization): Bayesian multiple-trait colocalization (moloc) was used to assess whether the same underlying genetic variant influences both gene expression and the cardiovascular trait, and/or DNA methylation (using ARIES mQTL data), which helps rule out linkage disequilibrium as the sole explanation.\nGenome-Wide Application: The pipeline was applied genome-wide using summary statistics from large-scale GWAS to discover novel signals.\n\n\n\nData Sources\n\nDiscovery Cohort: Avon Longitudinal Study of Parents and Children (ALSPAC) was used to identify initial associations.\neQTL Data: Initial eQTLs were obtained from the Framingham Heart Study (FHS) for eQTLWAS. For the tissue-specific MR, data from the Genotype-Tissue Expression Project (GTEx v6p) across various tissues was used.\nmQTL Data: DNA methylation data was sourced from the Accessible Resource for Integrated Epigenomics Studies (ARIES) cohort (ALSPAC mothers).\nGWAS Outcomes: Summary statistics were taken from major GWAS consortia for cardiovascular and anthropometric traits.",
    "crumbs": [
      "MR",
      "Papers",
      "Prioritizing putative influential genes in cardiovascular disease susceptibility by applying tissue-specific Mendelian randomization"
    ]
  },
  {
    "objectID": "mr/taylor_2019_30704512.html#conclusions-and-recommendations",
    "href": "mr/taylor_2019_30704512.html#conclusions-and-recommendations",
    "title": "Prioritizing putative influential genes in cardiovascular disease susceptibility by applying tissue-specific Mendelian randomization",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study confirms that the genetic susceptibility to complex diseases, such as CVD, is modulated by differential changes in tissue-specific gene expression and DNA methylation.\nThe authors conclude that the developed pipeline is a valuable resource for elucidating biological mechanisms and prioritizing putative causal genes at loci where conventional GWAS provides ambiguous results due to co-regulated proximal genes. The findings, particularly those that suggest genetic loci influence cardiovascular traits early in the life course (via validation in ALSPAC), allow for a longer window of intervention for disease prevention. Future research requires more comprehensive tissue-specific DNA methylation data to further validate these complex causal pathways.",
    "crumbs": [
      "MR",
      "Papers",
      "Prioritizing putative influential genes in cardiovascular disease susceptibility by applying tissue-specific Mendelian randomization"
    ]
  },
  {
    "objectID": "mr/robinson_2021_33504897.html",
    "href": "mr/robinson_2021_33504897.html",
    "title": "Transcriptome-wide Mendelian randomization study prioritising novel tissue-dependent genes for glioma susceptibility",
    "section": "",
    "text": "PubMed: 33504897 DOI: 10.1038/s41598-021-82169-5 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "MR",
      "Papers",
      "Transcriptome-wide Mendelian randomization study prioritising novel tissue-dependent genes for glioma susceptibility"
    ]
  },
  {
    "objectID": "mr/robinson_2021_33504897.html#key-findings-identifying-glioma-susceptibility-genes",
    "href": "mr/robinson_2021_33504897.html#key-findings-identifying-glioma-susceptibility-genes",
    "title": "Transcriptome-wide Mendelian randomization study prioritising novel tissue-dependent genes for glioma susceptibility",
    "section": "Key Findings: Identifying Glioma Susceptibility Genes",
    "text": "Key Findings: Identifying Glioma Susceptibility Genes\nThis study integrated genetic data and gene expression data using Mendelian randomization (MR) to identify genes whose expression levels in specific tissues are causally linked to glioma susceptibility and its subtypes (glioblastoma (GBM) and non-GBM gliomas).\n\nPrioritized Causal Genes: The combined MR and colocalization approach provided evidence that genetically predicted increased gene expression of 12 genes is associated with an increased risk of glioma, GBM, and/or non-GBM risk.\nNovel Susceptibility Genes: Three of these 12 genes are novel glioma susceptibility genes: RETREG2/FAM134A, FAM178B, and MVB12B.\nTissue Dependence: The findings demonstrated strong evidence for tissue-dependent effects. For example, the causal effect of MDM4 expression on glioma risk was found to be specific to brain tissue, whereas effects for other genes (like TP53) were more generalized (present in both brain and whole blood).\nGlioma Subtype Specificity: The study observed distinct genetic associations for the different glioma subtypes, suggesting that some genetic mechanisms may be specific to GBM or non-GBM tumors.",
    "crumbs": [
      "MR",
      "Papers",
      "Transcriptome-wide Mendelian randomization study prioritising novel tissue-dependent genes for glioma susceptibility"
    ]
  },
  {
    "objectID": "mr/robinson_2021_33504897.html#study-design-and-methods",
    "href": "mr/robinson_2021_33504897.html#study-design-and-methods",
    "title": "Transcriptome-wide Mendelian randomization study prioritising novel tissue-dependent genes for glioma susceptibility",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\n\nStudy Design\nThis was a two-sample Mendelian randomization (MR) study that utilized a combined MR and colocalization approach. The primary goal was to infer causal relationships between genetically predicted gene expression levels (the exposure) and glioma risk (the outcome).\n\n\nData Sources\n\nOutcome GWAS Data (Glioma Risk): Summary statistics were used from a large-scale glioma GWAS (7400 cases, 8257 controls), with subtype-specific analyses for glioblastoma (GBM, 3112 cases) and non-GBM gliomas (2411 cases).\nExposure eQTL Data (Gene Expression): Expression quantitative trait loci (eQTLs) were sourced from two main sources:\n\nGTEx Data (Brain and Whole Blood): eQTLs from whole blood (effective \\(n=31,684\\)) and brain tissue (estimated effective \\(n=1194\\)).\nBrain-Specific GTEx Data: A more granular analysis was conducted using eQTLs from 13 distinct brain tissues (e.g., cortex, cerebellum, amygdala) with smaller sample sizes (\\(n=114\\) to 209).\n\n\n\n\nStatistical Analysis\n\nMendelian Randomization (MR): The Inverse-Variance Weighted (IVW) method was used as the primary MR approach. This was complemented by robust sensitivity analyses, including:\n\nMR-Egger regression: To test for and correct horizontal pleiotropy.\nWeighted Median and Weighted Mode: To provide consistent causal estimates even if a subset of instruments are invalid.\nMR-PRESSO (Pleiotropy RESidual Sum and Outlier): To detect and remove pleiotropic outliers.\n\nColocalization Analysis (moloc): The moloc method was used to confirm that the observed association between gene expression and glioma risk was due to a shared causal genetic variant, rather than two separate causal variants in linkage disequilibrium. This analysis provided strong statistical evidence for a shared signal (posterior probability, \\(PP_4 &gt; 0.8\\)).",
    "crumbs": [
      "MR",
      "Papers",
      "Transcriptome-wide Mendelian randomization study prioritising novel tissue-dependent genes for glioma susceptibility"
    ]
  },
  {
    "objectID": "mr/robinson_2021_33504897.html#conclusions-and-recommendations",
    "href": "mr/robinson_2021_33504897.html#conclusions-and-recommendations",
    "title": "Transcriptome-wide Mendelian randomization study prioritising novel tissue-dependent genes for glioma susceptibility",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study concludes that integrating multi-tissue eQTL data with robust MR and colocalization methods is effective for identifying biologically plausible, tissue-dependent genes involved in complex disease risk.\n\nThe findings highlight the crucial role of tissue specificity in the genetic architecture of glioma, demonstrating that expression in the target tissue (brain) is often the most relevant exposure.\nThe prioritization of genes like RETREG2/FAM134A, FAM178B, and MVB12B provides strong candidates for functional studies aimed at understanding the precise molecular mechanisms of glioma initiation and progression.\nThe authors recommend that future genetic studies on brain disorders should prioritize the use of brain-specific eQTL data over more easily accessible (but less relevant) tissues like whole blood.",
    "crumbs": [
      "MR",
      "Papers",
      "Transcriptome-wide Mendelian randomization study prioritising novel tissue-dependent genes for glioma susceptibility"
    ]
  },
  {
    "objectID": "metabolomics/desouza_2020_32380880.html",
    "href": "metabolomics/desouza_2020_32380880.html",
    "title": "Network-based strategies in metabolomics data analysis and interpretation: from molecular networking to biological interpretation",
    "section": "",
    "text": "PubMed: 32380880 DOI: 10.1080/14789450.2020.1766975 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Network-based strategies in metabolomics data analysis and interpretation: from molecular networking to biological interpretation"
    ]
  },
  {
    "objectID": "metabolomics/desouza_2020_32380880.html#key-focus-network-based-approaches-for-metabolomics-data",
    "href": "metabolomics/desouza_2020_32380880.html#key-focus-network-based-approaches-for-metabolomics-data",
    "title": "Network-based strategies in metabolomics data analysis and interpretation: from molecular networking to biological interpretation",
    "section": "Key Focus: Network-Based Approaches for Metabolomics Data",
    "text": "Key Focus: Network-Based Approaches for Metabolomics Data\nThis review article focuses on network-based strategies as essential computational tools for the analysis and interpretation of complex metabolomics data. It highlights how these methods move beyond simple compound identification and quantification to provide crucial insights into metabolic pathways and biological context.\n\nThe Challenge and the Solution\n\nChallenge: Metabolomics datasets, especially those generated by untargeted Mass Spectrometry (MS), are vast and complex, containing numerous features (ions) that are often challenging to identify, map to biological functions, and integrate into a cohesive biological understanding.\nSolution: Network analysis offers a powerful way to organize these complex datasets into visual and interpretable structures, revealing relationships between metabolites based on either chemical similarity (Molecular Networking) or biological correlation (Metabolic Network Reconstruction).",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Network-based strategies in metabolomics data analysis and interpretation: from molecular networking to biological interpretation"
    ]
  },
  {
    "objectID": "metabolomics/desouza_2020_32380880.html#network-strategies-and-methods",
    "href": "metabolomics/desouza_2020_32380880.html#network-strategies-and-methods",
    "title": "Network-based strategies in metabolomics data analysis and interpretation: from molecular networking to biological interpretation",
    "section": "Network Strategies and Methods",
    "text": "Network Strategies and Methods\nThe review divides network strategies into two main categories:\n\n1. Molecular Networking (MN)\n\nGoal: To organize MS/MS spectra of unknown compounds based on chemical structural similarity.\nMechanism: MN algorithms cluster compounds whose fragmentation spectra suggest they are structurally related (e.g., belong to the same chemical family or pathway), thereby facilitating the identification of unknown compounds in a large family once a single member is characterized.\nAdvanced Tools: The use of complementary tools like MolNetEnhancer is highlighted, which combines MN with chemical classification tools to provide structural family enrichment and better chemical context.\n\n\n\n2. Metabolic Network Reconstruction\n\nGoal: To infer functional and biological relationships between metabolites.\nCorrelation Networks: These networks use statistical correlation between metabolite abundance levels across different samples (or time points) to infer shared regulation or sequential steps in a metabolic pathway.\nBiological Mapping: The ultimate aim is to map the identified metabolites and their relationships onto established biochemical pathways (e.g., KEGG, Reactome) to gain biological meaning and contextualize changes observed in response to a perturbation or disease state.",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Network-based strategies in metabolomics data analysis and interpretation: from molecular networking to biological interpretation"
    ]
  },
  {
    "objectID": "metabolomics/desouza_2020_32380880.html#applications-and-importance",
    "href": "metabolomics/desouza_2020_32380880.html#applications-and-importance",
    "title": "Network-based strategies in metabolomics data analysis and interpretation: from molecular networking to biological interpretation",
    "section": "Applications and Importance",
    "text": "Applications and Importance\nNetwork-based strategies are crucial for several areas in metabolomics:\n\nUnknown Metabolite Identification: MN is especially valuable in natural product research and untargeted metabolomics for annotating large numbers of unknown spectral features.\nData Reduction and Visualization: Networks simplify highly dimensional data into intuitive visual representations that highlight key regulatory hubs or pathways.\nIntegration with Other Omics: The network approach facilitates the integration of metabolomics data with genomics, transcriptomics, and proteomics by mapping different molecular layers onto shared biological pathways.",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Network-based strategies in metabolomics data analysis and interpretation: from molecular networking to biological interpretation"
    ]
  },
  {
    "objectID": "metabolomics/desouza_2020_32380880.html#conclusion",
    "href": "metabolomics/desouza_2020_32380880.html#conclusion",
    "title": "Network-based strategies in metabolomics data analysis and interpretation: from molecular networking to biological interpretation",
    "section": "Conclusion",
    "text": "Conclusion\nThe review concludes that network-based approaches are evolving rapidly, moving from basic visualization tools to sophisticated platforms for chemical and biological interpretation. They are indispensable for handling the inherent complexity of the metabolome and translating raw data into meaningful biological insights.",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Network-based strategies in metabolomics data analysis and interpretation: from molecular networking to biological interpretation"
    ]
  },
  {
    "objectID": "metabolomics/makinen_2023_36823293.html",
    "href": "metabolomics/makinen_2023_36823293.html",
    "title": "Longitudinal metabolomics of increasing body-mass index and waist-hip ratio reveals two dynamic patterns of obesity",
    "section": "",
    "text": "PubMed: 36823293 DOI: 10.1038/s41366-023-01281-w Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Longitudinal metabolomics of increasing body-mass index and waist-hip ratio reveals two dynamic patterns of obesity"
    ]
  },
  {
    "objectID": "metabolomics/makinen_2023_36823293.html#key-findings-two-distinct-metabolic-patterns-of-obesity",
    "href": "metabolomics/makinen_2023_36823293.html#key-findings-two-distinct-metabolic-patterns-of-obesity",
    "title": "Longitudinal metabolomics of increasing body-mass index and waist-hip ratio reveals two dynamic patterns of obesity",
    "section": "Key Findings: Two Distinct Metabolic Patterns of Obesity",
    "text": "Key Findings: Two Distinct Metabolic Patterns of Obesity\nThis longitudinal metabolomics study dissected the complex temporal associations between Body-Mass Index (BMI), Waist-Hip Ratio (WHR), and the circulating metabolome. The core finding is that there are two dynamically different metabolic patterns associated with increasing obesity over time, which are strongly influenced by the specific obesity metric used (BMI vs. WHR).\n\n1. BMI Trajectories: Metabolic Health vs. Risk\nThe longitudinal analysis (using the NFBC1966 cohort with data spanning 15 years) identified two main metabolic patterns related to BMI:\n\n“Metabolically Healthy” BMI: An increase in BMI that was not accompanied by an increase in fatty acid (FA) saturation (the degree to which FAs are saturated) was associated with a favorable lipid profile, better glucose metabolism, and lower inflammatory markers.\n“High-Risk” BMI: An increase in BMI that was accompanied by an increase in FA saturation was linked to an adverse metabolic profile, including elevated very-low-density lipoprotein (VLDL) and low-density lipoprotein (LDL) cholesterol, increased insulin resistance, and higher inflammation markers.\n\n\n\n2. WHR Trajectories: Highly Adverse Profile\nIn contrast to BMI, an increase in Waist-Hip Ratio (WHR) over time was uniformly associated with a single, highly adverse metabolic profile, regardless of the accompanying changes in FA saturation. WHR, a measure of central adiposity, was linked to: * Significantly worse glucose metabolism (insulin resistance). * Higher levels of inflammatory markers. * Higher concentrations of amino acids (specifically branched-chain amino acids, BCAA), which are known markers of insulin resistance.",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Longitudinal metabolomics of increasing body-mass index and waist-hip ratio reveals two dynamic patterns of obesity"
    ]
  },
  {
    "objectID": "metabolomics/makinen_2023_36823293.html#methods-and-study-design",
    "href": "metabolomics/makinen_2023_36823293.html#methods-and-study-design",
    "title": "Longitudinal metabolomics of increasing body-mass index and waist-hip ratio reveals two dynamic patterns of obesity",
    "section": "Methods and Study Design",
    "text": "Methods and Study Design\n\nCohorts and Data\n\nLongitudinal Cohorts: Northern Finland Birth Cohort (NFBC1966, n=3,117) with two time points (age 31 and 46), allowing for the calculation of BMI and WHR trajectories (changes over time).\nCross-sectional Cohort: FINRISK (n=9,708) for initial data-driven subgrouping.\nMetabolomics: Quantification of 174 circulating metabolic measures (including lipids, fatty acids, and amino acids) using proton nuclear magnetic resonance (\\(^1\\)H-NMR) spectroscopy.\n\n\n\nSystems Epidemiology Tools\n\nSelf-Organizing Map (SOM): An unsupervised machine learning algorithm used on the cross-sectional data to simplify the high-dimensional metabolomics data into four discrete, biologically interpretable metabolic subgroups (A, B, C, D).\nLongitudinal Modeling: The study used the continuous trajectory of BMI and WHR, along with the metabolomic subgroups, to identify the specific metabolic changes associated with the development of obesity over the 15-year follow-up.",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Longitudinal metabolomics of increasing body-mass index and waist-hip ratio reveals two dynamic patterns of obesity"
    ]
  },
  {
    "objectID": "metabolomics/makinen_2023_36823293.html#conclusions-and-implications",
    "href": "metabolomics/makinen_2023_36823293.html#conclusions-and-implications",
    "title": "Longitudinal metabolomics of increasing body-mass index and waist-hip ratio reveals two dynamic patterns of obesity",
    "section": "Conclusions and Implications",
    "text": "Conclusions and Implications\nThe study demonstrates that obesity is not a metabolically uniform state. The use of longitudinal metabolomics and systems epidemiology tools (like SOMs) is essential for dissecting the heterogeneity of obesity.\nThe key clinical implication is that WHR and BMI trajectories should be viewed as distinct risk factors: * WHR (central adiposity) is a more consistent marker of a severe, underlying adverse metabolic risk. * BMI alone may mask distinct metabolic phenotypes, and its risk assessment should be refined by incorporating the associated fatty acid saturation profile.\nThe findings support moving beyond simple single-time-point measurements to focus on dynamic changes in metabolic profiles for personalized risk assessment and intervention in obesity-related diseases.",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Longitudinal metabolomics of increasing body-mass index and waist-hip ratio reveals two dynamic patterns of obesity"
    ]
  },
  {
    "objectID": "metabolomics/index.html",
    "href": "metabolomics/index.html",
    "title": "metabolomics",
    "section": "",
    "text": "Best practices and tools in R and Python for statistical processing and visualization of lipidomics and metabolomics data\n\n\n\nObjective: This review compiles best practices and freely accessible tools in R and Python for the statistical processing and visualization of extensive mass spectrometry-based lipidomics and metabolomics data.\nFocus: The article provides a “solid core” of resources for exploratory data analysis (EDA) and visualization to help researchers identify and visualize statistically significant trends and biologically relevant differences within their complex datasets.\nImplication: It guides researchers on using modern computational platforms (R/Python) and integrating metadata (e.g., clinical parameters) with their omics data to perform robust and reproducible downstream analysis.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nCharacterization of missing values in untargeted MS-based metabolomics data and evaluation of missing data handling strategies\n\n\n\nObjective: The study systematically characterized the sources of missing values (MVs) in untargeted Mass Spectrometry (MS)-based metabolomics data and evaluated various imputation strategies.\nMissing Value Types: Distinguished between systematic missingness primarily due to Limits of Detection (LOD) and random missingness due to technical issues.\nBest Strategy: For the prevalent LOD-related MVs, which represent concentrations near zero, simple methods like imputation with half of the minimum observed value were found to be effective and often outperformed complex data-driven methods (e.g., PPCA).\nRecommendation: Researchers should use targeted imputation strategies based on the nature of the missingness to avoid introducing bias and reducing statistical power.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nAltered metabolite levels in cancer: implications for tumour biology and cancer therapy\n\n\n\nFocus: This review examines how altered intracellular metabolite concentrations in cancer cells, often driven by genetic mutations, actively promote tumor initiation and progression, moving beyond the idea that metabolic changes are merely a consequence of cancer.\nOncometabolites: Specific metabolites act as effector molecules:\n\n2-Hydroxyglutarate (2-HG): Produced by IDH1/2 mutations, it inhibits epigenetic regulators (like TET enzymes) leading to globally altered gene expression and oncogenesis.\nFumarate and Succinate: Accumulate due to FH/SDH mutations, leading to the stabilization of HIF-\\(1\\alpha\\) (pseudohypoxia), which drives proliferation and the Warburg effect.\n\nImplication: The altered metabolome is a rich source of therapeutic targets. Strategies can focus on counteracting the effects of oncometabolites or exploiting the metabolic dependencies created by these shifts (e.g., limited aspartate for nucleotide synthesis).\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nLongitudinal metabolomics of increasing body-mass index and waist-hip ratio reveals two dynamic patterns of obesity\n\n\n\nObjective: This longitudinal study used metabolomics and systems epidemiology tools (Self-Organizing Map, SOM) on over 12,800 participants to dissect the complex temporal associations between Body-Mass Index (BMI), Waist-Hip Ratio (WHR), and the circulating metabolome.\nKey Finding: The study revealed two dynamically different metabolic patterns for increasing obesity:\n\nAn increase in BMI that was not accompanied by an increase in fatty acid (FA) saturation was associated with a relatively favorable metabolic profile.\nAn increase in WHR (central adiposity) was uniformly associated with a single, highly adverse metabolic profile, characterized by worse glucose metabolism, inflammation, and high amino acid levels.\n\nImplication: The results suggest that obesity is metabolically heterogeneous and that WHR is a more consistent marker of severe adverse metabolic risk than BMI alone, emphasizing the need to consider dynamic metabolic changes for personalized risk assessment.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nNetworks and Graphs Discovery in Metabolomics Data Analysis and Interpretation\n\n\n\nFocus: This review highlights the use of networks and graph theory as powerful computational tools for analyzing and interpreting complex metabolomics data.\nNetwork Types:\n\nAnalytical/Chemical Networks: Derived from Mass Spectrometry (MS) data, such as Molecular Networking (MN), which connects spectra based on chemical structural similarity to aid in compound identification.\nBiological/Correlation Networks: Derived from quantitative metabolite abundance data. These networks use statistical correlation between metabolites to infer shared biological regulation, map metabolites onto known biochemical pathways, and facilitate multi-omics integration (connecting metabolites to genes/proteins).\n\nImplication: Graph theory enables researchers to move beyond simple lists of metabolites to view the metabolome as a structured, interactive system, which is essential for biological interpretation and hypothesis generation.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nNetwork-based strategies in metabolomics data analysis and interpretation: from molecular networking to biological interpretation\n\n\n\nFocus: This review highlights the use of network-based strategies as essential tools for analyzing and interpreting complex metabolomics data, especially from untargeted Mass Spectrometry (MS).\nMolecular Networking (MN): Used to organize MS/MS spectral data based on chemical structural similarity, thereby facilitating the identification of unknown compounds in chemical families.\nMetabolic Networks: Constructed using statistical correlation between metabolite abundance levels to infer functional and biological relationships, enabling mapping onto known biochemical pathways for biological contextualization.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "metabolomics",
      "Papers"
    ]
  },
  {
    "objectID": "metabolomics/idkowiak_2025_41027880.html",
    "href": "metabolomics/idkowiak_2025_41027880.html",
    "title": "Best practices and tools in R and Python for statistical processing and visualization of lipidomics and metabolomics data",
    "section": "",
    "text": "PubMed: 41027880 DOI: 10.1038/s41467-025-63751-1 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Best practices and tools in R and Python for statistical processing and visualization of lipidomics and metabolomics data"
    ]
  },
  {
    "objectID": "metabolomics/idkowiak_2025_41027880.html#key-focus-data-analysis-tools-and-best-practices-in-r-and-python",
    "href": "metabolomics/idkowiak_2025_41027880.html#key-focus-data-analysis-tools-and-best-practices-in-r-and-python",
    "title": "Best practices and tools in R and Python for statistical processing and visualization of lipidomics and metabolomics data",
    "section": "Key Focus: Data Analysis Tools and Best Practices in R and Python",
    "text": "Key Focus: Data Analysis Tools and Best Practices in R and Python\nThis review article serves as a comprehensive guide and compilation of best practices and freely accessible tools in R and Python for the statistical processing and visualization of mass spectrometry-based lipidomics and metabolomics data. The authors acknowledge that these “omics” generate extensive datasets that require specific data exploration skills to effectively identify and visualize statistically significant trends and biologically relevant differences.\n\nThe Need for Dedicated Tools\nMass spectrometry-based lipidomics and metabolomics workflows are characterized by high-dimensional data, complex normalization needs, and the necessity to integrate data with extensive metadata (such as clinical parameters). Standard spreadsheet software is insufficient for handling the volume and complexity of these datasets. The review addresses this gap by compiling and discussing computational resources tailored for this purpose.\n\n\nCore Areas Covered by the Review\nThe review focuses on the core stages of post-acquisition data analysis, primarily emphasizing exploratory data analysis (EDA) and visualization:\n\nStatistical Processing: Tools and packages for performing univariate and multivariate statistical analyses, including methods like Principal Component Analysis (PCA) and Partial Least Squares Discriminant Analysis (PLS-DA), which are standard for feature reduction and visualizing separation between groups.\nVisualization: Compilations of packages for generating high-quality graphical representations essential for biological interpretation, such as volcano plots, heatmaps, box plots, and specialized lipid/metabolite class distribution plots.\nBest Practices: Discussion of standardized workflows and best practices to ensure reproducibility and accurate results in data handling, which is critical given the inherent variability in mass spectrometry data.\nR and Python Focus: The article prioritizes tools within the R and Python ecosystems, which are the dominant platforms for modern biological data analysis due to their powerful statistical libraries and open-source nature.",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Best practices and tools in R and Python for statistical processing and visualization of lipidomics and metabolomics data"
    ]
  },
  {
    "objectID": "metabolomics/idkowiak_2025_41027880.html#conclusion-and-utility",
    "href": "metabolomics/idkowiak_2025_41027880.html#conclusion-and-utility",
    "title": "Best practices and tools in R and Python for statistical processing and visualization of lipidomics and metabolomics data",
    "section": "Conclusion and Utility",
    "text": "Conclusion and Utility\nThe review provides a valuable resource for researchers in the lipidomics and metabolomics fields, compiling the solid core of accessible tools required for transforming raw data into biologically meaningful insights. By focusing on R and Python, it guides users toward implementing robust, reproducible, and effective computational strategies for their high-dimensional data.",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Best practices and tools in R and Python for statistical processing and visualization of lipidomics and metabolomics data"
    ]
  },
  {
    "objectID": "multi-omics/adamer_2024_38940158.html",
    "href": "multi-omics/adamer_2024_38940158.html",
    "title": "Biomarker identification by interpretable maximum mean discrepancy",
    "section": "",
    "text": "PubMed: 38940158 DOI: 10.1093/bioinformatics/btae251 Overview generated by: Gemini 2.5 Flash, 27/11/2025",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Biomarker identification by interpretable maximum mean discrepancy"
    ]
  },
  {
    "objectID": "multi-omics/adamer_2024_38940158.html#background-and-objective",
    "href": "multi-omics/adamer_2024_38940158.html#background-and-objective",
    "title": "Biomarker identification by interpretable maximum mean discrepancy",
    "section": "Background and Objective",
    "text": "Background and Objective\nIn biomedical applications, researchers frequently deal with paired groups of samples (e.g., treated vs. control, or diseased vs. healthy) and aim to identify discriminating features, or biomarkers, based on high-dimensional omics data. This problem is fundamentally a two-sample problem, requiring a statistical test to establish a difference between the groups and a method to interpret which features cause that difference.\nWhile the multivariate Maximum Mean Discrepancy (MMD) test can quantify group-level differences, the identification of specific features (biomarkers) usually requires a separate, often less powerful, univariate feature selection step. The objective of this study was to introduce a novel method that combines two-sample testing and feature selection into a single, unified experiment.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Biomarker identification by interpretable maximum mean discrepancy"
    ]
  },
  {
    "objectID": "multi-omics/adamer_2024_38940158.html#methods-spinopt-mmd",
    "href": "multi-omics/adamer_2024_38940158.html#methods-spinopt-mmd",
    "title": "Biomarker identification by interpretable maximum mean discrepancy",
    "section": "Methods: SpInOpt-MMD",
    "text": "Methods: SpInOpt-MMD\nThe authors developed SpInOpt-MMD (Sparse, Interpretable, and Optimized MMD test), a novel statistical framework designed to simultaneously test for differences between two high-dimensional datasets and identify the most relevant features contributing to that difference.\n\nApproach: SpInOpt-MMD extends the standard MMD by incorporating sparse and interpretable optimization techniques. This allows the model to quantify the difference between two sample distributions (two-sample test) while simultaneously performing feature selection.\nFeature Selection: Unlike methods that rely on subsequent univariate analysis or complex post-hoc interpretation (like SHapley Additive exPlanations, or SHAP), SpInOpt-MMD directly outputs the set of statistically significant and distinguishing features (biomarkers) as part of the core testing process.\nVersatility: The method is versatile and was demonstrated on a variety of data types, including gene expression measurements, text data, and images.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Biomarker identification by interpretable maximum mean discrepancy"
    ]
  },
  {
    "objectID": "multi-omics/adamer_2024_38940158.html#key-results-and-conclusion",
    "href": "multi-omics/adamer_2024_38940158.html#key-results-and-conclusion",
    "title": "Biomarker identification by interpretable maximum mean discrepancy",
    "section": "Key Results and Conclusion",
    "text": "Key Results and Conclusion\nThe evaluation of SpInOpt-MMD highlighted its effectiveness, particularly in challenging scenarios:\n\nSuperior Performance: SpInOpt-MMD was shown to be highly effective at identifying relevant features, even in small sample sizes, and demonstrated superior performance compared to traditional feature selection methods such as SHAP and univariate association analysis in several experiments.\nUnified Analysis: The method provides a powerful, single-step solution for the two-sample testing and biomarker identification problem in high-dimensional settings, which is crucial for multi-omics research.\nOpen-Access Resource: The code and links to the public data are made available to promote reproducibility and widespread adoption of the new method.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Biomarker identification by interpretable maximum mean discrepancy"
    ]
  },
  {
    "objectID": "multi-omics/hsu_2020_32467615.html",
    "href": "multi-omics/hsu_2020_32467615.html",
    "title": "Integrating untargeted metabolomics, genetically informed causal inference, and pathway enrichment to define the obesity metabolome",
    "section": "",
    "text": "PubMed: 32467615 DOI: 10.1038/s41366-020-0603-x Overview generated by: Gemini 2.5 Flash, 27/11/2025",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Integrating untargeted metabolomics, genetically informed causal inference, and pathway enrichment to define the obesity metabolome"
    ]
  },
  {
    "objectID": "multi-omics/hsu_2020_32467615.html#background-and-objective",
    "href": "multi-omics/hsu_2020_32467615.html#background-and-objective",
    "title": "Integrating untargeted metabolomics, genetically informed causal inference, and pathway enrichment to define the obesity metabolome",
    "section": "Background and Objective",
    "text": "Background and Objective\nObesity is associated with significant metabolic disruption. Understanding the causal connections between obesity and changes in metabolite levels is crucial for identifying new intervention targets. Previous studies using Mendelian Randomization (MR) to infer causality between metabolites and obesity often ignored the majority of data generated by untargeted metabolomics—specifically, the signals from unknown or unidentified metabolites.\nThis study aimed to develop a comprehensive framework that integrates untargeted metabolomics, genetics, and pathway enrichment analysis to: 1. Identify a broad spectrum of metabolites causally related to Body Mass Index (BMI). 2. Characterize the biological pathways involved in the obesity metabolome, including those represented by unidentified metabolites.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Integrating untargeted metabolomics, genetically informed causal inference, and pathway enrichment to define the obesity metabolome"
    ]
  },
  {
    "objectID": "multi-omics/hsu_2020_32467615.html#study-methods",
    "href": "multi-omics/hsu_2020_32467615.html#study-methods",
    "title": "Integrating untargeted metabolomics, genetically informed causal inference, and pathway enrichment to define the obesity metabolome",
    "section": "Study Methods",
    "text": "Study Methods\nThe authors utilized a multi-stage approach across three large cohorts:\n\nMetabolomic Profiling: Untargeted metabolomic profiling was conducted on samples from the Framingham Heart Study (FHS), generating quantitative signals for both known (identified) and unknown (unidentified) metabolites.\nGenome-Wide Association Study (GWAS): A GWAS was performed to identify metabolite-QTLs (mQTLs)—genetic variants associated with the levels of both known and unknown metabolites. These mQTLs served as the instrumental variables (IVs) for the subsequent MR analysis.\nTwo-Sample Mendelian Randomization (MR):\n\nThe mQTLs identified from the FHS metabolomics GWAS were used as IVs.\nThe exposure was each metabolite (known and unknown).\nThe outcome was BMI, using summary statistics from a large published BMI GWAS.\nMultivariable MR was applied to test for independence among groups of putatively co-regulated metabolites.\n\nPathway Enrichment Analysis: To interpret the role of the unknown metabolites, a novel Pathway Enrichment Analysis method was developed. This method uses the shared genetic architecture (i.e., common mQTLs) between unknown and known metabolites to infer the metabolic pathway of the unknown metabolites.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Integrating untargeted metabolomics, genetically informed causal inference, and pathway enrichment to define the obesity metabolome"
    ]
  },
  {
    "objectID": "multi-omics/hsu_2020_32467615.html#key-results-and-findings",
    "href": "multi-omics/hsu_2020_32467615.html#key-results-and-findings",
    "title": "Integrating untargeted metabolomics, genetically informed causal inference, and pathway enrichment to define the obesity metabolome",
    "section": "Key Results and Findings",
    "text": "Key Results and Findings\n\nCausal Metabolites\n\nThe study identified 23 metabolites (15 known and 8 unknown) that were causally associated with BMI.\nPro-Obesity Metabolites: Metabolites whose higher levels were causally linked to higher BMI included certain amino acid catabolites, medium-chain acylcarnitines, and metabolites related to the carnitine cycle and fatty acid oxidation.\nAnti-Obesity Metabolites: Metabolites whose higher levels were causally linked to lower BMI included glycine, acetylated/formylated amino acids, and several glycerophospholipids.\n\n\n\nDefining the Obesity Metabolome\nThe integrated analysis defined the obesity metabolome as being characterized by a metabolic shift involving: 1. Impaired Amino Acid Catabolism: Specifically, branched-chain amino acid (BCAA) and aromatic amino acid catabolites showed a strong causal link with higher BMI. 2. Dysfunctional Lipid Metabolism: The enrichment analysis linked BMI to pathways of lipid metabolism, particularly those involving glycerophospholipids and the carnitine shuttle.\n\n\nImportance of Unknown Metabolites\nThe inclusion of 8 causally-associated unknown metabolites expanded the definition of the obesity metabolome. The novel pathway enrichment approach successfully mapped these unknown metabolites to relevant metabolic pathways (e.g., lipid and amino acid metabolism) based on their shared genetic control with known metabolites.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Integrating untargeted metabolomics, genetically informed causal inference, and pathway enrichment to define the obesity metabolome"
    ]
  },
  {
    "objectID": "multi-omics/hsu_2020_32467615.html#conclusions-and-significance",
    "href": "multi-omics/hsu_2020_32467615.html#conclusions-and-significance",
    "title": "Integrating untargeted metabolomics, genetically informed causal inference, and pathway enrichment to define the obesity metabolome",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nThe comprehensive framework successfully leveraged the power of untargeted metabolomics in combination with genetically informed causal inference (MR) and a novel pathway enrichment tool. This approach provides a more complete, systems-level view of the metabolic disturbances associated with obesity.\nThe identified causal metabolites and pathways offer promising avenues for targeted therapeutic and diagnostic interventions aimed at treating obesity and its related metabolic diseases.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Integrating untargeted metabolomics, genetically informed causal inference, and pathway enrichment to define the obesity metabolome"
    ]
  },
  {
    "objectID": "multi-omics/richenberg_2023_37873386.html",
    "href": "multi-omics/richenberg_2023_37873386.html",
    "title": "The tumor multi-omic landscape of endometrial cancers developed on a germline genetic background of adiposity",
    "section": "",
    "text": "PubMed: 37873386 DOI: 10.1101/2023.10.09.23296765 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "multi-omics",
      "Papers",
      "The tumor multi-omic landscape of endometrial cancers developed on a germline genetic background of adiposity"
    ]
  },
  {
    "objectID": "multi-omics/richenberg_2023_37873386.html#key-findings-linking-genetic-adiposity-to-tumor-biology",
    "href": "multi-omics/richenberg_2023_37873386.html#key-findings-linking-genetic-adiposity-to-tumor-biology",
    "title": "The tumor multi-omic landscape of endometrial cancers developed on a germline genetic background of adiposity",
    "section": "Key Findings: Linking Genetic Adiposity to Tumor Biology",
    "text": "Key Findings: Linking Genetic Adiposity to Tumor Biology\nThis study used a two-sample Mendelian randomization (MR) approach to investigate how germline genetic variants associated with higher Body Mass Index (BMI)/adiposity causally influence the multi-omic landscape (gene expression, DNA methylation, somatic mutations, and tumor microenvironment) of subsequent endometrial cancers (EC).\n\nCausal Effect on Tumor Expression: Genetically predicted higher BMI was causally associated with increased expression of the gene MDM2 in EC tumors (FDR-corrected \\(P &lt; 0.05\\)). MDM2 is a key regulator of the TP53 pathway and a potential oncogene, suggesting that the genetic effects of adiposity influence a critical tumor survival pathway.\nTumor Immune Microenvironment (TIME) Effect: Higher genetically predicted BMI was also associated with significant changes in the tumor immune microenvironment. Specifically, it was causally linked to:\n\nDecreased CD4+ T cell infiltration (\\(P &lt; 0.05\\) FDR-corrected).\nDecreased cytotoxic T cell infiltration (\\(P &lt; 0.05\\) FDR-corrected).\nThis finding suggests that germline adiposity may drive EC progression by creating a more immunosuppressive tumor microenvironment, potentially reducing the efficacy of immunotherapies.\n\nNo Causal Effect on Somatic Mutations: The study found no strong evidence that genetically predicted BMI causally influenced the frequency of common EC somatic mutations (e.g., in PTEN, PIK3CA, or TP53), indicating that the adiposity-driven mechanism likely impacts tumor biology through changes in gene expression and the TIME, rather than through altering fundamental mutation rates.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "The tumor multi-omic landscape of endometrial cancers developed on a germline genetic background of adiposity"
    ]
  },
  {
    "objectID": "multi-omics/richenberg_2023_37873386.html#study-design-and-methods",
    "href": "multi-omics/richenberg_2023_37873386.html#study-design-and-methods",
    "title": "The tumor multi-omic landscape of endometrial cancers developed on a germline genetic background of adiposity",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\n\nStudy Design\nThis was a two-sample Mendelian randomization (MR) study that integrated summary-level GWAS data for the exposure (adiposity) with tumor-specific multi-omic data from the outcome (EC tumor biology).\n\n\nData Sources and Exposures\n\nExposure (Adiposity): Genetic variants (SNPs) associated with BMI were used as instrumental variables (IVs). A total of 170 independent BMI-associated SNPs were selected from a large GWAS meta-analysis (\\(n=806,834\\)).\nOutcome (EC Tumor Multi-omics): Multi-omic data for Endometrial Cancer (EC) were sourced from The Cancer Genome Atlas (TCGA) and other large consortia:\n\nTranscriptomics: Gene expression levels for 17,929 genes.\nMethylomics: DNA methylation levels across 11,108 CpG sites.\nSomatic Mutations: Frequency of common driver mutations.\nTumor Microenvironment: Estimated immune cell infiltration levels (e.g., cytotoxic T cells, CD4+ T cells) derived from gene expression profiles using validated methods (e.g., CIBERSORT).\n\n\n\n\nStatistical Analysis\n\nMendelian Randomization (MR): The Inverse-Variance Weighted (IVW) method was used as the primary MR approach to estimate the causal effect of genetically predicted BMI on each multi-omic feature. The results were reported as the change in the outcome feature per 1 standard deviation (SD) increase in BMI.\nSensitivity Analyses: Robust MR methods, including MR-Egger, were used to test for and account for potential horizontal pleiotropy.\nMultiple Testing Correction: All results were subjected to a stringent False Discovery Rate (FDR) correction (Benjamini-Hochberg) to account for the large number of molecular outcomes tested (over 29,000 multi-omic traits).",
    "crumbs": [
      "multi-omics",
      "Papers",
      "The tumor multi-omic landscape of endometrial cancers developed on a germline genetic background of adiposity"
    ]
  },
  {
    "objectID": "multi-omics/richenberg_2023_37873386.html#conclusions-and-recommendations",
    "href": "multi-omics/richenberg_2023_37873386.html#conclusions-and-recommendations",
    "title": "The tumor multi-omic landscape of endometrial cancers developed on a germline genetic background of adiposity",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study concludes that a person’s germline genetic predisposition to adiposity plays a significant, causal role in shaping the molecular and immunological characteristics of their subsequent endometrial tumors.\n\nMechanistic Insight: The findings suggest that the metabolic/endocrine consequences of adiposity (driven by the germline genome) likely fuel tumor growth by activating oncogenic pathways (MDM2 overexpression) and suppressing anti-tumor immunity (reduced CD4+ and cytotoxic T cell infiltration).\nClinical Implications: This mechanistic understanding could inform targeted interventions. For instance, in individuals with high genetic adiposity risk, combining weight-loss interventions with treatments that counteract MDM2 activity or enhance T cell infiltration might prove particularly effective for EC prevention or therapy.\nFuture Work: The authors recommend applying this multi-omic MR approach to investigate the causal influence of genetic adiposity on other obesity-related cancers (e.g., breast and colorectal cancer) and to explore the specific pathways driving the suppression of anti-tumor immunity.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "The tumor multi-omic landscape of endometrial cancers developed on a germline genetic background of adiposity"
    ]
  },
  {
    "objectID": "multi-omics/tenenhaus_2014_24550197.html",
    "href": "multi-omics/tenenhaus_2014_24550197.html",
    "title": "Variable selection for generalized canonical correlation analysis",
    "section": "",
    "text": "PubMed: 24550197 DOI: 10.1093/biostatistics/kxu001 Overview generated by: Gemini 2.5 Flash, 27/11/2025",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Variable selection for generalized canonical correlation analysis"
    ]
  },
  {
    "objectID": "multi-omics/tenenhaus_2014_24550197.html#background-and-objective",
    "href": "multi-omics/tenenhaus_2014_24550197.html#background-and-objective",
    "title": "Variable selection for generalized canonical correlation analysis",
    "section": "Background and Objective",
    "text": "Background and Objective\nThe rise of multi-omics studies, which generate three or more heterogeneous datasets (e.g., transcriptomics, proteomics, clinical data), necessitates statistical methods that can effectively integrate these data blocks. Canonical Correlation Analysis (CCA) is a classic method for finding shared variance between two datasets, and Generalized Canonical Correlation Analysis (GCCA) is its generalization to three or more data blocks.\nThis paper addresses a major limitation of standard GCCA: the resulting components are linear combinations of all input variables, making biological interpretation difficult due to high dimensionality. The primary objective was to introduce a method to perform variable selection within the GCCA framework, simultaneously reducing dimensionality and identifying the most relevant features.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Variable selection for generalized canonical correlation analysis"
    ]
  },
  {
    "objectID": "multi-omics/tenenhaus_2014_24550197.html#methods-sparse-generalized-canonical-correlation-analysis-sgcca",
    "href": "multi-omics/tenenhaus_2014_24550197.html#methods-sparse-generalized-canonical-correlation-analysis-sgcca",
    "title": "Variable selection for generalized canonical correlation analysis",
    "section": "Methods: Sparse Generalized Canonical Correlation Analysis (SGCCA)",
    "text": "Methods: Sparse Generalized Canonical Correlation Analysis (SGCCA)\n\nRegularized GCCA (RGCCA)\nThe authors first utilize the Regularized Generalized Canonical Correlation Analysis (RGCCA) framework, a flexible method for integrating \\(K \\ge 2\\) data blocks by defining different objectives (e.g., maximizing the sum of pairwise correlations) and connections (a block design matrix) between the data blocks.\n\n\nThe Innovation: SGCCA\nThe key methodological contribution is the introduction of a sparse penalty (an \\(L_1\\) penalty, similar to LASSO) into the RGCCA objective function. This novel method is termed Sparse Generalized Canonical Correlation Analysis (SGCCA).\n\nFunction: SGCCA performs dimension reduction (by finding latent components) and variable selection simultaneously.\nMechanism: The sparse penalty forces the loading vectors (which define the components) to contain many zero values. This means that the latent components are computed using only a small, relevant subset of the original features from each omics block.\nGoal: By selecting only a few, highly contributing variables, SGCCA facilitates the interpretability of the integrated results and identifies a minimal set of molecular features that drive the shared correlation structure across all omics datasets.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Variable selection for generalized canonical correlation analysis"
    ]
  },
  {
    "objectID": "multi-omics/tenenhaus_2014_24550197.html#conclusions-and-significance",
    "href": "multi-omics/tenenhaus_2014_24550197.html#conclusions-and-significance",
    "title": "Variable selection for generalized canonical correlation analysis",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nThe introduction of SGCCA provides a powerful and flexible multivariate statistical tool for the unsupervised integration of multi-omics data.\nBy seamlessly incorporating variable selection into the generalized CCA framework, the method addresses the high dimensionality inherent in omics data. SGCCA enables researchers to distill the complex relationships across multiple omics layers into a coherent, biologically meaningful set of key molecular features that are responsible for the shared variation across the integrated datasets.\nThe method is foundational for subsequent multi-omics integration tools, providing a basis for correlation-based feature selection in systems biology.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Variable selection for generalized canonical correlation analysis"
    ]
  },
  {
    "objectID": "multi-omics/velten_2022_35027765.html",
    "href": "multi-omics/velten_2022_35027765.html",
    "title": "Identifying temporal and spatial patterns of variation from multimodal data using MEFISTO",
    "section": "",
    "text": "PubMed: 35027765 DOI: 10.1038/s41592-021-01343-9 Overview generated by: Gemini 2.5 Flash, 27/11/2025",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Identifying temporal and spatial patterns of variation from multimodal data using MEFISTO"
    ]
  },
  {
    "objectID": "multi-omics/velten_2022_35027765.html#background-and-objective",
    "href": "multi-omics/velten_2022_35027765.html#background-and-objective",
    "title": "Identifying temporal and spatial patterns of variation from multimodal data using MEFISTO",
    "section": "Background and Objective",
    "text": "Background and Objective\nFactor analysis, as implemented in tools like MOFA (Multi-Omics Factor Analysis), is widely used for dimensionality reduction and multi-omics integration. However, these models traditionally assume that samples are independent of one another. This assumption fails in modern high-resolution biological studies that profile molecular data with temporal (time-series) or spatial dependencies (e.g., spatial transcriptomics, longitudinal cohorts, developmental atlases).\nThis paper introduces MEFISTO (Multi-omics Factor Analysis Informed by Spatial and Temporal Omics), an extension of the MOFA framework, designed to: 1. Perform factor analysis while explicitly accounting for spatial or temporal dependencies between samples. 2. Perform spatio-temporally informed dimensionality reduction and imputation on high-dimensional multi-omics data. 3. Separate smooth (e.g., developmental trajectory) from non-smooth (e.g., technical noise) patterns of variation.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Identifying temporal and spatial patterns of variation from multimodal data using MEFISTO"
    ]
  },
  {
    "objectID": "multi-omics/velten_2022_35027765.html#methods-the-mefisto-framework",
    "href": "multi-omics/velten_2022_35027765.html#methods-the-mefisto-framework",
    "title": "Identifying temporal and spatial patterns of variation from multimodal data using MEFISTO",
    "section": "Methods: The MEFISTO Framework",
    "text": "Methods: The MEFISTO Framework\n\nCore Algorithm and Innovation\nMEFISTO maintains the core latent factor model of MOFA, where data from different omics layers is modeled as a linear combination of shared latent factors. The key innovation is how it models these latent factors:\n\nSpatio-Temporal Prior: MEFISTO incorporates a Gaussian Process (GP) prior on the factor values. This GP prior allows the factors to be smooth functions of the observed spatial or temporal coordinates (i.e., time points, spatial locations).\nModeling Dependencies: By using the GP prior, MEFISTO learns latent factors that are constrained to change gradually over space or time, which aligns with biological reality (e.g., developmental progression, cellular diffusion). Factors not relevant to the spatial/temporal axes are modeled with a non-smooth prior.\nJoint Integration: Like MOFA, MEFISTO can integrate data from multiple omics modalities measured on the same samples and simultaneously identify shared and modality-specific sources of variation.\n\n\n\nApplications\nMEFISTO was validated across diverse datasets with structured dependencies: 1. Spatial Transcriptomics: Applied to mouse organ data to reconstruct the spatial organization of gene expression. 2. Longitudinal Microbiome Study: Used to capture smooth temporal changes in the gut microbiome. 3. Single-Cell Multi-Omics: Applied to a mouse gastrulation atlas to align complex single-cell transcriptomic and epigenetic data along a developmental pseudotime axis.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Identifying temporal and spatial patterns of variation from multimodal data using MEFISTO"
    ]
  },
  {
    "objectID": "multi-omics/velten_2022_35027765.html#key-results-and-capabilities",
    "href": "multi-omics/velten_2022_35027765.html#key-results-and-capabilities",
    "title": "Identifying temporal and spatial patterns of variation from multimodal data using MEFISTO",
    "section": "Key Results and Capabilities",
    "text": "Key Results and Capabilities\n\nInformed Dimensionality Reduction\nIn all applications, MEFISTO successfully found latent factors that were biologically relevant and showed a smooth progression across the given time points or spatial coordinates.\n\n\nInterpolation and Imputation\nThe GP prior allows MEFISTO to perform robust interpolation—predicting factor values and corresponding omics data for unobserved time points or spatial locations. This is crucial for filling gaps in time-series experiments.\n\n\nData Alignment\nMEFISTO’s ability to identify underlying common factors makes it excellent for aligning multiple related datasets. For instance, it can align single-cell data from different samples or studies onto a common developmental trajectory, identifying the shared underlying factors of biological variation.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Identifying temporal and spatial patterns of variation from multimodal data using MEFISTO"
    ]
  },
  {
    "objectID": "multi-omics/velten_2022_35027765.html#conclusions-and-significance",
    "href": "multi-omics/velten_2022_35027765.html#conclusions-and-significance",
    "title": "Identifying temporal and spatial patterns of variation from multimodal data using MEFISTO",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nMEFISTO is a versatile and essential tool for the analysis of modern biological data that contains temporal or spatial structure. By incorporating Gaussian Process priors into the factor analysis framework, it overcomes the independence assumption of classical methods.\nMEFISTO’s capabilities in spatio-temporally informed dimensionality reduction, interpolation, and multi-dataset alignment make it a powerful method for extracting meaningful, smooth biological patterns from complex single-cell and multi-omics atlases.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Identifying temporal and spatial patterns of variation from multimodal data using MEFISTO"
    ]
  },
  {
    "objectID": "multi-omics/ryan_2024_39177104.html",
    "href": "multi-omics/ryan_2024_39177104.html",
    "title": "Multi-Omic Graph Diagnosis (MOGDx): a data integration tool to perform classification tasks for heterogeneous diseases",
    "section": "",
    "text": "PubMed: 39177104 DOI: 10.1093/bioinformatics/btae523 Overview generated by: Gemini 2.5 Flash, 27/11/2025",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Multi-Omic Graph Diagnosis (MOGDx): a data integration tool to perform classification tasks for heterogeneous diseases"
    ]
  },
  {
    "objectID": "multi-omics/ryan_2024_39177104.html#background-and-objective",
    "href": "multi-omics/ryan_2024_39177104.html#background-and-objective",
    "title": "Multi-Omic Graph Diagnosis (MOGDx): a data integration tool to perform classification tasks for heterogeneous diseases",
    "section": "Background and Objective",
    "text": "Background and Objective\nThe complexity and heterogeneity of human diseases (e.g., in cancer or psychiatric disorders) make precise diagnosis and treatment challenging. Multi-omics data offers the opportunity to redefine these diseases at a more granular, molecular level. However, existing integrative machine learning methods often face limitations in scalability, oversimplification of biological relationships, and effectively handling missing data.\nThis paper introduces Multi-Omic Graph Diagnosis (MOGDx), a flexible data integration tool that leverages Graph Neural Networks (GNNs) to perform robust classification tasks for heterogeneous diseases by capturing complex, non-linear relationships across multiple omics layers.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Multi-Omic Graph Diagnosis (MOGDx): a data integration tool to perform classification tasks for heterogeneous diseases"
    ]
  },
  {
    "objectID": "multi-omics/ryan_2024_39177104.html#methods-the-mogdx-framework",
    "href": "multi-omics/ryan_2024_39177104.html#methods-the-mogdx-framework",
    "title": "Multi-Omic Graph Diagnosis (MOGDx): a data integration tool to perform classification tasks for heterogeneous diseases",
    "section": "Methods: The MOGDx Framework",
    "text": "Methods: The MOGDx Framework\nMOGDx is a supervised learning framework that integrates diverse omics data types (like gene expression, DNA methylation) by modeling the study cohort as a graph.\n\nGraph Neural Network Integration\n\nNodes and Features: Each patient or sample is represented as a node in the graph. The various omics data (e.g., expression levels) are used as the initial feature vectors for each patient node.\nEdges (Similarity): The edges (connections) between patient nodes are determined by calculating a measure of molecular similarity across all omics data types (similar to Similarity Network Fusion).\nGraph Convolutional Network (GCN): The core of MOGDx is a GCN. This GNN propagates information across the patient-similarity graph, enabling the model to learn complex, non-linear dependencies both within and between the omics layers. This process captures subtle patterns of heterogeneity shared across modalities, which is then used for the classification task (e.g., predicting disease subtype).\n\n\n\nData Robustness\nThe GNN architecture allows MOGDx to be robust in handling missing data, which is a critical feature for real-world multi-omics cohorts where not all measurements are available for every patient.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Multi-Omic Graph Diagnosis (MOGDx): a data integration tool to perform classification tasks for heterogeneous diseases"
    ]
  },
  {
    "objectID": "multi-omics/ryan_2024_39177104.html#key-results-and-findings",
    "href": "multi-omics/ryan_2024_39177104.html#key-results-and-findings",
    "title": "Multi-Omic Graph Diagnosis (MOGDx): a data integration tool to perform classification tasks for heterogeneous diseases",
    "section": "Key Results and Findings",
    "text": "Key Results and Findings\nMOGDx was applied to several public cancer cohorts (including TCGA data for Glioblastoma, Lung Adenocarcinoma, and Lung Squamous Cell Carcinoma).\n\nSuperior Classification: MOGDx demonstrated superior classification accuracy compared to leading non-GNN multi-omics integration methods and single-omics models, validating the strength of the graph-based approach in learning complex patient relationships.\nBiomarker Identification: The framework facilitates the identification of the specific molecular features (biomarkers) that are most critical in distinguishing the disease classes, by analyzing the feature weights learned in the GNN layers.\nMissing Data Performance: The model maintained high classification performance even when substantial portions of the omics data were missing, confirming its robustness for real-world applications.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Multi-Omic Graph Diagnosis (MOGDx): a data integration tool to perform classification tasks for heterogeneous diseases"
    ]
  },
  {
    "objectID": "multi-omics/ryan_2024_39177104.html#conclusions-and-significance",
    "href": "multi-omics/ryan_2024_39177104.html#conclusions-and-significance",
    "title": "Multi-Omic Graph Diagnosis (MOGDx): a data integration tool to perform classification tasks for heterogeneous diseases",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nMOGDx offers a significant methodological advancement for multi-omics data integration using a flexible and robust Graph Neural Network approach. By effectively modeling patient relationships and learning complex cross-omics patterns, MOGDx provides a powerful tool for improving the classification accuracy of heterogeneous diseases and accelerating the discovery of precision biomarkers.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Multi-Omic Graph Diagnosis (MOGDx): a data integration tool to perform classification tasks for heterogeneous diseases"
    ]
  },
  {
    "objectID": "multi-omics/singh_2019_30657866.html",
    "href": "multi-omics/singh_2019_30657866.html",
    "title": "DIABLO: an integrative approach for identifying key molecular drivers from multi-omics assays",
    "section": "",
    "text": "PubMed: 30657866 DOI: 10.1093/bioinformatics/bty1054 Overview generated by: Gemini 2.5 Flash, 27/11/2025",
    "crumbs": [
      "multi-omics",
      "Papers",
      "DIABLO: an integrative approach for identifying key molecular drivers from multi-omics assays"
    ]
  },
  {
    "objectID": "multi-omics/singh_2019_30657866.html#background-and-objective",
    "href": "multi-omics/singh_2019_30657866.html#background-and-objective",
    "title": "DIABLO: an integrative approach for identifying key molecular drivers from multi-omics assays",
    "section": "Background and Objective",
    "text": "Background and Objective\nThe challenge in analyzing modern multi-omics data is the high dimensionality of individual datasets and the difficulty in integrating these diverse data types (e.g., transcriptomics, proteomics, metabolomics) while accounting for the shared biological signal across them. Current methods often integrate data post-analysis, neglecting the opportunity to jointly identify features that drive biological differences.\nThis paper introduces DIABLO (Data Integration Analysis for Biomarker discovery using Latent variable approaches for Omics datasets), a novel computational method designed to: 1. Perform supervised integration of multiple heterogeneous omics datasets. 2. Simultaneously identify a minimal set of key molecular features (biomarkers) that are highly correlated across omics types and maximally associated with a specific outcome variable (e.g., disease status, clinical subtype).",
    "crumbs": [
      "multi-omics",
      "Papers",
      "DIABLO: an integrative approach for identifying key molecular drivers from multi-omics assays"
    ]
  },
  {
    "objectID": "multi-omics/singh_2019_30657866.html#methods-the-diablo-framework",
    "href": "multi-omics/singh_2019_30657866.html#methods-the-diablo-framework",
    "title": "DIABLO: an integrative approach for identifying key molecular drivers from multi-omics assays",
    "section": "Methods: The DIABLO Framework",
    "text": "Methods: The DIABLO Framework\n\nCore Algorithm\nDIABLO is based on Partial Least Squares (PLS), a method for simultaneous dimension reduction and feature selection. Specifically, it uses a generalized form of PLS called multi-block PLS (MB-PLS) or Generalised Canonical Correlation Analysis (GCCA).\n\n\nSupervised Integration\nUnlike unsupervised integration methods, DIABLO is supervised—it incorporates an outcome variable (e.g., healthy vs. disease) into the model. The method identifies latent components (similar to principal components) that maximize the covariance between: 1. The multi-omics datasets. 2. The multi-omics datasets and the outcome variable.\n\n\nFeature Selection and Biomarker Discovery\nDIABLO employs a sparse penalty (using an \\(L_1\\) penalty, similar to LASSO regression) within its iterative algorithm. This forces the latent components to be linear combinations of only a small number of features. This feature selection step is critical for: * Dimension Reduction: Reducing the number of irrelevant features. * Biomarker Identification: Pinpointing the most important “key molecular drivers” that explain the variation in the biological outcome across the different omics layers.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "DIABLO: an integrative approach for identifying key molecular drivers from multi-omics assays"
    ]
  },
  {
    "objectID": "multi-omics/singh_2019_30657866.html#key-results-and-application",
    "href": "multi-omics/singh_2019_30657866.html#key-results-and-application",
    "title": "DIABLO: an integrative approach for identifying key molecular drivers from multi-omics assays",
    "section": "Key Results and Application",
    "text": "Key Results and Application\n\nPerformance\nThe authors demonstrated that DIABLO outperformed several state-of-the-art multi-omics integration and classification methods (e.g., iClusterPlus, multi-kernel learning) in terms of: * Classification Accuracy: Achieving higher predictive performance for distinguishing between sample groups. * Biological Relevance: Identifying smaller, more biologically coherent subsets of features that were consistently selected across different omics blocks.\n\n\nCase Study: Breast Cancer\nDIABLO was applied to a multi-omics breast cancer dataset (transcriptomics, metabolomics, miRNA) to distinguish between clinical subtypes. * Integrated Signatures: DIABLO identified a signature of features that were highly correlated across the omics layers, including specific genes (mRNA), microRNAs, and metabolites. * Key Drivers: The method pinpointed known and novel molecular drivers (e.g., genes and pathways related to cell cycle and proliferation) whose concerted changes across the different omics layers were responsible for the differences between cancer subtypes.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "DIABLO: an integrative approach for identifying key molecular drivers from multi-omics assays"
    ]
  },
  {
    "objectID": "multi-omics/singh_2019_30657866.html#conclusions-and-significance",
    "href": "multi-omics/singh_2019_30657866.html#conclusions-and-significance",
    "title": "DIABLO: an integrative approach for identifying key molecular drivers from multi-omics assays",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nDIABLO is a powerful, flexible, and scalable tool for supervised integration of multi-omics data. Its ability to simultaneously perform dimension reduction, feature selection, and association with a clinical outcome makes it particularly well-suited for biomarker discovery.\nBy identifying small, highly relevant, and correlated sets of features across different molecular layers, DIABLO provides valuable insights into the key molecular drivers underlying complex biological states or disease phenotypes, aiding in the transition toward personalized medicine.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "DIABLO: an integrative approach for identifying key molecular drivers from multi-omics assays"
    ]
  },
  {
    "objectID": "multi-omics/dugourd_2021_33502086.html",
    "href": "multi-omics/dugourd_2021_33502086.html",
    "title": "Causal integration of multi-omics data with prior knowledge to generate mechanistic hypotheses",
    "section": "",
    "text": "PubMed: 33502086 DOI: 10.15252/msb.20209703 Overview generated by: Gemini 2.5 Flash, 27/11/2025",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Causal integration of multi-omics data with prior knowledge to generate mechanistic hypotheses"
    ]
  },
  {
    "objectID": "multi-omics/dugourd_2021_33502086.html#background-and-objective",
    "href": "multi-omics/dugourd_2021_33502086.html#background-and-objective",
    "title": "Causal integration of multi-omics data with prior knowledge to generate mechanistic hypotheses",
    "section": "Background and Objective",
    "text": "Background and Objective\nThe rise of multi-omics technologies provides vast datasets that capture different layers of molecular information (e.g., phosphorylation, transcription, metabolism). However, methods for integrating these diverse data types to systematically extract mechanistic hypotheses—specifically, how a change in one molecular layer causally affects another—are limited.\nThis paper introduces COSMOS (Causal Oriented Search of Multi-Omics Space), a novel computational method designed to: 1. Integrate quantitative data from three omics layers: phosphoproteomics, transcriptomics, and metabolomics. 2. Combine this data with extensive prior knowledge contained within signaling, metabolic, and gene regulatory networks. 3. Infer the causal relationships between molecular activities across these layers to generate mechanistic hypotheses.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Causal integration of multi-omics data with prior knowledge to generate mechanistic hypotheses"
    ]
  },
  {
    "objectID": "multi-omics/dugourd_2021_33502086.html#methods-the-cosmos-framework",
    "href": "multi-omics/dugourd_2021_33502086.html#methods-the-cosmos-framework",
    "title": "Causal integration of multi-omics data with prior knowledge to generate mechanistic hypotheses",
    "section": "Methods: The COSMOS Framework",
    "text": "Methods: The COSMOS Framework\n\nCausal Integration Principle\nCOSMOS’s core strength lies in its use of a large, curated prior knowledge network (PKN) that encompasses relationships between various molecular entities, including transcription factors, kinases, genes, and metabolites. The method works by performing two key causal steps:\n\nActivity Inference: COSMOS first calculates the estimated activity of key molecular regulators from the measured omics data:\n\nKinase Activity (from phosphoproteomics)\nTranscription Factor (TF) Activity (from transcriptomics)\nEnzyme/Pathway Activity (from metabolomics)\n\nCausal Scoring and Hypothesis Generation: The inferred activity changes are then mapped onto the PKN. COSMOS calculates a causal score for every potential regulatory link between a regulator (e.g., a kinase) and its target (e.g., a TF or a metabolite) by evaluating whether the measured change in the regulator’s activity is consistent with the measured change in its target’s activity, considering the known network topology.\n\n\n\nApplication: Renal Cell Carcinoma (RCC)\nCOSMOS was applied to multi-omics data from a drug screen on Clear Cell Renal Cell Carcinoma (ccRCC) cells treated with various anti-cancer compounds. This allowed the authors to investigate how drug perturbations causally rewire the signaling, gene regulation, and metabolic networks of cancer cells.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Causal integration of multi-omics data with prior knowledge to generate mechanistic hypotheses"
    ]
  },
  {
    "objectID": "multi-omics/dugourd_2021_33502086.html#key-results-and-mechanistic-hypotheses",
    "href": "multi-omics/dugourd_2021_33502086.html#key-results-and-mechanistic-hypotheses",
    "title": "Causal integration of multi-omics data with prior knowledge to generate mechanistic hypotheses",
    "section": "Key Results and Mechanistic Hypotheses",
    "text": "Key Results and Mechanistic Hypotheses\n\nNetwork Rewiring in ccRCC\nCOSMOS successfully inferred activity changes in known regulatory pathways, demonstrating the effect of drug perturbations on the signaling, transcriptional, and metabolic machinery of the cancer cells.\n\n\nCausal Hypotheses Generated\nThe method generated specific, testable mechanistic hypotheses that connect the omics layers: 1. Signaling to Transcription (Kinase → TF): COSMOS identified the TSSK4 kinase as a causal regulator of the ZHX2 transcription factor following treatment with a CDK inhibitor. This suggested a specific kinase-TF cascade that could be important for therapeutic response. 2. Transcription to Metabolism (TF → Metabolite): The method also revealed a causal link from the ZHX2 TF to the regulation of GAPDH enzyme activity (a key player in glycolysis), which in turn regulated specific metabolites like pyruvate. This closed the loop, linking signaling through transcription to metabolic output. 3. Discovery of Novel Kinases: Through its integration, COSMOS implicated several previously uncharacterized kinases (e.g., CDK11) as potential drivers of the transcriptional response to drug treatment, suggesting novel targets for further investigation.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Causal integration of multi-omics data with prior knowledge to generate mechanistic hypotheses"
    ]
  },
  {
    "objectID": "multi-omics/dugourd_2021_33502086.html#conclusions-and-significance",
    "href": "multi-omics/dugourd_2021_33502086.html#conclusions-and-significance",
    "title": "Causal integration of multi-omics data with prior knowledge to generate mechanistic hypotheses",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nCOSMOS represents a significant advance in systems biology by offering a robust, transparent, and biologically constrained approach to causal integration of multi-omics data.\nBy leveraging extensive prior knowledge, it moves beyond simple association to generate specific, directional, and testable mechanistic hypotheses that bridge the gap between different omics layers. This capability is critical for understanding complex diseases like cancer and for accelerating the discovery of novel therapeutic targets and biomarkers.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Causal integration of multi-omics data with prior knowledge to generate mechanistic hypotheses"
    ]
  },
  {
    "objectID": "statistics/wasserstein_2016_26820252.html",
    "href": "statistics/wasserstein_2016_26820252.html",
    "title": "The ASA’s Statement on p-Values: Context, Process, and Purpose",
    "section": "",
    "text": "DOI: 10.1080/00031305.2016.1154108 Overview generated by: Gemini 2.5 Flash, 26/11/2025",
    "crumbs": [
      "statistics",
      "Papers",
      "The ASA's Statement on p-Values: Context, Process, and Purpose"
    ]
  },
  {
    "objectID": "statistics/wasserstein_2016_26820252.html#key-findings-principles-for-the-proper-use-of-the-p-value",
    "href": "statistics/wasserstein_2016_26820252.html#key-findings-principles-for-the-proper-use-of-the-p-value",
    "title": "The ASA’s Statement on p-Values: Context, Process, and Purpose",
    "section": "Key Findings: Principles for the Proper Use of the p-Value",
    "text": "Key Findings: Principles for the Proper Use of the p-Value\nThis article presents and explains the American Statistical Association’s (ASA) Statement on p-Values and Statistical Significance, the first-ever policy statement from the ASA specifically addressing a foundational issue of statistical inference. The statement was prompted by the widespread misuse and misinterpretation of the p-value across all fields of science, which contributes to the crisis of irreproducible research.\n\nSix Core Principles for p-Values\nThe statement outlines six key principles, aiming to promote better practice and curb common errors:\n\nP-values can indicate how incompatible the data are with a specified statistical model.\n\nThe p-value measures the incompatibility between the data and the assumed statistical model (usually the null hypothesis). A small p-value indicates that the data are unlikely under the null model.\n\nP-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.\n\nThis directly addresses the most common and disastrous misinterpretation: that a low p-value (e.g., P=0.01) means the null hypothesis has only a 1% chance of being true. P-values relate to data given the model, not the probability of the model given the data.\n\nScientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\n\nThe ASA strongly discourages the practice of dichotomizing results into “statistically significant” and “not statistically significant.” This threshold thinking can lead to flawed decisions, ignoring the importance of effect size, context, and other evidence.\n\nProper inference requires full reporting and transparency.\n\nThe selective reporting of only “significant” results (known as P-hacking or publication bias) renders the reported p-values meaningless. All analyses, assumptions, and findings, regardless of the p-value, must be disclosed.\n\nA p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\n\nStatistical significance is often confused with substantive (clinical, practical, or scientific) importance. A very large study can produce a statistically significant p-value for a trivial effect, while a small study might fail to find significance for a massive, important effect. The focus should be on effect magnitude.\n\nBy itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.\n\nScientific reasoning requires more than just a p-value. It demands contextual knowledge, common sense, study design quality, data reliability, and integration with external evidence. Other statistical measures, particularly Confidence Intervals (CIs) and Bayesian methods, should be used to provide a richer understanding of the evidence.\n\n\n\n\nPurpose of the Statement\nThe statement is not intended to be a simple replacement for null hypothesis significance testing, but rather a step toward a “post p&lt;0.05 era” where statistical methods are used more thoughtfully. It calls for a move toward better scientific practice characterized by open communication, transparent methodology, and a focus on effect size, precision (CIs), and context.",
    "crumbs": [
      "statistics",
      "Papers",
      "The ASA's Statement on p-Values: Context, Process, and Purpose"
    ]
  },
  {
    "objectID": "statistics/wasserstein_2019_1583913.html",
    "href": "statistics/wasserstein_2019_1583913.html",
    "title": "Moving to a World Beyond \\(p < 0.05\\)",
    "section": "",
    "text": "PubMed: Not Indexed (The American Statistician) DOI: 10.1080/00031305.2019.1583913 Overview generated by: Gemini 2.5 Flash, 26/11/2025",
    "crumbs": [
      "statistics",
      "Papers",
      "Moving to a World Beyond $p < 0.05$"
    ]
  },
  {
    "objectID": "statistics/wasserstein_2019_1583913.html#key-findings-the-case-against-dichotomous-statistical-significance",
    "href": "statistics/wasserstein_2019_1583913.html#key-findings-the-case-against-dichotomous-statistical-significance",
    "title": "Moving to a World Beyond \\(p < 0.05\\)",
    "section": "Key Findings: The Case Against Dichotomous Statistical Significance",
    "text": "Key Findings: The Case Against Dichotomous Statistical Significance\nThis highly influential editorial, which introduced a special issue of The American Statistician containing 43 papers on the topic, marks a critical step in the statistical community’s effort to reform scientific practice. It explicitly calls for an end to the culture of dichotomous thinking based solely on the threshold of \\(p &lt; 0.05\\). The authors argue that declaring a result “statistically significant” based on an arbitrary cutoff is counterproductive and has fueled the reproducibility crisis.\n\nThe Problem with Dichotomization\nThe fundamental issue lies in the over-simplification of complex statistical results into a binary “significant/non-significant” outcome:\n\nArbitrary Cutoff: The \\(p &lt; 0.05\\) threshold is arbitrary. Treating a \\(p=0.049\\) result as fundamentally different from a \\(p=0.051\\) result leads to illogical conclusions and flawed decision-making.\nExaggerated Confidence: Declaring a result “statistically significant” implies a certainty or importance that the p-value alone does not justify. It often leads researchers and the public to mistake statistical significance for scientific or clinical importance.\nFueling Bias: The dichotomous framework is the root cause of poor research practices like P-hacking (data-dependent manipulation to cross the threshold) and publication bias (selective reporting of results that pass the threshold). These biases inflate the true rate of false positives in the literature.\n\n\n\nThe Solution: Embracing a Continuous View of Evidence\nThe authors propose a simple, yet profound, shift in reporting and thinking:\n\nAbandon “Statistical Significance”: The term “statistically significant” and its binary cousin “non-significant” should be retired from scientific discourse.\nFocus on Compatibility: Researchers should instead report the p-value as a measure of incompatibility between the data and the assumed statistical model (usually the null hypothesis). They must interpret this value continuously, considering the p-value’s proximity to zero as a gradient of evidence against the null.\nEmphasize Magnitude and Precision: The core focus of reporting should be on the effect size and its uncertainty, which is best communicated via Confidence Intervals (CIs) or Credible Intervals (from Bayesian analysis). CIs show the range of plausible effects compatible with the data.\nIntegrate Context: Conclusions must be based on the entirety of the evidence, including the context of the research, the quality of the study design, external knowledge, and the costs/benefits of potential actions, not just the p-value.\n\n\n\nThe Role of Prediction Intervals and False Discovery Rates\nThe article encourages the use of various other tools that provide a more complete picture of the evidence, such as:\n\nPrediction Intervals: Show the range of values expected for a future observation.\nFalse Discovery Rates (FDR) and False Positive Risks (FPR): Help quantify the probability that a “significant” finding is actually false, which is often much higher than the p-value suggests.\n\nThe statement concludes by asserting that the scientific community needs to move toward a “post p &lt; 0.05 era” where thoughtful interpretation replaces rigid decision rules.",
    "crumbs": [
      "statistics",
      "Papers",
      "Moving to a World Beyond $p < 0.05$"
    ]
  },
  {
    "objectID": "statistics/ciolino_2013_24138438.html",
    "href": "statistics/ciolino_2013_24138438.html",
    "title": "Covariate Imbalance and Adjustment for Logistic Regression Analysis of Clinical Trial Data",
    "section": "",
    "text": "PubMed: 24138438 DOI: 10.1080/10543406.2013.834912 Overview generated by: Gemini 2.5 Flash, 26/11/2025",
    "crumbs": [
      "statistics",
      "Papers",
      "Covariate Imbalance and Adjustment for Logistic Regression Analysis of Clinical Trial Data"
    ]
  },
  {
    "objectID": "statistics/ciolino_2013_24138438.html#key-findings-the-necessity-of-covariate-adjustment-in-logistic-regression",
    "href": "statistics/ciolino_2013_24138438.html#key-findings-the-necessity-of-covariate-adjustment-in-logistic-regression",
    "title": "Covariate Imbalance and Adjustment for Logistic Regression Analysis of Clinical Trial Data",
    "section": "Key Findings: The Necessity of Covariate Adjustment in Logistic Regression",
    "text": "Key Findings: The Necessity of Covariate Adjustment in Logistic Regression\nThis paper uses simulation to quantify the benefits of covariate adjustment in the analysis of randomized controlled trials (RCTs) with a binary outcome, specifically focusing on models using logistic regression. The findings highlight that, unlike linear regression, unadjusted and adjusted treatment effect estimates in logistic regression are generally not equivalent, making adjustment a necessary step for precise inference.\n\nInequivalence of Adjusted vs. Unadjusted Estimates\nThe central statistical problem addressed is the non-collapsibility of the Odds Ratio (OR) in logistic regression:\n\nUnadjusted Bias: In the presence of influential covariates, the unadjusted OR (which estimates the marginal effect) is typically a biased estimate of the conditional OR (the effect after controlling for covariates), even if the groups are perfectly balanced due to randomization.\nImpact of Imbalance: While randomization ensures that any imbalance is due to chance, the presence of an influential, imbalanced covariate can further exacerbate the difference between the marginal (unadjusted) and conditional (adjusted) treatment effects.\n\n\n\nQuantifying the Benefit of Adjustment\nThe simulation study quantified the statistical benefits of using adjusted analysis:\n\nIncreased Power: Adjusting for important prognostic covariates significantly increases statistical power to detect a true treatment effect. Simulations demonstrated power benefits of up to 17.5% for log-normally distributed covariates and up to 9.4% for normally distributed covariates when the covariate effect was strong.\nReduced Bias: Adjustment helps reduce the bias of the treatment effect estimate with respect to the conditional treatment effect (the effect being targeted in the adjusted model).\n\n\n\nRecommendations for Clinical Trial Analysis\nThe authors reinforce established guidelines and provide practical advice for analysis:\n\nPre-specification: Following International Conference on Harmonization (ICH) guidelines, covariate adjustment should be pre-specified in the study protocol. Unplanned adjustments should be considered secondary analyses.\nBalance is Not Enough: If adjustment is not possible or unplanned, achieving strong baseline balance in continuous covariates can mitigate some of the shortcomings (lower power, greater potential for bias) of unadjusted analyses, but it cannot fully eliminate the inherent difference between the marginal and conditional ORs due to the non-collapsibility of the logistic model.\nFocus on Efficiency: The primary reason for adjustment is not to correct a failure of randomization, but to increase the efficiency (power) of the analysis by accounting for known sources of variation in the outcome.",
    "crumbs": [
      "statistics",
      "Papers",
      "Covariate Imbalance and Adjustment for Logistic Regression Analysis of Clinical Trial Data"
    ]
  },
  {
    "objectID": "statistics/index.html",
    "href": "statistics/index.html",
    "title": "statistics",
    "section": "",
    "text": "Can algorithms replace expert knowledge for causal inference? A case study on novice use of causal discovery\n\n\n\nObjective: To test whether causal discovery algorithms used by novices could replace expert knowledge in selecting covariates for estimating the known null effect of placebo adherence on mortality in the CDP trial.\nKey Finding: Adjustment sets derived from causal discovery algorithms resulted in more residual bias in complex time-varying analyses compared to expert-selected sets.\nConclusion: Due to high subjectivity in selecting algorithms/parameters and resolving complex graph outputs, the authors do not recommend novice use of causal discovery without an expert’s guidance.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nThe Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis\n\n\n\nCore Argument: This paper argues that retrospective power calculations (or observed power), performed after a study yields a non-significant result, are fundamentally flawed and should not be used to aid in interpretation.\nThe Flaw (Circularity): Retrospective power is a simple monotonic transformation of the p-value; it provides no new information. A non-significant test will always have low retrospective power, as both measures are driven by the large variance or small observed effect.\nRecommended Alternative: To interpret a non-significant finding, researchers should focus on the confidence interval, which reveals the range of plausible true effect sizes and indicates whether biologically or clinically important effects have been reasonably ruled out.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nStatistical tests, P values, confidence intervals, and power: a guide to misinterpretations\n\n\n\nCore Principle: This influential essay details the pervasive misinterpretations and abuse of fundamental statistics—P-values, Confidence Intervals (CIs), and statistical power—in scientific literature.\nP-value Clarification: The authors correct 12 common misconceptions, asserting that the P-value is not the probability of the null hypothesis (\\(H_0\\)) being true, but rather the probability of the observed data (or more extreme) given that \\(H_0\\) is true.\nMisuse of Significance: The essay strongly condemns the practice of drawing dichotomous conclusions based on an arbitrary threshold (e.g., P&lt;0.05), which falsely implies certainty and hinders scientific progress.\nRecommendation: Researchers are urged to shift focus from the P-value to the effect magnitude and its precision, which are better conveyed through Confidence Intervals, and to integrate statistical results with contextual, external evidence.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nTriangulation in aetiological epidemiology: Approaches to causal inference\n\n\n\nCore Principle: This article formally advocates for Triangulation in aetiological epidemiology, defined as the integration of results from multiple research approaches where each approach possesses uncorrelated sources of bias.\nCausal Inference: If different methods (e.g., observational studies, Mendelian Randomization, and clinical trials) all converge on the same causal conclusion, confidence in that finding is significantly strengthened, especially when method-specific biases would predict opposing results.\nAddressing Inconsistency: When results are inconsistent, the framework helps identify which source of bias (e.g., confounding, reverse causation) is most likely distorting a given approach, thereby guiding the direction of future, more focused research.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nCausal Inference Is Not Just a Statistics Problem\n\n\n\nCore Argument: This article argues that causal inference relies primarily on study design and domain knowledge to establish untestable assumptions (like exchangeability and no unmeasured confounding), with statistical methods serving only to quantify the effect under those assumptions.\nDesign Trumps Analysis: The authors emphasize that a causal estimate is invalid if the study design fails to account for critical (especially unmeasured) confounders, regardless of the sophistication of the statistical modeling used.\nThe Role of Causal Diagrams: Identifying which variables to control is a causal question, not a statistical one, necessitating the use of Directed Acyclic Graphs (DAGs) and scientific expertise to avoid introducing bias by controlling for mediators or colliders.\nConclusion: Teaching and practicing causal inference requires prioritizing the formulation of a causal question and the design of the study before the application of statistical tools.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nSpurious interaction as a result of categorization\n\n\n\nCore Argument: Categorizing continuous exposure variables in regression models, a common practice in epidemiology, can introduce spurious interaction effects that do not exist in the original continuous scale.\nMechanism: This phenomenon is demonstrated analytically and is interpreted as a form of differential measurement error or residual confounding caused by the categorization process.\nConclusion: Spurious interaction is induced unless the correlated variables are dichotomized at the median or are uncorrelated. The paper strongly recommends avoiding categorization to ensure valid results, suggesting non-parametric models as a preferred alternative.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nLarge-scale composite hypothesis testing procedure for omics data analyses\n\n\n\nNovel method (qch_copula) for testing composite hypotheses across multiple traits/omics levels using mixture models with copula functions\nControls Type I error effectively while achieving superior detection power compared to 8 state-of-the-art methods across diverse correlation scenarios\nMemory-efficient implementation enables analysis of 105-106 markers with up to 20 traits, validated through psychiatric disorder pleiotropy and plant virus resistance studies\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nComparison of Imputation Strategies for Incomplete Longitudinal Data in Life-Course Epidemiology\n\n\n\nTopic: A study comparing the performance of three Multiple Imputation (MI) methods for handling incomplete longitudinal data in life-course epidemiology, focusing on the effect of longitudinal depressive symptoms on mortality.\nMethods Compared: Normal Linear Regression (MICE), Predictive Mean Matching (PMM), and Variable-Tailored Specification were tested under nine scenarios varying missingness rate and mechanism (MCAR, MAR, MNAR).\nFinding: All methods showed similar levels of bias in estimating the causal effect (Hazard Ratios), but Predictive Mean Matching (PMM) was identified as the most appealing strategy due to its consistently low root mean square error (RMSE) and competitive computation time.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nThe cost of dichotomising continuous variables\n\n\n\nAvoidable Costs: The practice of dichotomizing continuous variables for statistical analysis leads to a substantial loss of statistical power and an inflation of the Type I error rate (false positives) in multivariable models.\nFlawed Interpretation: Dichotomization uses arbitrary or unstable cut-points, provides misleading effect estimates, and ignores the true shape of the relationship across the continuous scale.\nRecommendation: Continuous variables should be analyzed on their original scale (or using methods like fractional polynomials/splines) to preserve information and avoid these serious statistical drawbacks.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nTESTING FOR BASELINE BALANCE IN CLINICAL TRIALS\n\n\n\nCore Principle: This paper argues that performing tests of baseline homogeneity (p-value tests) in randomized controlled trials (RCTs) is philosophically unsound, of no practical value, and potentially misleading, as randomization ensures comparability in expectation.\nThe Flaw: A non-significant baseline difference does not prove balance (it reflects low power), and a significant difference is merely a chance event that does not invalidate the randomization.\nRecommendation: The authors recommend that researchers must abandon baseline testing and, instead, identify known prognostic variables in the trial-plan and fit them in an Analysis of Covariance (ANCOVA) model regardless of their baseline p-value, which increases the precision and power of the final treatment effect estimate.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nSifting the evidence—what’s wrong with significance tests?\n\n\n\nCore Critique: This classic commentary criticizes the widespread misuse and over-reliance on statistical significance tests (p-values) in medical literature, arguing that this practice fundamentally distorts evidence.\nCentral Problem: The authors identify publication bias (accentuating positive results over null results) as the key factor causing the mistaken publication of chance findings as real effects, thereby eroding confidence in science.\nRecommendation: Researchers should shift their focus from the binary question of whether P &lt; 0.05 to assessing the direction and the magnitude of the effect, specifically asking if the effect is of public health or clinical importance.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nAn efficient, not-only-linear correlation coefficient based on clustering\n\n\n\nNovel Statistic: The paper introduces the Clustering-based Correlation Coefficient (CCC), a computationally efficient measure that detects both linear and non-linear relationships, unlike traditional coefficients like Pearson’s \\(r\\).\nMechanism: CCC operates by assessing how well the marginal information of two variables aligns with the clusters formed by their joint distribution, allowing it to capture complex, non-monotonic associations.\nBiological Utility: In an application to human gene expression data, CCC successfully identified biologically meaningful non-linear patterns, including those driven by sex differences, that were missed by standard linear methods.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nStep away from stepwise\n\n\n\nCore Argument: This report critiques stepwise regression, arguing it is a flawed statistical method, especially in the context of Big Data, because it uses statistical significance to select variables.\nKey Flaw: Stepwise procedures may include nuisance variables that are coincidentally significant and exclude true explanatory variables that happen to be non-significant, leading to models that fit data well in-sample but perform poorly out-of-sample.\nBig Data Effect: The belief that stepwise is more useful with more variables is false; Big Data exacerbates its failings by increasing the chance of selecting spurious correlations, causing a dramatic deterioration in out-of-sample predictive accuracy.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nCovariate Imbalance and Adjustment for Logistic Regression Analysis of Clinical Trial Data\n\n\n\nCore Problem: This study demonstrates that, for binary outcomes analyzed with logistic regression, the unadjusted Odds Ratio (OR) is generally a biased estimate of the conditional OR due to the non-collapsibility of the OR, even in a perfectly randomized trial.\nKey Finding: Adjusting for prognostic covariates in logistic regression is critical, as it significantly increases statistical power (by up to 17.5% in simulations) and reduces bias in the treatment effect estimate.\nRecommendation: While good baseline balance in covariates is desirable, it never fully alleviates the shortcomings of unadjusted analyses in the logistic setting; researchers should pre-specify and use covariate adjustment for prognostic variables to ensure the most precise and efficient estimate.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nSynthesizing Subject-matter Expertise for Variable Selection in Causal Effect Estimation: A Case Study\n\n\n\nObjective: To empirically assess different strategies for Directed Acyclic Graph (DAG) creation and the resulting minimal adjustment sets for estimating a known causal effect (adherence on mortality in the CDP trial).\nKey Finding (Efficiency): The results confirm that including nonconfounding prognostic factors (outcome predictors) significantly reduced variance (increased efficiency) of the causal effect estimate, aligning with theoretical statistical advice.\nRecommendation: Researchers should prioritize the exhaustive identification of all potential outcome prognostic factors as the initial and most crucial step when constructing DAGs for causal effect estimation.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nMoving to a World Beyond \\(p &lt; 0.05\\)\n\n\n\nCore Argument: This editorial calls for the abandonment of the term “statistically significant” and the practice of dichotomizing results based on the arbitrary \\(p &lt; 0.05\\) threshold.\nCentral Problem: Relying on the binary threshold distorts evidence, fuels poor practices like P-hacking and publication bias, and leads to the false equating of statistical significance with scientific importance.\nRecommendation: Researchers must treat the p-value as a continuous measure of incompatibility between the data and the null hypothesis. They should focus their interpretation on the magnitude of the effect and its precision, primarily communicated via Confidence Intervals (CIs).\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nA Structural Description of Biases That Generate Immortal Time\n\n\n\nCore Argument: The true source of immortal time bias is the underlying selection or misclassification of subjects, not the period of event-free time itself.\nTwo Mechanisms: The paper structurally describes two ways the bias arises: 1) Selection bias when eligibility is applied after assignment; and 2) Misclassification bias when assignment is defined using post-eligibility data for strategies indistinguishable at baseline.\nPrevention: The definitive solution is to use Target Trial Emulation to achieve synchronization of eligibility and treatment assignment at the start of follow-up, thereby eliminating the generating biases.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nThe ASA’s Statement on p-Values: Context, Process, and Purpose\n\n\n\nCore Principle: This article presents the American Statistical Association’s (ASA) six core principles for the proper use of the P-value, aiming to correct pervasive misinterpretations in scientific research.\nKey Principles: The ASA states that the P-value does not measure the probability that a hypothesis is true (Principle 2) or the size/importance of an effect (Principle 5). Instead, it measures how incompatible the data are with a statistical model (Principle 1).\nCrucial Admonitions: The statement strongly advises against drawing dichotomous conclusions based solely on a fixed threshold (e.g., P&lt;0.05, Principle 3) and requires full reporting and transparency of all results to combat selective reporting (Principle 4).\nRecommendation: The ASA advocates for a move beyond the single P-value to integrate statistical results with effect sizes, Confidence Intervals, context, and external evidence (Principle 6).\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "statistics",
      "Papers"
    ]
  },
  {
    "objectID": "statistics/sterne_2001_11159626.html",
    "href": "statistics/sterne_2001_11159626.html",
    "title": "Sifting the evidence—what’s wrong with significance tests?",
    "section": "",
    "text": "PubMed: 11159626 DOI: 10.1136/bmj.322.7280.226 Overview generated by: Gemini 2.5 Flash, 26/11/2025",
    "crumbs": [
      "statistics",
      "Papers",
      "Sifting the evidence—what's wrong with significance tests?"
    ]
  },
  {
    "objectID": "statistics/sterne_2001_11159626.html#key-findings-the-misuse-and-limitations-of-statistical-significance",
    "href": "statistics/sterne_2001_11159626.html#key-findings-the-misuse-and-limitations-of-statistical-significance",
    "title": "Sifting the evidence—what’s wrong with significance tests?",
    "section": "Key Findings: The Misuse and Limitations of Statistical Significance",
    "text": "Key Findings: The Misuse and Limitations of Statistical Significance\nThis highly influential article critiques the over-reliance on statistical significance testing (p-values) in medical and epidemiological research, arguing that this practice fundamentally distorts the scientific literature and leads to public skepticism.\n\nThe Central Problem: Publication Bias\nThe primary issue is the medical literature’s strong tendency to accentuate the positive, leading to publication bias (also known as the “file drawer effect”):\n\nSkewed Reporting: Studies with positive outcomes (those achieving statistical significance, typically P&lt;0.05) are far more likely to be published than those with null results (non-significant findings).\nIncreased Chance Findings: This creates a system where a host of purely chance findings are published and subsequently mistaken for real biological or clinical effects. The authors note that, by conventional reasoning, examining 20 associations will produce one “significant at P=0.05” result by chance alone.\nErosion of Trust: The proliferation of inconsistent, purely chance findings contributes significantly to scepticism about medical research and epidemiological studies among the public and practitioners.\n\n\n\nShifting Focus: From P-value to Effect Magnitude\nThe authors argue that focusing solely on whether a P-value crosses the 0.05 threshold misses the point of much medical research, particularly in observational and interventional studies:\n\nNull Hypothesis Relevance: In many epidemiological studies and randomized controlled trials, there is often little reason to expect the true effect to be exactly null. The key issue is not consistency with a strict null hypothesis.\nThe Real Questions: Instead of asking if an effect exists (P-value), researchers should focus on:\n\nWhether the direction of the effect has been reasonably and firmly established.\nWhether the magnitude of the effect is such that it is of public health or clinical importance. A small, statistically significant effect may be clinically irrelevant. A large, non-significant effect may warrant further study.\n\n\n\n\nHistorical Context\nThe critique is contextualized by referencing the founder of statistical significance testing, Ronald Fisher, and the early criticism he received from his colleague F. Yates, who noted that Fisher’s emphasis on significance testing was problematic. The paper implicitly advocates for the use of Confidence Intervals (CIs), which convey both the direction and the magnitude of the effect along with the precision of the estimate.",
    "crumbs": [
      "statistics",
      "Papers",
      "Sifting the evidence—what's wrong with significance tests?"
    ]
  },
  {
    "objectID": "statistics/altman_2006_16675816.html",
    "href": "statistics/altman_2006_16675816.html",
    "title": "The cost of dichotomising continuous variables",
    "section": "",
    "text": "PubMed: 16675816 DOI: 10.1136/bmj.332.7549.1080 Overview generated by: Gemini 2.5 Flash, 26/11/2025",
    "crumbs": [
      "statistics",
      "Papers",
      "The cost of dichotomising continuous variables"
    ]
  },
  {
    "objectID": "statistics/altman_2006_16675816.html#key-finding-dichotomization-of-continuous-variables-is-statistically-detrimental",
    "href": "statistics/altman_2006_16675816.html#key-finding-dichotomization-of-continuous-variables-is-statistically-detrimental",
    "title": "The cost of dichotomising continuous variables",
    "section": "Key Finding: Dichotomization of Continuous Variables is Statistically Detrimental",
    "text": "Key Finding: Dichotomization of Continuous Variables is Statistically Detrimental\nThis article critiques the common practice in clinical research of converting continuous variables (e.g., blood pressure, weight, cholesterol) into binary categories (dichotomization, e.g., “hypertensive” or “not hypertensive”). The authors argue that while this practice is useful for clinical decision-making and data presentation, it is unnecessary for statistical analysis and introduces several serious, avoidable drawbacks.",
    "crumbs": [
      "statistics",
      "Papers",
      "The cost of dichotomising continuous variables"
    ]
  },
  {
    "objectID": "statistics/altman_2006_16675816.html#study-design-and-methods",
    "href": "statistics/altman_2006_16675816.html#study-design-and-methods",
    "title": "The cost of dichotomising continuous variables",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\nThis paper is a Statistics Note (a commentary/review) that uses theoretical arguments and a review of existing statistical literature to demonstrate the flaws of dichotomization.\nThe core argument is based on quantifying the statistical and inferential costs associated with replacing precise continuous data with a simple binary indicator (0 or 1). The analysis focuses on the detrimental impact on statistical power and Type I error control in subsequent analyses, particularly in regression modeling.",
    "crumbs": [
      "statistics",
      "Papers",
      "The cost of dichotomising continuous variables"
    ]
  },
  {
    "objectID": "statistics/altman_2006_16675816.html#results-and-major-drawbacks",
    "href": "statistics/altman_2006_16675816.html#results-and-major-drawbacks",
    "title": "The cost of dichotomising continuous variables",
    "section": "Results and Major Drawbacks",
    "text": "Results and Major Drawbacks\nThe authors identify four main statistical costs associated with dichotomization:\n\n1. Substantial Loss of Statistical Power\nThe primary statistical consequence is a reduction in power to detect a true association or effect. By converting a continuous measure into a binary one, researchers discard a significant amount of information (i.e., the magnitude of a value relative to others). This loss of information is equivalent to conducting a study with a much smaller sample size, making it harder to achieve statistical significance for a genuine effect.\n\n\n2. Inflation of the Type I Error Rate\nWhen a confounding variable is dichotomized in a multivariable model (e.g., logistic regression), it often fails to adequately control for the confounding effect across the full range of the variable. This residual confounding can lead to a substantial inflation of the Type I error rate for other variables in the model, increasing the risk of false-positive findings.\n\n\n3. Arbitrary and Unstable Cut-points\nThe choice of the cut-point used to divide the continuous variable is frequently arbitrary and lacks strong biological justification. Furthermore, attempts to find an “optimal” cut-point based on the observed data are statistically dangerous, as this practice can lead to biased effect estimates (overestimation) and a loss of validity when generalizing results to new data.\n\n\n4. Misleading Effect Estimates\nDichotomization assumes a simple step-function relationship between the variable and the outcome. This ignores the detailed variation within each group and can entirely misrepresent a true biological relationship, especially if that relationship is non-linear across the continuous scale. The resulting effect estimate only represents the difference between the mean values of the two resulting groups, masking the true dose-response curve.",
    "crumbs": [
      "statistics",
      "Papers",
      "The cost of dichotomising continuous variables"
    ]
  },
  {
    "objectID": "statistics/altman_2006_16675816.html#conclusion-and-recommendations",
    "href": "statistics/altman_2006_16675816.html#conclusion-and-recommendations",
    "title": "The cost of dichotomising continuous variables",
    "section": "Conclusion and Recommendations",
    "text": "Conclusion and Recommendations\nThe authors conclude that dichotomization sacrifices statistical rigor for an unnecessary simplicity in the analysis stage.\nThey strongly recommend that continuous variables should be analyzed on their original continuous scale in statistical models (e.g., as continuous predictors in regression models). If the relationship is suspected to be non-linear, more appropriate methods such as fractional polynomials or splines should be used instead of categorization.",
    "crumbs": [
      "statistics",
      "Papers",
      "The cost of dichotomising continuous variables"
    ]
  },
  {
    "objectID": "statistics/dewalsche_2025_40918066.html",
    "href": "statistics/dewalsche_2025_40918066.html",
    "title": "Large-scale composite hypothesis testing procedure for omics data analyses",
    "section": "",
    "text": "PubMed: 40918066\nDOI: 10.1093/nargab/lqaf118\nOverview generated by: Claude Sonnet 4.5, 25/11/2025",
    "crumbs": [
      "statistics",
      "Papers",
      "Large-scale composite hypothesis testing procedure for omics data analyses"
    ]
  },
  {
    "objectID": "statistics/dewalsche_2025_40918066.html#key-findings",
    "href": "statistics/dewalsche_2025_40918066.html#key-findings",
    "title": "Large-scale composite hypothesis testing procedure for omics data analyses",
    "section": "Key Findings",
    "text": "Key Findings\nThis study introduces qch_copula, a novel composite hypothesis testing (CHT) method for analyzing multiple traits or omics levels simultaneously, addressing key limitations in existing approaches for large-scale genomic studies.\n\nMain Discoveries\n\nNovel P-value derivation: First method to provide rigorously defined P-values directly from mixture model approaches for composite hypothesis testing\nSuperior performance: qch_copula effectively controls Type I error rates while maintaining higher detection power compared to eight state-of-the-art methods (DACT, HDMT, PLACO, adaFilter, IMIX, c-csmGmm, Primo, qch)\nScalability breakthrough: Memory-efficient EM algorithm reduces storage from O(n × 2^Q) to O(n + 2^Q), enabling analysis of up to 20 traits and 105-106 markers\nDependency modeling: Explicitly accounts for correlations between traits/omics levels through copula functions, improving false positive control",
    "crumbs": [
      "statistics",
      "Papers",
      "Large-scale composite hypothesis testing procedure for omics data analyses"
    ]
  },
  {
    "objectID": "statistics/dewalsche_2025_40918066.html#study-design",
    "href": "statistics/dewalsche_2025_40918066.html#study-design",
    "title": "Large-scale composite hypothesis testing procedure for omics data analyses",
    "section": "Study Design",
    "text": "Study Design\n\nMethodological Framework\nThe method addresses testing composite null hypotheses of the form H₀: “a marker/gene has effects in at most q̃ - 1 conditions” versus H₁: “effects in at least q̃ conditions.”\nKey components: - Mixture model with 2^Q components (one per configuration) - Gaussian copula to capture dependencies between conditions - Nonparametric estimation of alternative distributions - Two-step inference: marginal distributions, then proportions and copula parameters\n\n\nModel Specification\nFor Q conditions, z-scores (negative probit transforms of P-values) follow:\n\\[Z_i \\sim \\sum_{c \\in C} w_c \\psi_c\\]\nwhere each component ψ_c combines: - Univariate marginal distributions F^q_0 (null) and F^q_1 (alternative) - Copula function C_θ describing dependencies\nPosterior-based P-value:\n\\[\\text{pval}(z) = \\frac{1}{n\\hat{W}_0} \\sum_{j=1}^n \\mathbb{1}_{\\{\\hat{\\tau}_j &gt; \\hat{\\tau}_i\\}} (1 - \\hat{\\tau}_j)\\]\nwhere τ represents the posterior probability of belonging to alternative configurations.",
    "crumbs": [
      "statistics",
      "Papers",
      "Large-scale composite hypothesis testing procedure for omics data analyses"
    ]
  },
  {
    "objectID": "statistics/dewalsche_2025_40918066.html#major-results",
    "href": "statistics/dewalsche_2025_40918066.html#major-results",
    "title": "Large-scale composite hypothesis testing procedure for omics data analyses",
    "section": "Major Results",
    "text": "Major Results\n\nSimulation Study Design\nEvaluated across 24 settings varying: - Number of conditions: Q = 2, 8, 16 - Correlation levels: ρ = 0, 0.3, 0.5, 0.7 - Scenarios: sparse (≥90% null) vs. dense (50% alternative) - Sample size: n = 10^5 markers\n\n\nType I Error Control (Q = 2)\nIndependent traits (ρ = 0): - Most methods controlled FDR at nominal 5% level - DACT-JC showed substantial inflation (~0.25) - Minor deviations for PLACO and c-csmGmm (~0.08)\nCorrelated traits (ρ = 0.3): - Only DACT_Efron and qch_copula maintained proper FDR control - qch showed severe inflation (FDR = 0.256 sparse, 0.145 dense) - HDMT, PLACO, IMIX, c-csmGmm all exceeded nominal level\nHigher correlations (ρ = 0.5, 0.7): - qch_copula consistently controlled FDR near 0.05 - Other methods (except DACT_Efron) showed increased inflation\n\n\nType I Error Control (Q = 8, 16)\nQ = 8 with ρ = 0.3: - All three scalable methods (Primo, adaFilter, qch_copula) controlled FDR - Primo and adaFilter were conservative (FDR &lt; 0.02 in most cases) - qch_copula maintained FDR close to nominal level\nQ = 16 with ρ = 0.3: - qch_copula: slight inflation in sparse scenario (FDR ≤ 0.08) - adaFilter: comparable performance - Primo: computational failure (&gt;24h runtime or memory exhaustion)\nSpatial dependence (ξ = 0.3): - Improved FDR control for qch_copula and Primo - No impact on adaFilter - qch_copula reduced FDR from ~0.11 to &lt;0.065 in challenging scenarios\n\n\nDetection Power\nQ = 2: - DACT_Efron: zero power (too conservative) - qch_copula: 0.03-0.124 depending on scenario\nQ = 8, ρ = 0.3:\n\n\n\nTesting hypothesis\nPrimo Power\nadaFilter Power\nqch_copula Power\n\n\n\n\n≥2 traits (dense)\n0.143\n0.317\n0.609\n\n\n≥4 traits (dense)\n0.126\n0.148\n0.534\n\n\n≥8 traits (dense)\n0.125\n0.033\n0.186\n\n\n\nQ = 16, ρ = 0.3:\n\n\n\nTesting hypothesis\nadaFilter Power\nqch_copula Power\n\n\n\n\n≥2 traits (dense)\n0.278\n0.646\n\n\n≥4 traits (dense)\n0.166\n0.678\n\n\n≥8 traits (sparse)\n0.097\n0.568\n\n\n\nqch_copula showed 6× higher power than adaFilter for detecting associations with ≥8 traits in sparse scenarios.",
    "crumbs": [
      "statistics",
      "Papers",
      "Large-scale composite hypothesis testing procedure for omics data analyses"
    ]
  },
  {
    "objectID": "statistics/dewalsche_2025_40918066.html#computational-efficiency",
    "href": "statistics/dewalsche_2025_40918066.html#computational-efficiency",
    "title": "Large-scale composite hypothesis testing procedure for omics data analyses",
    "section": "Computational Efficiency",
    "text": "Computational Efficiency\n\nMemory-Efficient EM Algorithm\nClassical approach: Stores full posterior matrix T = (τ_ic) requiring 26 GB for n=10^5, Q=15\nqch_copula approach: - Computes posteriors on-the-fly during M-step - Stores only summary statistics S^(t)_i - Reduces memory from 26 GB to 1 MB (same example)\nRuntime (Q=16, n=10^5): - Model fitting: 78 minutes - Per hypothesis test: ~1 minute - Platform: Single thread, 3.2 GB RAM",
    "crumbs": [
      "statistics",
      "Papers",
      "Large-scale composite hypothesis testing procedure for omics data analyses"
    ]
  },
  {
    "objectID": "statistics/dewalsche_2025_40918066.html#application-i-psychiatric-disorders",
    "href": "statistics/dewalsche_2025_40918066.html#application-i-psychiatric-disorders",
    "title": "Large-scale composite hypothesis testing procedure for omics data analyses",
    "section": "Application I: Psychiatric Disorders",
    "text": "Application I: Psychiatric Disorders\n\nDataset\n\n14 psychiatric disorders from Psychiatric Genomics Consortium\n5,172,884 common SNPs\nObjective: Identify pleiotropic regions associated with ≥8 disorders\n\n\n\nComparison with PLACO\nOriginal analysis (PLACO): - Aggregated SNPs to 26,024 genes using MAGMA - Performed 91 pairwise analyses (all combinations of 2 disorders) - Identified 38 candidate genes\nqch_copula analysis: - Direct SNP-level analysis (no aggregation) - Single joint test across all 14 disorders - Identified 1,608 SNPs in 28 distinct regions\n\n\nNovel Findings\n35/38 PLACO genes confirmed plus 8 new regions:\n\n\n\nRegion\nChr\nPosition (Mb)\n# SNPs\nTop SNP P-value\n\n\n\n\nNovel 1\n5\n103.6-104.0\n338\n5.16×10^-12\n\n\nNovel 2\n1\n73.8-73.9\n156\n1.01×10^-7\n\n\nNovel 3\n3\n52.6-53.1\n90\n7.27×10^-8\n\n\n\nChromosome 5 region (top finding): - 338 SNPs detected - 25 SNPs associated with 11 disorders - Overlaps with RP11-6N13.1 gene - Previously reported for ADHD, ASD, BIP, MDD, SCZ, TS\nThree PLACO-only genes (NEGR1, TMX2, C11orf31): - Had only 3-7 P-values &lt;0.01 out of 14 (insufficient for ≥8 disorders) - Likely false positives from pairwise approach",
    "crumbs": [
      "statistics",
      "Papers",
      "Large-scale composite hypothesis testing procedure for omics data analyses"
    ]
  },
  {
    "objectID": "statistics/dewalsche_2025_40918066.html#application-ii-cucumber-virus-resistance",
    "href": "statistics/dewalsche_2025_40918066.html#application-ii-cucumber-virus-resistance",
    "title": "Large-scale composite hypothesis testing procedure for omics data analyses",
    "section": "Application II: Cucumber Virus Resistance",
    "text": "Application II: Cucumber Virus Resistance\n\nDataset\n\n289 cucumber lines (elite, landraces, hybrids)\n6 viruses: CGMMV, CMV, CVYV, PRSV, WMV, ZYMV\n339,804 common SNPs (after QC)\nObjective: Detect QTLs for multi-virus resistance\n\n\n\nResults by Pleiotropy Level\n\n\n\nNumber of viruses\n# SNPs detected\n# Regions\n\n\n\n\n≥2\n1,845\n5\n\n\n≥3\n164\n1\n\n\n≥4\n15\n1\n\n\n\n\n\nIdentified Hotspot Regions\nFive regions associated with ≥2 viruses:\n\n\n\nRegion\nViruses\nOriginal study\nExternal validation\n\n\n\n\nChr 5: 6.3-8.8 Mb\nWMV, CGMMV, CVYV, CMV\nReported\n-\n\n\nChr 6: 6.8-14.7 Mb\nPRSV, ZYMV\nReported\n-\n\n\nChr 1: 9.1-10.1 Mb\n-\nNovel\n-\n\n\nChr 2: 1.3 Mb\nPRSV, ZYMV\nNovel\nCMV, CABYV QTLs\n\n\nChr 6: 22.8-26.4 Mb\nPRSV, ZYMV\nNovel\nWMV, CABYV QTLs\n\n\n\nThree novel regions not reported in original study: - Two validated by independent studies showing shared resistance mechanisms - Demonstrates enhanced power of joint analysis over individual GWAS",
    "crumbs": [
      "statistics",
      "Papers",
      "Large-scale composite hypothesis testing procedure for omics data analyses"
    ]
  },
  {
    "objectID": "statistics/dewalsche_2025_40918066.html#methodological-insights",
    "href": "statistics/dewalsche_2025_40918066.html#methodological-insights",
    "title": "Large-scale composite hypothesis testing procedure for omics data analyses",
    "section": "Methodological Insights",
    "text": "Methodological Insights\n\nAdvantages over Existing Methods\nMixture model approaches (IMIX, c-csmGmm, Primo): - Fully parametric: constrain alternative distributions to Gaussian - qch_copula: nonparametric estimation with copula dependencies\nPairwise methods (PLACO, HDMT): - Limited to Q=2 - Multiple pairwise tests lack statistical guarantees for joint inference - Can produce inconsistent results\nFiltering methods (adaFilter): - More conservative - Lower power for stringent hypotheses\n\n\nP-values vs. Posteriors\nqch_copula establishes theoretical equivalence between: - Adaptive Benjamini-Hochberg FDR control on derived P-values - Local FDR control on posteriors\nAdvantages of P-values: - Compatible with any multiple testing procedure - Enable diagnostic tools (QQ-plots, histograms) - Allow Volcano/Manhattan plots - Facilitate method comparison\n\n\nCopula Modeling Strategy\nSingle correlation matrix across all components: - Balances model flexibility and computational efficiency - Avoids poor estimation from under-represented components - Captures essential dependency structure\nAlternative approaches: - Component-specific matrices (IMIX): computationally prohibitive - No dependencies (qch): severe Type I error inflation\n\n\nRobustness to Dependencies\nWithin-series correlation (ξ = 0.3): - Minimal impact on most methods - Actually improved FDR control for qch_copula - Particularly beneficial in challenging scenarios (Q=16)\nFurther options: - Combine with dependency-aware multiple testing procedures - Apply local score techniques for spatial clustering",
    "crumbs": [
      "statistics",
      "Papers",
      "Large-scale composite hypothesis testing procedure for omics data analyses"
    ]
  },
  {
    "objectID": "statistics/dewalsche_2025_40918066.html#practical-recommendations",
    "href": "statistics/dewalsche_2025_40918066.html#practical-recommendations",
    "title": "Large-scale composite hypothesis testing procedure for omics data analyses",
    "section": "Practical Recommendations",
    "text": "Practical Recommendations\n\nChoosing q̃ (Minimum Effect Threshold)\nMultiple strategies: 1. Hypothesis-driven: Based on research question (e.g., pleiotropy → q̃=2) 2. Prior evidence: From previous analyses (e.g., q̃=8 from literature) 3. Cost-benefit: Economic considerations (breeding value of multi-trait resistance) 4. Exploratory: Test multiple q̃ values for ranking\n\n\nMethod Selection\nUse qch_copula when: - Q &gt; 2 traits/conditions - Correlations between traits exist - Need rigorously defined P-values - Large-scale data (105-106 markers, Q ≤ 20) - Testing various composite hypotheses\nConsider alternatives when: - Q = 2 and no dependencies: simpler methods may suffice - Extremely high correlations (ρ &gt; 0.7): expect minor FDR inflation - Need for component-specific direction effects: extensions required\n\n\nImplementation Details\nAvailable in R package qch on CRAN\nKey functions: - Model fitting with copula dependencies - P-value computation for any composite hypothesis - Multiple hypothesis testing without re-estimation",
    "crumbs": [
      "statistics",
      "Papers",
      "Large-scale composite hypothesis testing procedure for omics data analyses"
    ]
  },
  {
    "objectID": "statistics/dewalsche_2025_40918066.html#limitations-and-extensions",
    "href": "statistics/dewalsche_2025_40918066.html#limitations-and-extensions",
    "title": "Large-scale composite hypothesis testing procedure for omics data analyses",
    "section": "Limitations and Extensions",
    "text": "Limitations and Extensions\n\nCurrent Limitations\n\nCorrelation levels: Slight FDR inflation at ρ ≥ 0.5 for large Q (16+)\nIndependence assumption: Items assumed independent within series\nDirection agnostic: Does not account for effect signs\nPost-hoc inference: Testing multiple q̃ values raises multiple comparisons issues\n\n\n\nOngoing/Future Work\nEffect direction: - Extension accounting for effect signs available in qch package (independent case) - Copula + direction effects: under development\nWithin-series dependencies: - Compatible with dependency-aware multiple testing - Integration with local score techniques\nModel selection: - Post-hoc inference procedures for q̃ selection",
    "crumbs": [
      "statistics",
      "Papers",
      "Large-scale composite hypothesis testing procedure for omics data analyses"
    ]
  },
  {
    "objectID": "statistics/dewalsche_2025_40918066.html#related-concepts",
    "href": "statistics/dewalsche_2025_40918066.html#related-concepts",
    "title": "Large-scale composite hypothesis testing procedure for omics data analyses",
    "section": "Related Concepts",
    "text": "Related Concepts\n\nComposite hypothesis: Union of multiple elementary hypotheses (e.g., H₀¹ ∪ H₀²)\nConfiguration: Vector c = (c₁, …, c_Q) indicating null/alternative status across conditions\nCopula function: Describes dependency structure between marginal distributions\nFlattening: Memory-efficient EM update using on-the-fly computation\nLocal FDR: Posterior probability of belonging to null configuration\nPleiotropy: Single locus affecting multiple traits/phenotypes",
    "crumbs": [
      "statistics",
      "Papers",
      "Large-scale composite hypothesis testing procedure for omics data analyses"
    ]
  },
  {
    "objectID": "statistics/mcgowan_2024_2276446.html",
    "href": "statistics/mcgowan_2024_2276446.html",
    "title": "Causal Inference Is Not Just a Statistics Problem",
    "section": "",
    "text": "PubMed: Not Indexed (Journal of Statistics and Data Science Education) DOI: 10.1080/26939169.2023.2276446 Overview generated by: Gemini 2.5 Flash, 26/11/2025",
    "crumbs": [
      "statistics",
      "Papers",
      "Causal Inference Is Not Just a Statistics Problem"
    ]
  },
  {
    "objectID": "statistics/mcgowan_2024_2276446.html#key-findings",
    "href": "statistics/mcgowan_2024_2276446.html#key-findings",
    "title": "Causal Inference Is Not Just a Statistics Problem",
    "section": "Key Findings",
    "text": "Key Findings\nThis article argues that causal inference—the process of determining whether an exposure causes an outcome—is a challenge rooted primarily in study design and conceptual modeling, not just statistical analysis. While statistical methods are necessary for quantifying effects, they cannot salvage a poorly designed study or validate a flawed causal hypothesis.\n\nCausal Inference is Not Statistical Regression\nThe authors emphasize the crucial distinction between prediction/association (a purely statistical exercise) and causal inference (a scientific exercise):\n\nCausality Requires Assumptions: Unlike association, causality requires making strong, often untestable assumptions about the data-generating process. These assumptions include positivity (everyone had a chance to receive the treatment), consistency (the treatment is well-defined), and exchangeability (the treated and untreated groups are comparable, usually requiring control for all confounders).\nStatistics Quantifies, Design Ensures Validity: Statistical methods (like regression, matching, or propensity scores) can only adjust for observed confounding variables under the assumption that all necessary confounders have been identified and measured without error. If a critical confounder is unmeasured (unmeasured confounding), the resulting causal estimate is likely biased, regardless of the sophistication of the statistics used.\n\n\n\nThe Primacy of Study Design\nThe article strongly aligns with the philosophy that “Design Trumps Analysis” (a concept attributed to Donald Rubin).\n\nNeed for Domain Knowledge: The process of identifying the correct causal model and the necessary variables to control (confounders) is a non-statistical process that relies entirely on domain-specific scientific knowledge (e.g., biology, epidemiology, medicine).\nThe “Which Variables to Control?” Problem: The decision of which covariates to include in a regression model is a causal question, not a statistical one. Including a variable that is actually a collider or a mediator can introduce bias where none existed, an error statisticians cannot prevent without external scientific guidance.\nRandomized Controlled Trials (RCTs): RCTs are the gold standard for causal inference precisely because they use a design (randomization) to satisfy the assumption of exchangeability (balance all confounders, measured and unmeasured), thereby circumventing the statistical problem of controlling for confounders.\n\n\n\nConclusion and Education Focus\nThe conclusion stresses that teaching causal inference must go beyond simply running statistical models. Students must be trained to: * Formulate a Causal Question first. * Diagram the Causal Structure using tools like Directed Acyclic Graphs (DAGs). * Identify the Sources of Bias (confounding, selection bias, measurement error) based on their domain knowledge. * Choose a Design (experimental or observational) that minimizes these biases. * Use Statistics only to quantify the effect size within the context of the chosen design and stated causal assumptions.",
    "crumbs": [
      "statistics",
      "Papers",
      "Causal Inference Is Not Just a Statistics Problem"
    ]
  },
  {
    "objectID": "statistics/greenland_2016_27209009.html",
    "href": "statistics/greenland_2016_27209009.html",
    "title": "Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations",
    "section": "",
    "text": "PubMed: 27209009 DOI: 10.1007/s10654-016-0149-3 Overview generated by: Gemini 2.5 Flash, 26/11/2025",
    "crumbs": [
      "statistics",
      "Papers",
      "Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations"
    ]
  },
  {
    "objectID": "statistics/greenland_2016_27209009.html#key-findings-correcting-the-misinterpretations-of-statistical-inference",
    "href": "statistics/greenland_2016_27209009.html#key-findings-correcting-the-misinterpretations-of-statistical-inference",
    "title": "Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations",
    "section": "Key Findings: Correcting the Misinterpretations of Statistical Inference",
    "text": "Key Findings: Correcting the Misinterpretations of Statistical Inference\nThis essential essay, authored by a collective of prominent statisticians and epidemiologists, provides a detailed guide to the widespread misinterpretations and abuse of basic statistical concepts: P-values, confidence intervals (CIs), and statistical power. It serves as a strong complement to the American Statistical Association’s (ASA) 2016 statement on p-values, emphasizing that these misuses are rampant and lead to profoundly flawed scientific conclusions.\n\nMisinterpretations of the P-value\nThe authors outline 12 common misconceptions about the P-value, which is defined correctly as the probability of observing a test statistic as extreme as, or more extreme than, the one calculated from the data, assuming the null hypothesis (\\(H_0\\)) is true.\nThe paper explicitly states that the P-value is NOT: * The probability that the study hypothesis is true. * The probability that the null hypothesis is true. * The probability that a result is due to chance. * A measure of the magnitude or importance of an effect.\nThey emphasize that a common and disastrous error is using the P-value threshold (e.g., P&lt;0.05) to draw a dichotomous conclusion (i.e., ‘significant’ or ‘non-significant’), which falsely suggests the conclusion is certain or that two studies with slightly different P-values (e.g., P=0.04 and P=0.06) have fundamentally different results.\n\n\nMisinterpretations of Confidence Intervals (CIs)\nThe paper clarifies that a Confidence Interval (CI), commonly 95% CI, is defined by its long-run performance. If one were to repeat the study an infinite number of times, 95% of the CIs constructed would contain the true value of the parameter.\nThe paper stresses that a CI is NOT a probability statement about the parameter in the specific study at hand. Misinterpretations include: * Assuming there is a 95% probability that the true effect lies within the observed interval. * Assuming that values outside the interval are refuted or implausible.\nThe main value of CIs is their ability to convey the precision of the estimate and the range of effect magnitudes that are compatible with the data, encouraging researchers to focus on effect size rather than just statistical significance.\n\n\nMisinterpretations of Statistical Power\nStatistical power is the probability of obtaining a statistically significant result, given a specific assumed effect size and the study’s design.\nThe authors note the main misuses: * Treating power as a continuous measure of study quality; it is highly dependent on the hypothesized effect size. * Misinterpreting low power: A non-significant result from a low-power study does not imply that the true effect is small or non-existent, only that the study was incapable of detecting the hypothesized effect.\n\n\nConclusion\nThe essay’s ultimate conclusion is that statistical inference tools, including P-values, are just one component of scientific reasoning. Their correct use requires attention to design, measurement quality, data integrity, and background knowledge. They recommend using CIs to emphasize effect magnitude and avoiding the common practice of dichotomizing results based on arbitrary P-value thresholds.",
    "crumbs": [
      "statistics",
      "Papers",
      "Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations"
    ]
  },
  {
    "objectID": "statistics/gururaghavendran_2025_39218433.html",
    "href": "statistics/gururaghavendran_2025_39218433.html",
    "title": "Can algorithms replace expert knowledge for causal inference? A case study on novice use of causal discovery",
    "section": "",
    "text": "PubMed: 39218433 DOI: 10.1093/aje/kwae338 Overview generated by: Gemini 2.5 Flash, 26/11/2025",
    "crumbs": [
      "statistics",
      "Papers",
      "Can algorithms replace expert knowledge for causal inference? A case study on novice use of causal discovery"
    ]
  },
  {
    "objectID": "statistics/gururaghavendran_2025_39218433.html#background-and-purpose",
    "href": "statistics/gururaghavendran_2025_39218433.html#background-and-purpose",
    "title": "Can algorithms replace expert knowledge for causal inference? A case study on novice use of causal discovery",
    "section": "Background and Purpose",
    "text": "Background and Purpose\nThis paper addresses the increasing discussion within epidemiology regarding the use of causal discovery algorithms (a form of machine learning) to automate the construction of Causal Directed Acyclic Graphs (DAGs) for covariate selection. The study’s objective was to assess the performance of these data-driven methods, when applied by a novice user, against a known confounded effect: the relationship between placebo adherence and mortality in the Coronary Drug Project (CDP) trial. This relationship is widely accepted to have a true causal effect of null, providing a strong benchmark for evaluating the ability of the algorithms to correctly control for confounding.",
    "crumbs": [
      "statistics",
      "Papers",
      "Can algorithms replace expert knowledge for causal inference? A case study on novice use of causal discovery"
    ]
  },
  {
    "objectID": "statistics/gururaghavendran_2025_39218433.html#study-design-and-methods",
    "href": "statistics/gururaghavendran_2025_39218433.html#study-design-and-methods",
    "title": "Can algorithms replace expert knowledge for causal inference? A case study on novice use of causal discovery",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\n\nCausal Discovery Implementation\nThe authors tested 4 common causal discovery algorithms: Peter-Clark (PC), Fast Causal Inference (FCI), Fast Greedy Causal Search (FGES), and Greedy Relaxed Sparsest Permutation (GRaSP). These algorithms were run on the CDP placebo arm data using 39 baseline covariates and the adherence/mortality outcome.\n\n\nParameter Variation\nTo simulate novice use and assess robustness, the authors varied several inputs: 1. Statistical Thresholds: For test-based algorithms (PC, FCI, GRaSP), the \\(\\chi^2\\) alpha level (\\(\\alpha\\)) was varied from 0.001 to 0.20. 2. Prior Knowledge: Models were run with no prior knowledge, a 3-tier time-ordering (covariates \\(\\rightarrow\\) adherence \\(\\rightarrow\\) death), and a 4-tier time-ordering (age/race \\(\\rightarrow\\) other covariates \\(\\rightarrow\\) adherence \\(\\rightarrow\\) death).\n\n\nAdjustment Set Selection\nFrom 17 model parameterizations (including 100 bootstrap samples per ensemble), 15 adjustment sets were identified. Because the bootstrapped results often produced cyclic graphs that could not be resolved into minimally sufficient adjustment sets, the authors adopted a simplification strategy: selecting all covariates identified as potential causes of either mortality or adherence in at least one bootstrap sample.",
    "crumbs": [
      "statistics",
      "Papers",
      "Can algorithms replace expert knowledge for causal inference? A case study on novice use of causal discovery"
    ]
  },
  {
    "objectID": "statistics/gururaghavendran_2025_39218433.html#key-findings-residual-bias-and-subjectivity",
    "href": "statistics/gururaghavendran_2025_39218433.html#key-findings-residual-bias-and-subjectivity",
    "title": "Can algorithms replace expert knowledge for causal inference? A case study on novice use of causal discovery",
    "section": "Key Findings: Residual Bias and Subjectivity",
    "text": "Key Findings: Residual Bias and Subjectivity\n\nPerformance\n\nBaseline-only Adjustment: The adjustment sets identified by the algorithms, when used to adjust for only baseline covariates, performed similarly to prior published results by achieving a roughly 36% reduction of bias from the unadjusted relationship.\nTime-Varying Adjustment (IPW): When more complex methods were used (Inverse Probability Weighting) to adjust for time-varying confounding, the adjustment sets from the causal discovery algorithms resulted in more residual bias compared to the adjustment sets selected by the original CDP expert team.\n\n\n\nChallenges and Subjectivity\n\nInconsistent Results: Varying the input parameters and algorithm type resulted in a wide range of unique adjustment sets. Only about half of the resulting analyses showed compatibility with the known null effect.\nExpert Knowledge Value: The use of time-ordering (prior knowledge) was essential for improving the interpretability of the resulting graphs, particularly concerning temporal relationships like age and race.\nMethodological Difficulty: The use of bootstrap samples frequently led to cyclic graphs, requiring subjective decisions on simplification and adjustment set selection, which introduces substantial subjectivity into the process.",
    "crumbs": [
      "statistics",
      "Papers",
      "Can algorithms replace expert knowledge for causal inference? A case study on novice use of causal discovery"
    ]
  },
  {
    "objectID": "statistics/gururaghavendran_2025_39218433.html#conclusions-and-recommendations",
    "href": "statistics/gururaghavendran_2025_39218433.html#conclusions-and-recommendations",
    "title": "Can algorithms replace expert knowledge for causal inference? A case study on novice use of causal discovery",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study concludes that while causal discovery algorithms can partially replicate expert findings, their use is not straightforward and introduces a significant degree of subjectivity through the selection of algorithms, input parameters, and interpretation of non-acyclic graphs.\nThe authors strongly recommend that researchers without detailed knowledge of causal discovery algorithms not attempt to use these tools without the aid of an expert in the field. Absent this expert support, the use of traditional subject matter experts to generate causal graphs provides greater transparency about the assumptions made and, in this case study, yielded the best estimate of the true causal effect.",
    "crumbs": [
      "statistics",
      "Papers",
      "Can algorithms replace expert knowledge for causal inference? A case study on novice use of causal discovery"
    ]
  },
  {
    "objectID": "statistics/hoenig_2001.html",
    "href": "statistics/hoenig_2001.html",
    "title": "The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis",
    "section": "",
    "text": "PubMed: Not Indexed (The American Statistician) DOI: 10.1080/00031305.2001.10473582 Overview generated by: Gemini 2.5 Flash, 26/11/2025",
    "crumbs": [
      "statistics",
      "Papers",
      "The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis"
    ]
  },
  {
    "objectID": "statistics/hoenig_2001.html#key-finding-the-fallacy-of-retrospective-power-analysis",
    "href": "statistics/hoenig_2001.html#key-finding-the-fallacy-of-retrospective-power-analysis",
    "title": "The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis",
    "section": "Key Finding: The Fallacy of Retrospective Power Analysis",
    "text": "Key Finding: The Fallacy of Retrospective Power Analysis\nThis article by Hoenig and Heisey critically examines and rejects the common practice of performing post-experiment or retrospective power calculations (also called “observed power”) to interpret a statistically non-significant result (i.e., a failure to reject the null hypothesis, \\(H_0\\)).\n\nThe Flawed Logic of Retrospective Power\nThe authors demonstrate that the use of retrospective power as an aid to interpretation is fundamentally flawed because it is a simple monotonic transformation of the p-value.\n\nDefinition: Retrospective power (or observed power) is typically calculated as the statistical power to detect the observed effect size using the observed sample size and the observed variance.\nThe Circularity Problem: Because the observed effect size is used as the hypothetical “true” effect, the retrospective power calculation is nearly equivalent to the p-value:\n\nA small p-value (significant result) will always lead to a high retrospective power.\nA large p-value (non-significant result) will always lead to a low retrospective power.\n\nNo New Information: Retrospective power provides no additional information beyond what is already contained in the p-value and the confidence interval. Stating that a non-significant result had low power is merely restating the finding that the confidence interval around the point estimate is wide enough to include the null hypothesis.\n\n\n\nWhy the Flaw is Pervasive\nThe practice of retrospective power analysis stems from a misunderstanding of the dilemma of the nonrejected null hypothesis: when we fail to reject \\(H_0\\), we want to know if it’s because the true effect is small (or zero), or because the study lacked power to detect an important effect.\n\nMisleading Interpretation: Advocates of retrospective power claim that a low observed power, combined with a non-significant test, suggests the result is “inconclusive” and that a “Type II error” (failing to reject a false \\(H_0\\)) is likely.\nThe Correct Interpretation: A non-significant result means that the data are consistent with the null hypothesis (\\(H_0\\) being true). The only way to address the dilemma is by looking at the confidence interval to see if it excludes effect sizes that are considered biologically or clinically important.\n\n\n\nRecommendation\nThe authors recommend that statistical power should be used only for planning an experiment (prospective analysis). To interpret the results of a completed study, especially a non-significant finding, researchers should focus on:\n\nThe p-value.\nThe point estimate (observed effect size).\nThe confidence interval (which indicates the range of true effects consistent with the data).\n\nThe confidence interval is the superior tool for interpreting non-significant results because it shows whether important effect sizes have been reasonably ruled out, which is the actual goal of most post-hoc power discussions.",
    "crumbs": [
      "statistics",
      "Papers",
      "The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis"
    ]
  },
  {
    "objectID": "statistics/lawlor_2017_28108528.html",
    "href": "statistics/lawlor_2017_28108528.html",
    "title": "Triangulation in aetiological epidemiology: Approaches to causal inference",
    "section": "",
    "text": "PubMed: 28108528 DOI: 10.1093/ije/dyw314 Overview generated by: Gemini 2.5 Flash, 26/11/2025",
    "crumbs": [
      "statistics",
      "Papers",
      "Triangulation in aetiological epidemiology: Approaches to causal inference"
    ]
  },
  {
    "objectID": "statistics/lawlor_2017_28108528.html#key-findings-enhancing-causal-inference-through-triangulation",
    "href": "statistics/lawlor_2017_28108528.html#key-findings-enhancing-causal-inference-through-triangulation",
    "title": "Triangulation in aetiological epidemiology: Approaches to causal inference",
    "section": "Key Findings: Enhancing Causal Inference Through Triangulation",
    "text": "Key Findings: Enhancing Causal Inference Through Triangulation\nThis foundational article in aetiological epidemiology advocates for the systematic use of Triangulation—the practice of integrating results from multiple distinct research approaches—to strengthen causal inference in the study of disease aetiology. The authors argue that relying on single methods or studies is inherently problematic due to the limitations and biases specific to that approach.\n\nThe Principle of Triangulation\nTriangulation requires the simultaneous use of several approaches where:\n\nDifferent Bias Sources: Each approach must possess different key sources of potential bias that are uncorrelated with the biases of the other approaches. This ensures that any consistent finding is less likely to be an artefact of a single, shared flaw.\nIncreased Confidence: When the findings from different, methodologically distinct approaches all point to the same conclusion regarding a causal relationship (e.g., exposure \\(A\\) causes outcome \\(B\\)), confidence in that causal finding is significantly increased.\n\n\n\nIdentifying and Addressing Bias\nThe power of triangulation is particularly evident when methodological biases are explicitly considered:\n\nBias Prediction: The approach is strongest when the key sources of bias of some methods (e.g., unmeasured confounding in observational studies, reverse causation in cross-sectional studies) would predict findings that point in opposite directions if those biases were solely responsible for the observed association. Consistency despite these opposing biases provides strong evidence for causality.\nInconsistency as a Guide: When inconsistencies or contradictions arise between the results of different approaches, the triangulation framework provides a mechanism to identify and dissect the key sources of bias inherent in each method. This process then guides researchers to design further, more robust studies to address the causal question.\n\n\n\nApplication in Aetiological Epidemiology\nThe paper illustrates the application of triangulation by combining evidence from a variety of sources to address epidemiological causal questions, including:\n\nConventional Epidemiology: Standard cohort or case-control studies (prone to confounding and reverse causation).\nMendelian Randomization (MR): Genetic association studies that use genetic variants as instrumental variables (less prone to confounding and reverse causation).\nQuasi-experimental designs: Such as sibling comparisons or natural experiments.\nRandomized Controlled Trials (RCTs): The gold standard for causality, often unfeasible or unethical for long-term aetiological questions.\n\nBy integrating evidence across these different methods, triangulation provides a robust framework for overcoming the inherent limitations of any single study design in defining causality.",
    "crumbs": [
      "statistics",
      "Papers",
      "Triangulation in aetiological epidemiology: Approaches to causal inference"
    ]
  },
  {
    "objectID": "statistics/thoresen_2019_30732587.html",
    "href": "statistics/thoresen_2019_30732587.html",
    "title": "Spurious interaction as a result of categorization",
    "section": "",
    "text": "PubMed: 30732587 DOI: 10.1186/s12874-019-0667-2 Overview generated by: Gemini 2.5 Flash, 26/11/2025",
    "crumbs": [
      "statistics",
      "Papers",
      "Spurious interaction as a result of categorization"
    ]
  },
  {
    "objectID": "statistics/thoresen_2019_30732587.html#key-findings-the-generation-of-spurious-interaction",
    "href": "statistics/thoresen_2019_30732587.html#key-findings-the-generation-of-spurious-interaction",
    "title": "Spurious interaction as a result of categorization",
    "section": "Key Findings: The Generation of Spurious Interaction",
    "text": "Key Findings: The Generation of Spurious Interaction\nThis paper presents an additional argument against the common practice in epidemiological and clinical research of converting continuous exposure variables into categorical variables. It demonstrates that such categorization can lead to spurious interaction effects in multiple regression models, even when no true interaction exists between the continuous variables.\nThe spurious interaction problem is fundamentally linked to other well-known issues caused by categorization, including loss of information and statistical power and an increased risk of Type I error if continuous confounder variables are also categorized.",
    "crumbs": [
      "statistics",
      "Papers",
      "Spurious interaction as a result of categorization"
    ]
  },
  {
    "objectID": "statistics/thoresen_2019_30732587.html#study-design-and-methods",
    "href": "statistics/thoresen_2019_30732587.html#study-design-and-methods",
    "title": "Spurious interaction as a result of categorization",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\nThe investigation used a combination of analytical and simulation-based methods:\n\nAnalytical Development\nThe authors derived precise analytical expressions for the linear regression model with two bivariate normally distributed exposure variables (\\(X_1\\) and \\(X_2\\)) and a continuous outcome (\\(Y\\)). Crucially, the true model assumed no interaction between the continuous exposure variables. The analysis then examined the conditions under which an interaction term would appear in a model using the categorized versions (\\(\\tilde{X}_1\\) and \\(\\tilde{X}_2\\)).\n\n\nInterpretation\nThe authors interpret the spurious interaction in two related ways: 1. Measurement Error: Categorization is viewed as an extreme form of differential measurement error. Because the reliability (as measured by the point-biserial correlation) of the categorized variable \\(\\tilde{X}_i\\) varies with the level of the other variable \\(X_j\\), the measurement error is differential, which is known to induce interaction. 2. Residual Confounding: Categorization of a continuous variable leaves residual confounding. Differences in this residual confounding across strata defined by the other exposure variable may lead to the observed spurious interaction.",
    "crumbs": [
      "statistics",
      "Papers",
      "Spurious interaction as a result of categorization"
    ]
  },
  {
    "objectID": "statistics/thoresen_2019_30732587.html#results",
    "href": "statistics/thoresen_2019_30732587.html#results",
    "title": "Spurious interaction as a result of categorization",
    "section": "Results",
    "text": "Results\n\nAnalytical Result\nFor two correlated, normally distributed exposure variables, both categorized at the same cut point (\\(c\\)), a spurious interaction term (\\(\\tilde{\\beta}_3\\)) will be induced unless one of two conditions is met: * The two variables are uncorrelated (\\(\\rho=0\\)). * The variables are categorized precisely at the median (\\(c=0\\) in the standardized case).\n\n\nEmpirical Illustrations and Simulation Findings\n\nReal Data Examples: The paper provides two practical examples (one linear model for lung function and one logistic model for myocardial infarction mortality) showing that categorization can change the interpretation of data by generating a statistically significant interaction where the original continuous model showed a non-significant or practically insignificant effect.\nMagnitude: Simulations demonstrated that the magnitude of the induced interaction term (relative to the main effects) increases substantially as the chosen cut point becomes more extreme (further from the median) and as the correlation (\\(\\rho\\)) between the variables increases.\nGeneralizability: Simulations using different distributions (Normal, Uniform, and Chi-square) confirmed that the general effect of spurious interaction due to categorization is present across various distributional shapes.",
    "crumbs": [
      "statistics",
      "Papers",
      "Spurious interaction as a result of categorization"
    ]
  },
  {
    "objectID": "statistics/thoresen_2019_30732587.html#conclusions-and-recommendations",
    "href": "statistics/thoresen_2019_30732587.html#conclusions-and-recommendations",
    "title": "Spurious interaction as a result of categorization",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe primary conclusion is a strong recommendation that the categorization of continuous variables in regression modeling should be avoided.\nThe practice introduces a number of problems, including biased estimates, loss of power, and inflated Type-I error rates, with the generation of spurious interaction being another critical drawback.\nIf an interaction effect is found in an analysis using categorized explanatory variables, the researcher must consider the categorization method itself as a potential and likely explanation for the finding.\nAs alternatives, the authors suggest: * Using non-parametric regression methods if the relationship cannot be easily modeled by classical parametric models. * If one chooses to categorize despite the warnings, it is preferable to categorize into more than two groups to minimize the resulting information loss.",
    "crumbs": [
      "statistics",
      "Papers",
      "Spurious interaction as a result of categorization"
    ]
  },
  {
    "objectID": "statistics/shaw_2023_37338987.html",
    "href": "statistics/shaw_2023_37338987.html",
    "title": "Comparison of Imputation Strategies for Incomplete Longitudinal Data in Life-Course Epidemiology",
    "section": "",
    "text": "PubMed: 37338987 DOI: 10.1093/aje/kwad139 Overview generated by: Gemini 2.5 Flash, 27/11/2025",
    "crumbs": [
      "statistics",
      "Papers",
      "Comparison of Imputation Strategies for Incomplete Longitudinal Data in Life-Course Epidemiology"
    ]
  },
  {
    "objectID": "statistics/shaw_2023_37338987.html#background-and-objective",
    "href": "statistics/shaw_2023_37338987.html#background-and-objective",
    "title": "Comparison of Imputation Strategies for Incomplete Longitudinal Data in Life-Course Epidemiology",
    "section": "Background and Objective",
    "text": "Background and Objective\nIncomplete longitudinal data—data with values missing at different time points for the same individuals—is a pervasive challenge in life-course epidemiology. If not handled correctly, this missingness can introduce bias and lead to incorrect statistical inference. Multiple imputation (MI) is the increasingly preferred method for addressing this, but real-world performance comparisons between different MI techniques are limited.\nThe objective of this study was to conduct a direct comparison of three Multiple Imputation (MI) methods using real-world longitudinal data to assess their performance, feasibility, and impact on causal effect estimates across various missing-data scenarios.",
    "crumbs": [
      "statistics",
      "Papers",
      "Comparison of Imputation Strategies for Incomplete Longitudinal Data in Life-Course Epidemiology"
    ]
  },
  {
    "objectID": "statistics/shaw_2023_37338987.html#methods-benchmarking-on-hrs-data",
    "href": "statistics/shaw_2023_37338987.html#methods-benchmarking-on-hrs-data",
    "title": "Comparison of Imputation Strategies for Incomplete Longitudinal Data in Life-Course Epidemiology",
    "section": "Methods: Benchmarking on HRS Data",
    "text": "Methods: Benchmarking on HRS Data\nThe researchers compared three Multiple Imputation (MI) methods using data from the Health and Retirement Study (HRS):\n\nNormal Linear Regression (Multiple Imputation by Chained Equations - MICE)\nPredictive Mean Matching (PMM) (a non-parametric MICE method)\nVariable-Tailored Specification\n\nThe comparison was conducted under nine missing-data scenarios that represented combinations of: * Missingness Rate: 10%, 20%, and 30% missingness. * Missingness Mechanism: Missing Completely at Random (MCAR), Missing at Random (MAR), and Missing Not at Random (MNAR).\nThe study introduced record-level missingness to a complete sample of HRS participants and then used the imputed datasets to fit Cox proportional hazards models. The outcome of interest was the effect of four different operationalizations of longitudinal depressive symptoms on subsequent mortality.",
    "crumbs": [
      "statistics",
      "Papers",
      "Comparison of Imputation Strategies for Incomplete Longitudinal Data in Life-Course Epidemiology"
    ]
  },
  {
    "objectID": "statistics/shaw_2023_37338987.html#key-results-and-conclusion",
    "href": "statistics/shaw_2023_37338987.html#key-results-and-conclusion",
    "title": "Comparison of Imputation Strategies for Incomplete Longitudinal Data in Life-Course Epidemiology",
    "section": "Key Results and Conclusion",
    "text": "Key Results and Conclusion\nThe comparison focused on three performance metrics: bias in hazard ratios, root mean square error (RMSE), and computation time.\n\nBias: Bias in the estimated hazard ratios was found to be similar across all three MI methods.\nConsistency: The results were consistent across the four different ways the longitudinal exposure (depressive symptoms) was defined.\nPerformance: The authors suggest that Predictive Mean Matching (PMM) may be the most appealing strategy for imputing life-course exposure data. PMM consistently showed the lowest root mean square error (RMSE)—indicating greater precision—and maintained competitive computation times with few implementation challenges.\n\nThis study provides valuable, empirically-based guidance for researchers in life-course epidemiology regarding the choice of MI method for handling incomplete longitudinal exposure data.",
    "crumbs": [
      "statistics",
      "Papers",
      "Comparison of Imputation Strategies for Incomplete Longitudinal Data in Life-Course Epidemiology"
    ]
  },
  {
    "objectID": "statistics/senn_1994_7997705.html",
    "href": "statistics/senn_1994_7997705.html",
    "title": "TESTING FOR BASELINE BALANCE IN CLINICAL TRIALS",
    "section": "",
    "text": "PubMed: 7997705 DOI: 10.1002/sim.4780131610 Overview generated by: Gemini 2.5 Flash, 26/11/2025",
    "crumbs": [
      "statistics",
      "Papers",
      "TESTING FOR BASELINE BALANCE IN CLINICAL TRIALS"
    ]
  },
  {
    "objectID": "statistics/senn_1994_7997705.html#key-findings-the-misguided-practice-of-testing-baseline-balance",
    "href": "statistics/senn_1994_7997705.html#key-findings-the-misguided-practice-of-testing-baseline-balance",
    "title": "TESTING FOR BASELINE BALANCE IN CLINICAL TRIALS",
    "section": "Key Findings: The Misguided Practice of Testing Baseline Balance",
    "text": "Key Findings: The Misguided Practice of Testing Baseline Balance\nThis influential paper by Stephen Senn critiques the common, but statistically unsound, practice in randomized controlled trials (RCTs) of performing “tests of baseline homogeneity” (i.e., p-value tests comparing baseline characteristics between treatment arms) before proceeding to analyze the treatment effect on the outcome. Senn argues that this practice is both philosophically flawed and practically misleading.\n\nThe Problem with Testing for Balance\nThe core issue lies in the interpretation of randomization and the nature of the null hypothesis in a properly conducted RCT:\n\nPhilosophically Unsound: The goal of randomization is to ensure that, in expectation, the treatment groups are comparable. Any observed differences at baseline are due purely to chance, and no statistical test is required to confirm this. Performing a test is equivalent to testing the effectiveness of a coin toss—a pointless exercise.\nNo Practical Value: A statistically significant difference at baseline (e.g., \\(p &lt; 0.05\\) for age difference) does not indicate a flaw in the randomization process; it is merely a low-probability chance event that is expected to occur in 5% of all covariates tested. Such a finding offers no useful guidance on how to analyze the treatment effect.\nPotentially Misleading:\n\nNon-significant difference (\\(p &gt; 0.05\\)): This does not prove the groups are “balanced” or comparable. It only means the study lacked the power to detect a difference, or that the observed difference was not large enough to cross the arbitrary significance threshold. The non-significant result might wrongly convince the investigator that no adjustment is needed, even if the difference is clinically important.\nSignificant difference (\\(p &lt; 0.05\\)): This may wrongly prompt an investigator to use an ad hoc adjustment (like Analysis of Covariance, ANCOVA) only for that specific variable, introducing subjectivity into the analysis plan and potentially invalidating the comparison of treatment effects.\n\n\n\n\nThe Recommended Practice: Analysis of Covariance (ANCOVA)\nSenn strongly recommends replacing baseline balance testing with a principled approach to analysis:\n\nPre-specify Prognostic Variables: The study protocol should identify and list all known or suspected prognostic covariates (variables predictive of the outcome), regardless of their baseline distribution.\nRoutine ANCOVA: These prognostic variables should be included in the primary analysis model using Analysis of Covariance (ANCOVA), irrespective of their p-value from the baseline comparison.\nStatistical Advantages: Adjusting for important prognostic variables using ANCOVA increases the precision and statistical power of the treatment effect estimate, leading to narrower confidence intervals. Crucially, the validity of ANCOVA in an RCT relies on the fact that randomization ensures the baseline covariates are unrelated to the treatment assignment, not on their baseline p-value.\n\n\n\nConclusion\nThe paper concludes that the practice of baseline testing is a confusion of philosophy and statistics. Researchers should focus on design (randomization) to ensure validity and statistical efficiency (ANCOVA) to maximize power, rather than using post-randomization tests that are incapable of fulfilling the purpose for which they are intended.",
    "crumbs": [
      "statistics",
      "Papers",
      "TESTING FOR BASELINE BALANCE IN CLINICAL TRIALS"
    ]
  },
  {
    "objectID": "statistics/pividori_2024_39243756.html",
    "href": "statistics/pividori_2024_39243756.html",
    "title": "An efficient, not-only-linear correlation coefficient based on clustering",
    "section": "",
    "text": "PubMed: 39243756 DOI: 10.1016/j.cels.2024.08.005 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "statistics",
      "Papers",
      "An efficient, not-only-linear correlation coefficient based on clustering"
    ]
  },
  {
    "objectID": "statistics/pividori_2024_39243756.html#key-findings-the-clustering-based-correlation-coefficient-ccc",
    "href": "statistics/pividori_2024_39243756.html#key-findings-the-clustering-based-correlation-coefficient-ccc",
    "title": "An efficient, not-only-linear correlation coefficient based on clustering",
    "section": "Key Findings: The Clustering-based Correlation Coefficient (CCC)",
    "text": "Key Findings: The Clustering-based Correlation Coefficient (CCC)\nThis paper introduces the Clustering-based Correlation Coefficient (CCC), an efficient and easy-to-use statistical measure designed to identify both linear and non-linear relationships between variables. The core motivation is to overcome the limitation of standard coefficients, such as Pearson’s \\(r\\), which are fundamentally restricted to measuring linear associations and therefore miss complex patterns prevalent in high-dimensional biological data.",
    "crumbs": [
      "statistics",
      "Papers",
      "An efficient, not-only-linear correlation coefficient based on clustering"
    ]
  },
  {
    "objectID": "statistics/pividori_2024_39243756.html#study-design-and-methods-introducing-ccc",
    "href": "statistics/pividori_2024_39243756.html#study-design-and-methods-introducing-ccc",
    "title": "An efficient, not-only-linear correlation coefficient based on clustering",
    "section": "Study Design and Methods: Introducing CCC",
    "text": "Study Design and Methods: Introducing CCC\n\nThe Nature of CCC\nThe CCC is a not-only-linear correlation coefficient that operates by leveraging a clustering-based statistic. Instead of relying on linear regression assumptions, it assesses the strength of the relationship by quantifying how well the marginal information of each variable separately aligns with the clusters formed by the joint distribution of the two variables.\n\n\nMethodology\n\nJoint Clustering: A clustering algorithm is applied to the data points based on the values of both variables simultaneously.\nMarginal Assessment: The CCC then measures the extent to which the values of the first variable alone can explain the clusters identified in step one, and the same is done for the second variable.\nFinal Score: The resulting coefficient reflects the consistency between the joint clustering and the clustering explained by the individual variables.\n\nThe authors show that CCC is a statistically sound measure that can effectively capture a wide array of patterns, including linear, non-linear (e.g., parabolic, or U-shaped), and non-monotonic dependencies.",
    "crumbs": [
      "statistics",
      "Papers",
      "An efficient, not-only-linear correlation coefficient based on clustering"
    ]
  },
  {
    "objectID": "statistics/pividori_2024_39243756.html#results-and-empirical-application",
    "href": "statistics/pividori_2024_39243756.html#results-and-empirical-application",
    "title": "An efficient, not-only-linear correlation coefficient based on clustering",
    "section": "Results and Empirical Application",
    "text": "Results and Empirical Application\n\nSimulation and Comparison\nThrough simulated data, the authors demonstrate that the CCC successfully detects relationships where standard measures like Pearson’s \\(r\\), Spearman’s \\(\\rho\\), and even other non-linear coefficients like the Maximal Information Coefficient (MIC), exhibit reduced performance. The CCC is also shown to be computationally efficient, making it scalable for large datasets.\n\n\nApplication to Genome-Scale Data\nThe CCC was applied to human gene expression data (a common source of complex, non-linear biological associations) to identify correlated gene pairs.\n\nDetection of Biological Patterns: When applied to gene expression data, CCC successfully identified non-linear patterns that were not captured by linear-only coefficients.\nSex Differences: The identified non-linear associations were often explained by sex differences, where the relationship between two genes varied significantly across male and female subgroups. For instance, the expression of certain gene pairs might be highly correlated within sexes but exhibit a non-linear overall pattern when sexes are combined, which CCC could detect while linear methods could not.",
    "crumbs": [
      "statistics",
      "Papers",
      "An efficient, not-only-linear correlation coefficient based on clustering"
    ]
  },
  {
    "objectID": "statistics/pividori_2024_39243756.html#conclusions-and-recommendations",
    "href": "statistics/pividori_2024_39243756.html#conclusions-and-recommendations",
    "title": "An efficient, not-only-linear correlation coefficient based on clustering",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe Clustering-based Correlation Coefficient (CCC) is a valuable and robust tool for pattern identification in complex datasets. It provides a computationally efficient, simple, and not-only-linear alternative to standard correlation measures. The demonstrated ability of CCC to reveal biologically meaningful non-linear patterns—such as those linked to sex differences in gene expression—highlights its utility for high-throughput analyses in genomics and other fields where relationships are rarely simple or purely linear.",
    "crumbs": [
      "statistics",
      "Papers",
      "An efficient, not-only-linear correlation coefficient based on clustering"
    ]
  },
  {
    "objectID": "statistics/smith_2018_dyw314.html",
    "href": "statistics/smith_2018_dyw314.html",
    "title": "Step away from stepwise",
    "section": "",
    "text": "PubMed: Not Indexed (Journal of Big Data) DOI: 10.1186/s40537-018-0143-6 Overview generated by: Gemini 2.5 Flash, 26/11/2025",
    "crumbs": [
      "statistics",
      "Papers",
      "Step away from stepwise"
    ]
  },
  {
    "objectID": "statistics/smith_2018_dyw314.html#key-findings-the-dangers-of-stepwise-regression-in-the-era-of-big-data",
    "href": "statistics/smith_2018_dyw314.html#key-findings-the-dangers-of-stepwise-regression-in-the-era-of-big-data",
    "title": "Step away from stepwise",
    "section": "Key Findings: The Dangers of Stepwise Regression in the Era of Big Data",
    "text": "Key Findings: The Dangers of Stepwise Regression in the Era of Big Data\nThis short report by Gary Smith critiques the continued use of stepwise regression, a popular but flawed variable selection method, especially in the context of Big Data. The central argument is that stepwise procedures are fundamentally unable to distinguish between genuine explanatory variables and nuisance variables (spurious correlations), a problem that is dramatically exacerbated when the pool of potential predictors is large.\n\nThe Fundamental Flaws of Stepwise Regression\nStepwise regression (which includes forward selection, backward elimination, and bidirectional methods) uses an arbitrary threshold of statistical significance (p-value) to automate the inclusion or exclusion of variables in a multiple-regression model. The author identifies three major, interconnected problems with this approach:\n\nSelection of Nuisance Variables: By relying solely on statistical significance, stepwise procedures frequently select nuisance variables that happen to be coincidentally significant in the in-sample data. Since these variables have no true causal effect, they are useless for prediction with fresh data (out-of-sample).\nExclusion of True Explanatory Variables: Conversely, genuine explanatory variables with causal effects may be incorrectly excluded because they happen not to be statistically significant in the particular sample analyzed.\nSevere Out-of-Sample Failure: The resulting model, while often providing an excellent fit to the estimation data (high \\(R^2\\) due to including significant noise variables), does poorly out-of-sample. The selection of irrelevant variables provides a false confidence in the estimated model because of the high t-values and the boost to the in-sample \\(R^2\\). ### Big Data Exacerbates the Problem\n\nThe article specifically addresses the belief held by some “Big-Data researchers” that the larger the number of possible explanatory variables, the more useful stepwise regression becomes.\n\nIncreased Chance of Spurious Correlation: In reality, the efficacy of stepwise regression is less effective the larger the number of potential explanatory variables. The sheer number of variables in Big Data increases the probability of finding highly significant but spurious correlations purely by chance.\nWorsening Out-of-Sample Fit: As the number of candidate variables increases, the in-sample fit improves, but the out-of-sample fit deteriorates, causing the ratio of the out-of-sample errors to the in-sample errors to “balloon”.\n\n\n\nConclusion\nThe paper concludes that stepwise regression does not offer a solution to the challenge of too many explanatory variables in the Big Data era; rather, Big Data exacerbates the failings of stepwise regression. The focus should instead be on methods that prioritize predictive accuracy and robust out-of-sample validation.",
    "crumbs": [
      "statistics",
      "Papers",
      "Step away from stepwise"
    ]
  },
  {
    "objectID": "statistics/debertin_2024_38860706.html",
    "href": "statistics/debertin_2024_38860706.html",
    "title": "Synthesizing Subject-matter Expertise for Variable Selection in Causal Effect Estimation: A Case Study",
    "section": "",
    "text": "PubMed: 38860706 DOI: 10.1097/EDE.0000000000001758 Overview generated by: Gemini 2.5 Flash, 26/11/2025",
    "crumbs": [
      "statistics",
      "Papers",
      "Synthesizing Subject-matter Expertise for Variable Selection in Causal Effect Estimation: A Case Study"
    ]
  },
  {
    "objectID": "statistics/debertin_2024_38860706.html#background-and-purpose",
    "href": "statistics/debertin_2024_38860706.html#background-and-purpose",
    "title": "Synthesizing Subject-matter Expertise for Variable Selection in Causal Effect Estimation: A Case Study",
    "section": "Background and Purpose",
    "text": "Background and Purpose\nDirected Acyclic Graphs (DAGs) are a crucial theoretical tool for covariate selection in causal effect estimation, as they allow researchers to identify minimal adjustment sets that control for confounding. However, there is limited empirical research on the practical creation of these graphs. This paper assesses different approaches to DAG construction using data from the Coronary Drug Project (CDP) trial. The focus is on estimating the effect of placebo adherence on mortality, a relationship where the true causal effect is assumed to be zero (as a placebo cannot cause mortality), providing a robust benchmark for comparing methods.",
    "crumbs": [
      "statistics",
      "Papers",
      "Synthesizing Subject-matter Expertise for Variable Selection in Causal Effect Estimation: A Case Study"
    ]
  },
  {
    "objectID": "statistics/debertin_2024_38860706.html#study-methods-and-design",
    "href": "statistics/debertin_2024_38860706.html#study-methods-and-design",
    "title": "Synthesizing Subject-matter Expertise for Variable Selection in Causal Effect Estimation: A Case Study",
    "section": "Study Methods and Design",
    "text": "Study Methods and Design\nThe authors created multiple DAGs based on various strategies for identifying and linking variables. For each DAG, the corresponding minimal adjustment sets were derived to control for confounding variables. These adjustment sets were then applied to the CDP data under two primary modeling strategies:\n\nBaseline-only Adjustment: Estimating the cumulative effect of adherence on mortality by adjusting only for baseline covariate values in a standard regression.\nTime-Varying Adjustment: Estimating the effect by adjusting for time-varying covariates of adherence using Inverse Probability Weighting (IPW).",
    "crumbs": [
      "statistics",
      "Papers",
      "Synthesizing Subject-matter Expertise for Variable Selection in Causal Effect Estimation: A Case Study"
    ]
  },
  {
    "objectID": "statistics/debertin_2024_38860706.html#empirical-results",
    "href": "statistics/debertin_2024_38860706.html#empirical-results",
    "title": "Synthesizing Subject-matter Expertise for Variable Selection in Causal Effect Estimation: A Case Study",
    "section": "Empirical Results",
    "text": "Empirical Results\n\nEffect of Nonconfounding Prognostic Factors\nWhen estimating the cumulative effect using only baseline covariates, the results showed that the specific choice of covariates had minimal effect on the (expectedly biased) point estimates. However, including nonconfounding prognostic factors (variables that predict the outcome but not the exposure) led to smaller variance estimates. This finding provides empirical support for the theoretical advice that including prognostic factors increases the efficiency of the causal estimate without introducing bias.\n\n\nEffect of Exposure Predictors\nConversely, when using IPW to adjust for time-varying covariates, adjustment sets that included exposure predictors that were not prognostic factors were shown to result in less bias control.\n\n\nPerformance of DAG Creation Strategies\nOverall, the DAGs that were explicitly created by focusing subject-matter expertise on the identification of potential outcome prognostic factors performed best, particularly in the more complex time-varying covariate scenario using IPW.",
    "crumbs": [
      "statistics",
      "Papers",
      "Synthesizing Subject-matter Expertise for Variable Selection in Causal Effect Estimation: A Case Study"
    ]
  },
  {
    "objectID": "statistics/debertin_2024_38860706.html#conclusions-and-recommendations",
    "href": "statistics/debertin_2024_38860706.html#conclusions-and-recommendations",
    "title": "Synthesizing Subject-matter Expertise for Variable Selection in Causal Effect Estimation: A Case Study",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\n\nConfirmation of Theory\nThe study empirically confirms key theoretical advice regarding causal variable selection:\n\nInclude Prognostic Factors: Identifying and including covariates that are strong predictors of the outcome (prognostic factors) but not predictors of the exposure is highly recommended to reduce variance and increase statistical power.\nCaution with Exposure Predictors: Covariates that are strong predictors of the exposure but not the outcome may interfere with bias control and should be considered with caution.\n\n\n\nPractical Recommendation\nThe paper recommends that researchers and subject-matter experts begin the hand-creation of DAGs with a systematic effort to identify and include all potential outcome prognostic factors, as this strategy proved most effective in constructing a robust adjustment set for causal effect estimation.",
    "crumbs": [
      "statistics",
      "Papers",
      "Synthesizing Subject-matter Expertise for Variable Selection in Causal Effect Estimation: A Case Study"
    ]
  },
  {
    "objectID": "statistics/hernan_2025_39494894.html",
    "href": "statistics/hernan_2025_39494894.html",
    "title": "A Structural Description of Biases That Generate Immortal Time",
    "section": "",
    "text": "PubMed: 39494894 DOI: 10.1097/EDE.0000000000001808 Overview generated by: Gemini 2.5 Flash, 26/11/2025",
    "crumbs": [
      "statistics",
      "Papers",
      "A Structural Description of Biases That Generate Immortal Time"
    ]
  },
  {
    "objectID": "statistics/hernan_2025_39494894.html#key-concepts-and-goal",
    "href": "statistics/hernan_2025_39494894.html#key-concepts-and-goal",
    "title": "A Structural Description of Biases That Generate Immortal Time",
    "section": "Key Concepts and Goal",
    "text": "Key Concepts and Goal\nImmortal time is defined as an event-free period included in a survival analysis during which a person, by definition, cannot experience the event of interest. The authors argue that the term “immortal time bias” is misleading because the bias is not generated by the time itself, but by the underlying selection or misclassification that creates the immortal time. The primary goal of the paper is to review the two mechanisms that produce immortal time and to propose causal diagrams to represent them.\nThe ultimate prevention strategy is Target Trial Emulation, which explicitly specifies eligibility and assignment to the treatment strategies and synchronizes them at the start of follow-up. This alignment prevents the selection and misclassification that lead to immortal time.",
    "crumbs": [
      "statistics",
      "Papers",
      "A Structural Description of Biases That Generate Immortal Time"
    ]
  },
  {
    "objectID": "statistics/hernan_2025_39494894.html#mechanism-1-immortal-time-due-to-selection",
    "href": "statistics/hernan_2025_39494894.html#mechanism-1-immortal-time-due-to-selection",
    "title": "A Structural Description of Biases That Generate Immortal Time",
    "section": "Mechanism 1: Immortal Time Due to Selection",
    "text": "Mechanism 1: Immortal Time Due to Selection\nThis mechanism arises when an eligibility criterion is (incorrectly) applied after the start of follow-up (i.e., after treatment assignment).\n\nStudy Design Failure\nThe issue is created when researchers restrict the analysis to individuals who survived or completed a certain period of follow-up after the initial treatment assignment. For example, if follow-up data for the first 3 months are accidentally deleted, or if an observational analysis is restricted to individuals who have completed 3 months of follow-up, the resulting dataset only contains survivors, creating an “immortal” period where all included individuals survived.\n\n\nResulting Bias\n\nThe selection of surviving individuals results in selection bias due to a differential exclusion of the individuals most susceptible to the outcome in each treatment group.\nIf the follow-up is started at the time of treatment assignment but the immortal period is included, the resulting selection bias is often called “immortal time bias.”\nIf the follow-up is started after the immortal period (e.g., using a landmark approach), the resulting bias is sometimes referred to as “prevalent user bias.”\n\n\n\nSolution\nTo prevent this bias, researchers must ensure that all eligibility criteria are defined at time zero so that no selection occurs after treatment assignment. This is naturally achieved by explicitly emulating a target trial based on data for all eligible individuals from the time of treatment assignment.",
    "crumbs": [
      "statistics",
      "Papers",
      "A Structural Description of Biases That Generate Immortal Time"
    ]
  },
  {
    "objectID": "statistics/hernan_2025_39494894.html#mechanism-2-immortal-time-due-to-misclassification",
    "href": "statistics/hernan_2025_39494894.html#mechanism-2-immortal-time-due-to-misclassification",
    "title": "A Structural Description of Biases That Generate Immortal Time",
    "section": "Mechanism 2: Immortal Time Due to Misclassification",
    "text": "Mechanism 2: Immortal Time Due to Misclassification\nThis mechanism occurs when individuals are misclassified into a treatment group that differs from the one they were assigned to, typically because the treatment strategies under study are not distinguishable at time zero.\n\nStudy Design Failure\nThis arises when a treatment strategy includes a grace period or a waiting period (e.g., “start treatment within 3 months” vs. “never start treatment”). If the assignment indicator (\\(Z\\)) is deleted (or unknown in observational data), researchers might reconstruct an assignment (\\(Z^*\\)) based on whether the individual actually received treatment during the grace period. This forces individuals who were assigned to treatment but died before starting it to be classified into the “no treatment” group.\n\n\nResulting Bias\n\nThis reconstruction uses information on the outcome (survival until treatment) to define the assignment variable \\(Z^*\\), which is a violation of the rule that assignment at time zero must not depend on future outcome values.\nThe resulting error is outcome-dependent misclassification, which makes the treated group look artificially “immortal” during the grace period (as anyone who died before treatment is excluded from the \\(Z^*\\) group).\n\n\n\nSolutions\n\nChange the Causal Question: Re-define the comparison to strategies that are distinguishable at time zero (e.g., comparing immediate treatment to no treatment today).\nCloning Followed by Censoring: Create multiple “clones” of each individual for every treatment strategy compatible with their data at baseline. Each clone is censored if they deviate from the assigned strategy. This approach requires inverse-probability weighting to adjust for the induced selection bias.\nPlug-in G-Formula: A complex estimation approach requiring the modeling of the joint distribution or iterated conditional expectation of time-varying treatment, outcome, and confounders.",
    "crumbs": [
      "statistics",
      "Papers",
      "A Structural Description of Biases That Generate Immortal Time"
    ]
  },
  {
    "objectID": "statistics/hernan_2025_39494894.html#cautionary-note-on-alternative-methods",
    "href": "statistics/hernan_2025_39494894.html#cautionary-note-on-alternative-methods",
    "title": "A Structural Description of Biases That Generate Immortal Time",
    "section": "Cautionary Note on Alternative Methods",
    "text": "Cautionary Note on Alternative Methods\nWhile landmark analysis and person-time analysis can avoid immortal time, they do not explicitly specify the target trial and do not eliminate the fundamental misalignment of eligibility and assignment. These methods can still be susceptible to bias (like selection bias for landmark analysis) or rely on implausible assumptions (like the constant hazard ratio assumption for person-time analysis).",
    "crumbs": [
      "statistics",
      "Papers",
      "A Structural Description of Biases That Generate Immortal Time"
    ]
  },
  {
    "objectID": "multi-omics/atabaki_2025_40502600.html",
    "href": "multi-omics/atabaki_2025_40502600.html",
    "title": "A Biological-Systems-Based Analyses Using Proteomic and Metabolic Network Inference Reveals Mechanistic Insights into Hepatic Lipid Accumulation: An IMI-DIRECT Study",
    "section": "",
    "text": "PubMed: 40502600 DOI: 10.1101/2025.06.02.25328773 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "multi-omics",
      "Papers",
      "A Biological-Systems-Based Analyses Using Proteomic and Metabolic Network Inference Reveals Mechanistic Insights into Hepatic Lipid Accumulation: An IMI-DIRECT Study"
    ]
  },
  {
    "objectID": "multi-omics/atabaki_2025_40502600.html#study-goal-and-context",
    "href": "multi-omics/atabaki_2025_40502600.html#study-goal-and-context",
    "title": "A Biological-Systems-Based Analyses Using Proteomic and Metabolic Network Inference Reveals Mechanistic Insights into Hepatic Lipid Accumulation: An IMI-DIRECT Study",
    "section": "Study Goal and Context",
    "text": "Study Goal and Context\nMetabolic Dysfunction-Associated Steatotic Liver Disease (MASLD), formerly known as NAFLD, affects about a third of the global adult population, with prevalence rising to about 67% in people with Type 2 Diabetes (T2D). The underlying metabolic and proteomic features driving the association between MASLD and T2D are poorly understood.\nThis study aimed to delineate the organ-specific and systemic drivers of MASLD by applying integrative causal inference across clinical, imaging, proteomic, and metabolic domains using data from the IMI-DIRECT prospective cohort study.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "A Biological-Systems-Based Analyses Using Proteomic and Metabolic Network Inference Reveals Mechanistic Insights into Hepatic Lipid Accumulation: An IMI-DIRECT Study"
    ]
  },
  {
    "objectID": "multi-omics/atabaki_2025_40502600.html#methods-multi-omics-causal-network-inference",
    "href": "multi-omics/atabaki_2025_40502600.html#methods-multi-omics-causal-network-inference",
    "title": "A Biological-Systems-Based Analyses Using Proteomic and Metabolic Network Inference Reveals Mechanistic Insights into Hepatic Lipid Accumulation: An IMI-DIRECT Study",
    "section": "Methods: Multi-Omics Causal Network Inference",
    "text": "Methods: Multi-Omics Causal Network Inference\nThe study analyzed data from the IMI-DIRECT cohort consisting of 331 adults with new-onset T2D and 964 adults without diabetes. Participants were comprehensively phenotyped, including:\n\nMetabolic Measures: Glucose and insulin dynamics from frequently-sampled metabolic challenge tests, including the Basal Insulin Secretion Rate (BasalISR) and insulin clearance (Clinsb).\nImaging Measures: MRI-derived fat content in the liver and abdomen (Visceral Adipose Tissue - VAT).\nProteomics: Plasma proteins were quantified using Olink Proximity Extension Assays (446 proteins initially analyzed).\n\nThe core analytical approach involved:\n\nBayesian Network Analysis: Used to quantify potential causal pathways and interactions, generating Directed Acyclic Graphs (DAGs) to suggest cause-and-effect relationships between clinical and protein features.\nMendelian Randomization (MR): Employed as a complementary technique for associations where the Bayesian network could not determine the causal direction with high probability (suggesting bidirectional links).",
    "crumbs": [
      "multi-omics",
      "Papers",
      "A Biological-Systems-Based Analyses Using Proteomic and Metabolic Network Inference Reveals Mechanistic Insights into Hepatic Lipid Accumulation: An IMI-DIRECT Study"
    ]
  },
  {
    "objectID": "multi-omics/atabaki_2025_40502600.html#key-findings-basal-insulin-as-the-causal-driver",
    "href": "multi-omics/atabaki_2025_40502600.html#key-findings-basal-insulin-as-the-causal-driver",
    "title": "A Biological-Systems-Based Analyses Using Proteomic and Metabolic Network Inference Reveals Mechanistic Insights into Hepatic Lipid Accumulation: An IMI-DIRECT Study",
    "section": "Key Findings: Basal Insulin as the Causal Driver",
    "text": "Key Findings: Basal Insulin as the Causal Driver\n\n1. Basal Insulin Hypersecretion Drives Liver Fat Accumulation\nHigh basal insulin secretion rate (BasalISR) was identified as the primary causal driver of liver fat accumulation in both the non-diabetes and T2D cohorts.\n\nIn the non-diabetes network, BasalISR was the parental node for both liver fat and Visceral Adipose Tissue (VAT), suggesting it drives fat accumulation in both ectopic (liver) and visceral areas.\nThe effect of BasalISR on the liver is partially mediated through VAT accumulation.\n\n\n\n2. Bidirectional and Consequence Pathways\n\nVAT-Liver Fat Loop: Excess visceral adipose tissue (VAT) was found to be bidirectionally associated with liver fat, indicating a self-reinforcing metabolic loop.\nInsulin Clearance: Basal insulin clearance (Clinsb) was identified as a consequence (downstream effect) of liver fat accumulation. This worsening of Clinsb was more pronounced before the onset of T2D.\n\n\n\n3. Proteomic Drivers and Sex-Specific Differences\nOf the 446 analyzed proteins, 34 were identified as key components of the metabolic networks (27 in the non-diabetes network, 18 in the T2D network, and 11 common).\n\nDirectly Associated Proteins: Key proteins directly associated with liver fat included GUSB, ALDH1A1, LPL, IGFBP1/2, CTSD, HMOX1, FGF21, AGRP, and ACE2.\nInsulin-IGFBP Axis: The network showed that basal hyperinsulinemia has a direct inverse effect on IGFBP-1 (Insulin-like Growth Factor Binding Protein-1), a suppressive feedback loop consistent with IGFBP-1 being produced by the liver.\nSex-Stratified Drivers: GUSB was the most predictive of liver fat in females, while LEP (Leptin) was most predictive in males, highlighting a sex-specific proteomic architecture of hepatic steatosis.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "A Biological-Systems-Based Analyses Using Proteomic and Metabolic Network Inference Reveals Mechanistic Insights into Hepatic Lipid Accumulation: An IMI-DIRECT Study"
    ]
  },
  {
    "objectID": "multi-omics/atabaki_2025_40502600.html#conclusions-and-recommendations",
    "href": "multi-omics/atabaki_2025_40502600.html#conclusions-and-recommendations",
    "title": "A Biological-Systems-Based Analyses Using Proteomic and Metabolic Network Inference Reveals Mechanistic Insights into Hepatic Lipid Accumulation: An IMI-DIRECT Study",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study concludes that basal insulin hypersecretion is a modifiable, causal driver of MASLD, especially prior to glycemic decompensation.\nThe findings demonstrate a complex, multifactorial, sex- and disease-stage-specific proteo-metabolic architecture of hepatic steatosis. Proteins such as GUSB, ALDH1A1, LPL, and IGFBPs warrant further investigation as potential biomarkers or therapeutic targets for MASLD prevention and treatment.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "A Biological-Systems-Based Analyses Using Proteomic and Metabolic Network Inference Reveals Mechanistic Insights into Hepatic Lipid Accumulation: An IMI-DIRECT Study"
    ]
  },
  {
    "objectID": "multi-omics/okamoto_2025_39901160.html",
    "href": "multi-omics/okamoto_2025_39901160.html",
    "title": "Multi-INTACT: integrative analysis of the genome, transcriptome, and proteome identifies causal mechanisms of complex traits",
    "section": "",
    "text": "PubMed: 39901160 DOI: 10.1186/s13059-025-03480-2 Overview generated by: Gemini 2.5 Flash, 27/11/2025",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Multi-INTACT: integrative analysis of the genome, transcriptome, and proteome identifies causal mechanisms of complex traits"
    ]
  },
  {
    "objectID": "multi-omics/okamoto_2025_39901160.html#background-and-objective",
    "href": "multi-omics/okamoto_2025_39901160.html#background-and-objective",
    "title": "Multi-INTACT: integrative analysis of the genome, transcriptome, and proteome identifies causal mechanisms of complex traits",
    "section": "Background and Objective",
    "text": "Background and Objective\nGenome-Wide Association Studies (GWAS) have localized thousands of single nucleotide polymorphisms (SNPs) associated with complex traits. However, most of these variants reside in non-coding regions, making it challenging to identify the effector genes and the complete molecular cascade by which they influence disease. It is widely hypothesized that genetic variants primarily act by perturbing molecular intermediate traits, such as gene expression (transcriptomics) and protein levels (proteomics).\nThis paper introduces Multi-INTACT (Integrative Analysis of Causal Transcriptome and Proteome), a novel statistical framework designed to: 1. Jointly analyze GWAS, eQTL (expression quantitative trait loci), and pQTL (protein quantitative trait loci) summary statistics. 2. Systematically identify the causal chain from genetic variant to gene expression to protein level, and finally to the complex trait outcome. 3. Quantify the proportion of GWAS heritability mediated by both transcriptional and proteomic regulation.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Multi-INTACT: integrative analysis of the genome, transcriptome, and proteome identifies causal mechanisms of complex traits"
    ]
  },
  {
    "objectID": "multi-omics/okamoto_2025_39901160.html#methods-the-multi-intact-framework",
    "href": "multi-omics/okamoto_2025_39901160.html#methods-the-multi-intact-framework",
    "title": "Multi-INTACT: integrative analysis of the genome, transcriptome, and proteome identifies causal mechanisms of complex traits",
    "section": "Methods: The Multi-INTACT Framework",
    "text": "Methods: The Multi-INTACT Framework\n\nData Integration and Causal Modeling\nMulti-INTACT extends the functionality of the existing INTACT method by incorporating three molecular layers: genome, transcriptome, and proteome.\n\nSNP-to-Gene Expression (eQTL) and SNP-to-Protein (pQTL) Mapping: The method simultaneously estimates the joint effects of all SNPs in a locus on both gene expression and protein abundance.\nCausal Chain Analysis: Multi-INTACT employs a multi-mediator causal model based on the Inverse-Variance Weighted (IVW) method from Mendelian Randomization (MR). It models the complex trait (outcome) as being causally influenced by protein levels (direct mediator) and gene expression (indirect mediator). This allows the tool to distinguish between genetic effects that flow through expression, through protein, or through a combination.\nHeritability Partitioning: The framework also quantifies the proportion of the GWAS heritability explained by the genetic effects on gene expression (\\(\\text{h}^2_E\\)) and on protein levels (\\(\\text{h}^2_P\\)).\n\n\n\nApplication\nThe method was applied to publicly available summary statistics for seven complex traits (e.g., triglycerides, HDL cholesterol) and integrated with large-scale multi-omics datasets from various tissues (e.g., adipose, muscle, liver).",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Multi-INTACT: integrative analysis of the genome, transcriptome, and proteome identifies causal mechanisms of complex traits"
    ]
  },
  {
    "objectID": "multi-omics/okamoto_2025_39901160.html#key-results-and-findings",
    "href": "multi-omics/okamoto_2025_39901160.html#key-results-and-findings",
    "title": "Multi-INTACT: integrative analysis of the genome, transcriptome, and proteome identifies causal mechanisms of complex traits",
    "section": "Key Results and Findings",
    "text": "Key Results and Findings\n\nCausal Genes and Mediation\n\nSignificant Causal Proteins: Multi-INTACT identified numerous proteins whose plasma levels were found to be causally associated with the complex traits studied.\nTranscriptional and Proteomic Mediation: The framework successfully partitioned the heritability, demonstrating that many GWAS signals are indeed mediated by changes in molecular phenotypes. For example, for lipids, the method identified regulatory effects flowing from genetic variants, through gene expression, and subsequently through the regulation of Apolipoproteins (proteins) to affect the final trait.\n\n\n\nNovel Regulatory Mechanisms\nThe joint modeling allowed the authors to uncover specific regulatory mechanisms that would be missed by single-omics or simpler MR approaches:\n\nProtein-Mediated Effects: The analysis confirmed the involvement of genes known to be related to the traits (e.g., LPL and APOE for lipids) and refined the causal path, often showing that the effects are primarily mediated by the protein level, not just the mRNA level.\nNovel Loci: Multi-INTACT uncovered several novel gene-trait associations and provided mechanistic evidence that these associations are driven by molecular regulation.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Multi-INTACT: integrative analysis of the genome, transcriptome, and proteome identifies causal mechanisms of complex traits"
    ]
  },
  {
    "objectID": "multi-omics/okamoto_2025_39901160.html#conclusions-and-significance",
    "href": "multi-omics/okamoto_2025_39901160.html#conclusions-and-significance",
    "title": "Multi-INTACT: integrative analysis of the genome, transcriptome, and proteome identifies causal mechanisms of complex traits",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nMulti-INTACT provides a sophisticated and statistically rigorous framework for performing causal integration of tri-omics data (genome, transcriptome, proteome). It successfully moves beyond simple association to illuminate the molecular mechanism underlying GWAS hits.\nThe ability to jointly model and partition heritability across the transcriptional and proteomic layers is crucial for pinpointing the exact effector genes and the most proximal molecular target for therapeutic intervention, accelerating the translation of GWAS findings into clinical insights.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Multi-INTACT: integrative analysis of the genome, transcriptome, and proteome identifies causal mechanisms of complex traits"
    ]
  },
  {
    "objectID": "multi-omics/argelaguet_2018_29925568.html",
    "href": "multi-omics/argelaguet_2018_29925568.html",
    "title": "Multi-Omics Factor Analysis a framework for unsupervised integration of multi-omics data sets",
    "section": "",
    "text": "PubMed: 29925568 DOI: 10.15252/msb.20178124 Overview generated by: Gemini 2.5 Flash, 27/11/2025",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Multi-Omics Factor Analysis a framework for unsupervised integration of multi-omics data sets"
    ]
  },
  {
    "objectID": "multi-omics/argelaguet_2018_29925568.html#background-and-objective",
    "href": "multi-omics/argelaguet_2018_29925568.html#background-and-objective",
    "title": "Multi-Omics Factor Analysis a framework for unsupervised integration of multi-omics data sets",
    "section": "Background and Objective",
    "text": "Background and Objective\nThe complexity and heterogeneity of multi-omics data (e.g., combining genomics, transcriptomics, and epigenomics) require sophisticated statistical methods for integration. Most existing methods are either limited to two data types or are “supervised,” meaning they require a known outcome variable (like disease status) for analysis. There was a critical need for unsupervised integration methods that can systematically discover the principal sources of variation in multi-omics datasets without prior knowledge of the relevant biological axes.\nThis paper introduces MOFA (Multi-Omics Factor Analysis), a computational framework designed to: 1. Unsupervisedly integrate multiple heterogeneous multi-omics data sets. 2. Infer a set of latent factors (hidden variables) that capture the major axes of biological and technical variability. 3. Disentangle shared heterogeneity (variation common to multiple omics layers) from modality-specific heterogeneity (variation unique to a single omics layer).",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Multi-Omics Factor Analysis a framework for unsupervised integration of multi-omics data sets"
    ]
  },
  {
    "objectID": "multi-omics/argelaguet_2018_29925568.html#methods-the-mofa-framework",
    "href": "multi-omics/argelaguet_2018_29925568.html#methods-the-mofa-framework",
    "title": "Multi-Omics Factor Analysis a framework for unsupervised integration of multi-omics data sets",
    "section": "Methods: The MOFA Framework",
    "text": "Methods: The MOFA Framework\n\nCore Algorithm\nMOFA is based on a general form of Factor Analysis. It models each observed omics data matrix (e.g., RNA expression, DNA methylation) as a linear combination of a small number of shared, hidden latent factors.\n\nShared Factors: These factors simultaneously explain the variability across all data modalities. The model automatically learns which factors are relevant for which omics layer, thus mapping the factors to the data modalities they affect.\nSparsity: The model employs a sparse prior on the factor loadings. This ensures that each factor is defined by only a small, interpretable subset of molecular features (genes, CpGs, etc.), making the resulting factors easier to interpret biologically.\nProbabilistic Framework: Being a fully probabilistic model, MOFA can naturally handle missing values (data imputation), batch effects, and different data distributions (e.g., binary for mutations, continuous for expression).\n\n\n\nApplication: Chronic Lymphocytic Leukemia (CLL)\nMOFA was applied to a large cohort of Chronic Lymphocytic Leukemia (CLL) patient samples, profiled for four molecular modalities: somatic mutations, RNA expression, DNA methylation, and ex vivo drug responses.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Multi-Omics Factor Analysis a framework for unsupervised integration of multi-omics data sets"
    ]
  },
  {
    "objectID": "multi-omics/argelaguet_2018_29925568.html#key-results-and-unsupervised-disentanglement",
    "href": "multi-omics/argelaguet_2018_29925568.html#key-results-and-unsupervised-disentanglement",
    "title": "Multi-Omics Factor Analysis a framework for unsupervised integration of multi-omics data sets",
    "section": "Key Results and Unsupervised Disentanglement",
    "text": "Key Results and Unsupervised Disentanglement\n\nDissecting CLL Heterogeneity\nMOFA successfully identified 12 latent factors that captured the major dimensions of disease heterogeneity in CLL. Crucially, the factors could be grouped:\n\nShared Factors: Factors shared across RNA expression, DNA methylation, and drug responses were strongly associated with known clinical drivers, such as the immunoglobulin heavy chain variable (IGHV) mutation status, a major prognostic marker in CLL.\nModality-Specific Factors: Other factors were specific to a single omics layer, such as a factor that only loaded on somatic mutations and distinguished known mutations like SF3B1.\n\n\n\nDownstream Applications\nThe inferred latent factors enabled several downstream analyses:\n\nSample Clustering: The factors were used to robustly identify subgroups of patients that were characterized by distinct multi-omics profiles.\nFeature Set Identification: The sparse loadings immediately identified the minimal set of key molecular features (genes, CpGs, etc.) that defined each factor and its associated biological axis.\nPrediction and Imputation: The factors could be used for data imputation of missing values and for improved outcome prediction compared to models built on raw data.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Multi-Omics Factor Analysis a framework for unsupervised integration of multi-omics data sets"
    ]
  },
  {
    "objectID": "multi-omics/argelaguet_2018_29925568.html#conclusions-and-significance",
    "href": "multi-omics/argelaguet_2018_29925568.html#conclusions-and-significance",
    "title": "Multi-Omics Factor Analysis a framework for unsupervised integration of multi-omics data sets",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nMOFA is a highly effective, flexible, and scalable statistical framework for the unsupervised integration of multi-omics data sets .\nIts ability to disentangle shared and specific axes of variation is paramount for discovering and interpreting the underlying biological phenomena in complex datasets. By distilling high-dimensional omics data into a small, interpretable set of latent factors, MOFA provides a powerful foundation for precision medicine, subtyping diseases, and gaining mechanistic insights.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Multi-Omics Factor Analysis a framework for unsupervised integration of multi-omics data sets"
    ]
  },
  {
    "objectID": "multi-omics/index.html",
    "href": "multi-omics/index.html",
    "title": "multi-omics",
    "section": "",
    "text": "A Biological-Systems-Based Analyses Using Proteomic and Metabolic Network Inference Reveals Mechanistic Insights into Hepatic Lipid Accumulation: An IMI-DIRECT Study\n\n\n\nObjective: This multi-omics study used Bayesian network analysis and Mendelian Randomization (MR) on the IMI-DIRECT cohort to determine the causal network linking glucose/insulin dynamics, fat distribution (MRI), and plasma proteins to MASLD (liver fat accumulation).\nKey Causal Driver: High Basal Insulin Secretion Rate (BasalISR) was identified as the primary causal driver of liver fat accumulation in both the non-diabetes and Type 2 Diabetes cohorts, suggesting it is a modifiable therapeutic target.\nMechanistic Insights: The study revealed a self-reinforcing bidirectional association between Visceral Adipose Tissue (VAT) and liver fat. It also identified sex-specific proteomic drivers of liver fat, with GUSB being more predictive in females and LEP (Leptin) in males.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nCausal integration of multi-omics data with prior knowledge to generate mechanistic hypotheses\n\n\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nMulti-INTACT: integrative analysis of the genome, transcriptome, and proteome identifies causal mechanisms of complex traits\n\n\n\nMethod: Multi-INTACT is a novel statistical framework that jointly analyzes GWAS, eQTL, and pQTL summary statistics to model the causal chain from genetic variant \\(\\rightarrow\\) gene expression \\(\\rightarrow\\) protein level \\(\\rightarrow\\) complex trait.\nCausal Partitioning: The method successfully partitions GWAS heritability and identifies the precise molecular layer (transcriptome or proteome) mediating the genetic effect, showing that many effects are primarily mediated by protein levels.\nImpact: Applied to complex traits like lipids, Multi-INTACT confirmed known genes and revealed novel gene-trait associations by providing the mechanistic evidence (the specific regulatory path) driving the GWAS signal.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nDIABLO: an integrative approach for identifying key molecular drivers from multi-omics assays\n\n\n\nMethod: DIABLO (Data Integration Analysis for Biomarker discovery using Latent variable approaches for Omics datasets) is a supervised multi-block PLS/GCCA method for joint analysis of heterogeneous omics data.\nFeature Selection: It uses a sparse penalty (L1) to select a minimal set of key molecular drivers that are highly correlated across omics layers and maximally associated with a specific clinical outcome (e.g., disease status).\nImpact: DIABLO demonstrated superior classification accuracy and biological coherence in identifying integrated biomarkers for complex diseases, such as the molecular drivers distinguishing breast cancer subtypes.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nMulti-Omics Factor Analysis a framework for unsupervised integration of multi-omics data sets\n\n\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nMulti-Omic Graph Diagnosis (MOGDx): a data integration tool to perform classification tasks for heterogeneous diseases\n\n\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nIdentifying temporal and spatial patterns of variation from multimodal data using MEFISTO\n\n\n\nMethod: MEFISTO (Multi-omics Factor Analysis Informed by Spatial and Temporal Omics) is an extension of MOFA that uses Gaussian Process (GP) priors on latent factors to model spatial or temporal dependencies between samples.\nKey Capabilities: It performs spatio-temporally informed dimensionality reduction, allowing factors to change smoothly over time/space. It also enables robust interpolation of data for unobserved locations or time points.\nApplication: MEFISTO successfully analyzed data from spatial transcriptomics, longitudinal microbiome studies, and single-cell multi-omics atlases to align and extract common developmental or temporal patterns.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nBenchmarking joint multi-omics dimensionality reduction approaches for the study of cancer\n\n\n\nTopic: A systematic benchmarking of nine joint Dimensionality Reduction (jDR) methods for integrating multi-omics data, using simulated data, TCGA cancer cohorts, and single-cell data.\nKey Findings: intNMF excelled in unsupervised clustering tasks, while MCIA (Multiple Co-Inertia Analysis) was identified as the most robust, all-around performer across various prediction and integration tasks.\nResource: The study created a reproducible code platform called momix to aid researchers in selecting and applying jDR methods, offering practical guidelines for multi-omics integration.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nVariable selection for generalized canonical correlation analysis\n\n\n\nMethod: The paper introduces SGCCA (Sparse Generalized Canonical Correlation Analysis), an extension of the RGCCA framework designed for integrating three or more multi-omics data blocks.\nKey Innovation: SGCCA incorporates a sparse (\\(L_1\\)) penalty to simultaneously perform dimension reduction and variable selection.\nSignificance: SGCCA pinpoints a minimal, highly relevant set of features from each omics layer that drives the shared correlation structure across the integrated datasets, significantly improving the biological interpretability of multi-omics results.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nA fully Bayesian latent variable model for integrative clustering analysis of multi-type omics data\n\n\n\nMethod: The paper proposes a fully Bayesian latent variable model for integrative clustering analysis of multi-omics data, building on the iCluster framework.\nKey Innovation: The Bayesian approach incorporates adaptive shrinkage priors to enforce sparsity (feature selection) on the omics-specific loading matrices, which simultaneously identifies robust disease subtypes and their minimal molecular signatures.\nImpact: Applied to TCGA cancer data, the model demonstrated superior performance in identifying clinically relevant, stable subtypes and the specific genes/loci driving the differences across mRNA, methylation, and CNV data.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nThe tumor multi-omic landscape of endometrial cancers developed on a germline genetic background of adiposity\n\n\n\nObjective: This study used Mendelian randomization (MR) to investigate the causal effect of germline genetic risk for adiposity (BMI) on the multi-omic landscape (gene expression, somatic mutations, and immune microenvironment) of endometrial cancers (EC).\nKey Findings: Genetically predicted higher BMI was causally associated with increased expression of the oncogene MDM2 in EC tumors. It was also linked to a detrimental change in the tumor immune microenvironment, specifically decreasing CD4+ and cytotoxic T cell infiltration.\nConclusion: The study suggests that germline adiposity fuels EC progression by promoting a more immunosuppressive tumor environment and activating key survival pathways, but not by altering the frequency of common somatic mutations.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nStatistical Methods for Integrative Clustering of Multi-omics Data\n\n\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nIntegrating untargeted metabolomics, genetically informed causal inference, and pathway enrichment to define the obesity metabolome\n\n\n\nApproach: A multi-omics framework was developed, combining untargeted metabolomics (measuring both known and unknown metabolites) with two-sample Mendelian Randomization (MR) using metabolite-QTLs (mQTLs).\nCausal Findings: 23 metabolites (15 known and 8 unknown) were identified as causally associated with BMI, with specific pathways like amino acid catabolism and lipid metabolism being implicated in the obesity metabolome.\nInnovation: A novel pathway enrichment method was used to infer the metabolic function of the causally associated unknown metabolites based on their shared genetic links with identified metabolites.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nA General Framework for Integrative Analysis of Incomplete Multi-omics Data\n\n\n\nProblem: Multi-omics analysis is challenged by missing values (incomplete subject profiling) and detection limits (censored data).\nMethod: A general statistical framework based on a joint likelihood function and an Expectation-Maximization (EM) algorithm was developed to rigorously model and integrate multi-omics data while accounting for arbitrary missingness and censoring.\nImpact: Applied to the SPIROMICS cohort, the framework demonstrated superior statistical power and reduced bias compared to ad-hoc imputation methods, particularly in identifying protein quantitative trait loci (pQTLs) and biomarker-phenotype associations.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nBiomarker identification by interpretable maximum mean discrepancy\n\n\n\nTopic: Introduction of SpInOpt-MMD (Sparse, Interpretable, and Optimized Maximum Mean Discrepancy), a novel method for simultaneously performing two-sample testing and biomarker feature selection in high-dimensional omics data.\nMethod: SpInOpt-MMD integrates sparse and interpretable optimization into the Maximum Mean Discrepancy (MMD) test, allowing it to detect statistically significant group differences and identify the features (biomarkers) responsible in a single step.\nImpact: The method is effective for small sample sizes and outperforms other feature selection approaches (like SHAP) in several contexts, offering a powerful, unified approach for biomarker discovery in multi-omics and biomedical applications.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nDisease prediction with multi-omics and biomarkers empowers case-control genetic discoveries in the UK Biobank\n\n\n\nMethod: MILTON (Machine Learning with Phenotype Associations), an ensemble machine-learning framework, was developed to integrate multi-omics (including plasma proteomics) and biomarker data from the UK Biobank to predict disease risk.\nObjective: To demonstrate how these biomarker-based predictions can augment genetic association analyses in a phenome-wide context.\nImpact: MILTON outperformed Polygenic Risk Scores (PRSs) in predicting incident disease. Its application in a PheWAS improved signals for 88 known and 14 novel genetic associations, showing its utility in empowering genetic discovery for complex diseases by improving disease classification.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "multi-omics",
      "Papers"
    ]
  },
  {
    "objectID": "multi-omics/cantini_2021_33402734.html",
    "href": "multi-omics/cantini_2021_33402734.html",
    "title": "Benchmarking joint multi-omics dimensionality reduction approaches for the study of cancer",
    "section": "",
    "text": "PubMed: 33402734 DOI: 10.1038/s41467-020-20430-7 Overview generated by: Gemini 2.5 Flash, 27/11/2025",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Benchmarking joint multi-omics dimensionality reduction approaches for the study of cancer"
    ]
  },
  {
    "objectID": "multi-omics/cantini_2021_33402734.html#background-and-objective",
    "href": "multi-omics/cantini_2021_33402734.html#background-and-objective",
    "title": "Benchmarking joint multi-omics dimensionality reduction approaches for the study of cancer",
    "section": "Background and Objective",
    "text": "Background and Objective\nHigh-dimensional multi-omics data is now standard for understanding complex biological systems like cancer. Joint Dimensionality Reduction (jDR) methods are crucial for the effective integration of these heterogeneous datasets. However, the large number of available jDR methods necessitates a systematic evaluation to provide researchers with reliable guidance on which method to choose for their specific research question.\nThis paper presents a comprehensive benchmark of nine representative joint multi-omics dimensionality reduction approaches to offer practical guidelines for their application, particularly in the study of cancer.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Benchmarking joint multi-omics dimensionality reduction approaches for the study of cancer"
    ]
  },
  {
    "objectID": "multi-omics/cantini_2021_33402734.html#methods-systematic-evaluation",
    "href": "multi-omics/cantini_2021_33402734.html#methods-systematic-evaluation",
    "title": "Benchmarking joint multi-omics dimensionality reduction approaches for the study of cancer",
    "section": "Methods: Systematic Evaluation",
    "text": "Methods: Systematic Evaluation\nThe study systematically evaluated nine representative jDR methods (including MCIA, iCluster, and intNMF) across three complementary benchmark scenarios:\n\nGround-Truth Clustering: Assessing the methods’ ability to retrieve known sample clustering patterns from simulated multi-omics datasets.\nClinical Relevance (TCGA): Using The Cancer Genome Atlas (TCGA) cancer data to evaluate how well the methods’ reduced dimensions predict patient survival, clinical annotations, and enrich for known pathways/biological processes.\nSingle-Cell Classification: Assessing performance in the classification of multi-omics single-cell data.\n\nThe authors also created a reproducible code platform named momix (multi-omics mix), implementing the code developed for this benchmark to support users and future comparative studies.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Benchmarking joint multi-omics dimensionality reduction approaches for the study of cancer"
    ]
  },
  {
    "objectID": "multi-omics/cantini_2021_33402734.html#key-results-and-conclusions",
    "href": "multi-omics/cantini_2021_33402734.html#key-results-and-conclusions",
    "title": "Benchmarking joint multi-omics dimensionality reduction approaches for the study of cancer",
    "section": "Key Results and Conclusions",
    "text": "Key Results and Conclusions\nThe in-depth comparisons provided clear performance distinctions among the nine methods:\n\nBest Clustering Performer: The intNMF (integrated Non-negative Matrix Factorization) method demonstrated the best performance in retrieving ground-truth clustering from simulated data.\nBest All-Rounder: MCIA (Multiple Co-Inertia Analysis) offered effective and consistent behavior across many different contexts and benchmark criteria, suggesting it is a robust general-purpose tool for integration.\nSignificance: The benchmarking study is a critical resource for the multi-omics community, offering data-driven recommendations for selecting appropriate jDR tools based on the specific research question (e.g., whether the goal is clustering or prediction).",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Benchmarking joint multi-omics dimensionality reduction approaches for the study of cancer"
    ]
  },
  {
    "objectID": "multi-omics/mo_2018_30657866.html",
    "href": "multi-omics/mo_2018_30657866.html",
    "title": "A fully Bayesian latent variable model for integrative clustering analysis of multi-type omics data",
    "section": "",
    "text": "PubMed: 28541380 DOI: 10.1093/biostatistics/kxx017 Overview generated by: Gemini 2.5 Flash, 27/11/2025",
    "crumbs": [
      "multi-omics",
      "Papers",
      "A fully Bayesian latent variable model for integrative clustering analysis of multi-type omics data"
    ]
  },
  {
    "objectID": "multi-omics/mo_2018_30657866.html#background-and-objective",
    "href": "multi-omics/mo_2018_30657866.html#background-and-objective",
    "title": "A fully Bayesian latent variable model for integrative clustering analysis of multi-type omics data",
    "section": "Background and Objective",
    "text": "Background and Objective\nThe identification of clinically relevant disease subtypes and their corresponding molecular signatures is a central goal in precision medicine, particularly in cancer research. Large-scale projects like The Cancer Genome Atlas (TCGA) generate vast amounts of heterogeneous multi-omics data (e.g., gene expression, DNA methylation, copy number variation). However, standard clustering methods applied to individual omics layers often yield conflicting or unstable results, failing to capture the comprehensive biological signal.\nThis paper introduces a novel statistical framework based on a fully Bayesian latent variable model for integrative clustering analysis of multi-type omics data. The objective is to simultaneously: 1. Identify robust, shared disease subtypes across all omics platforms. 2. Identify the key molecular features (signatures) from each omics platform that characterize these subtypes.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "A fully Bayesian latent variable model for integrative clustering analysis of multi-type omics data"
    ]
  },
  {
    "objectID": "multi-omics/mo_2018_30657866.html#methods-the-bayesian-integrative-clustering-icluster-framework",
    "href": "multi-omics/mo_2018_30657866.html#methods-the-bayesian-integrative-clustering-icluster-framework",
    "title": "A fully Bayesian latent variable model for integrative clustering analysis of multi-type omics data",
    "section": "Methods: The Bayesian Integrative Clustering (iCluster) Framework",
    "text": "Methods: The Bayesian Integrative Clustering (iCluster) Framework\n\nCore Algorithm: Latent Variable Model\nThe method is an advancement of the iCluster and iClusterPlus frameworks. It assumes that the variation in each omics dataset (\\(\\mathbf{X}_k\\)) for the \\(k\\)-th data type can be explained by a set of shared, hidden (latent) variables (\\(\\mathbf{Z}\\)), plus noise:\n\\[\\mathbf{X}_k = \\mathbf{Z} \\mathbf{W}_k + \\mathbf{E}_k\\]\nwhere \\(\\mathbf{Z}\\) represents the common latent factors (which define the patient clusters) and \\(\\mathbf{W}_k\\) are the omics-specific loading matrices (which define the molecular features).\n\n\nBayesian Implementation and Innovation\nThe key methodological innovations are the implementation of the model using a fully Bayesian approach (via Gibbs sampling) and the incorporation of sparsity priors:\n\nIntegrative Clustering: The latent variables \\(\\mathbf{Z}\\) are clustered using a Dirichlet process prior, which automatically determines the optimal number of clusters (\\(K\\)) that best explains the integrated data structure.\nFeature Selection (Sparsity): Adaptive shrinkage priors are placed on the loading matrices (\\(\\mathbf{W}_k\\)). This is crucial because it drives many of the loadings to zero, effectively performing automatic variable selection and identifying a sparse set of molecular features (genes, loci, etc.) that are most relevant to the cluster assignments. This simultaneously resolves the high-dimensionality problem and provides the subtype-specific molecular signatures.\nHandling Multi-type Data: The Bayesian framework naturally accommodates the different distributional properties of multi-omics data (e.g., continuous expression, count data, binary mutation status).",
    "crumbs": [
      "multi-omics",
      "Papers",
      "A fully Bayesian latent variable model for integrative clustering analysis of multi-type omics data"
    ]
  },
  {
    "objectID": "multi-omics/mo_2018_30657866.html#key-results-and-application",
    "href": "multi-omics/mo_2018_30657866.html#key-results-and-application",
    "title": "A fully Bayesian latent variable model for integrative clustering analysis of multi-type omics data",
    "section": "Key Results and Application",
    "text": "Key Results and Application\n\nApplication to Cancer Data\nThe method was applied to three publicly available cancer datasets (Glioblastoma Multiforme, Lung Squamous Cell Carcinoma, and Endometrial Carcinoma) from TCGA, integrating data types such as: * mRNA expression * DNA methylation * Copy number variation (CNV)\n\n\nRobust Subtype Identification\nThe Bayesian integrative clustering approach demonstrated superior performance in identifying biologically and clinically relevant subtypes compared to methods applied to single omics layers or less sophisticated integrative methods. The inferred clusters showed strong agreement with established cancer subtyping schemes and often refined them.\n\n\nSignature Discovery\nThe sparse loading matrices (\\(\\mathbf{W}_k\\)) successfully identified the specific, highly characteristic molecular signatures for each subtype across the different omics layers, providing a clear biological interpretation of the patient groupings.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "A fully Bayesian latent variable model for integrative clustering analysis of multi-type omics data"
    ]
  },
  {
    "objectID": "multi-omics/mo_2018_30657866.html#conclusions-and-significance",
    "href": "multi-omics/mo_2018_30657866.html#conclusions-and-significance",
    "title": "A fully Bayesian latent variable model for integrative clustering analysis of multi-type omics data",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nThis fully Bayesian latent variable model offers a statistically rigorous and powerful solution for the integrative clustering of multi-omics data . The method’s ability to automatically determine the number of clusters and simultaneously perform sparse feature selection is a major advance. By providing robust disease subtypes and their corresponding minimal molecular signatures, this framework is critical for advancing precision medicine and translational research in complex diseases.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "A fully Bayesian latent variable model for integrative clustering analysis of multi-type omics data"
    ]
  },
  {
    "objectID": "multi-omics/chalise_2023_36929074.html",
    "href": "multi-omics/chalise_2023_36929074.html",
    "title": "Statistical Methods for Integrative Clustering of Multi-omics Data",
    "section": "",
    "text": "PubMed: 36929074 DOI: 10.1007/978-1-0716-2986-4_5 Overview generated by: Gemini 2.5 Flash, 27/11/2025",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Statistical Methods for Integrative Clustering of Multi-omics Data"
    ]
  },
  {
    "objectID": "multi-omics/chalise_2023_36929074.html#background-and-objective",
    "href": "multi-omics/chalise_2023_36929074.html#background-and-objective",
    "title": "Statistical Methods for Integrative Clustering of Multi-omics Data",
    "section": "Background and Objective",
    "text": "Background and Objective\nThe heterogeneity of cancers, driven by complex alterations across multiple molecular levels (genomics, epigenomics, transcriptomics), necessitates advanced statistical methods. Identifying robust molecular subtypes of cancer is a crucial step for personalized medicine. Traditional clustering methods applied to single omics layers often yield unstable results.\nThis paper provides an overview and practical guide to integrative clustering—an unsupervised learning approach that uses multi-omics data to simultaneously identify shared disease subtypes and their associated molecular signatures. The chapter primarily focuses on model-based statistical approaches.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Statistical Methods for Integrative Clustering of Multi-omics Data"
    ]
  },
  {
    "objectID": "multi-omics/chalise_2023_36929074.html#methods-integrative-clustering-taxonomy",
    "href": "multi-omics/chalise_2023_36929074.html#methods-integrative-clustering-taxonomy",
    "title": "Statistical Methods for Integrative Clustering of Multi-omics Data",
    "section": "Methods: Integrative Clustering Taxonomy",
    "text": "Methods: Integrative Clustering Taxonomy\nThe chapter classifies and describes the prominent statistical methods used for integrative clustering of multi-omics data:\n\n1. Model-Based Approaches\nThese methods assume that the variation across different omics datasets can be explained by a set of shared, hidden (latent) variables or factors.\n\niCluster (Integrative Clustering): A latent variable model that uses a joint likelihood function to integrate multiple omics data types. It incorporates penalized regression to enforce sparsity and perform feature selection, identifying the key molecular features that define the clusters.\niClusterPlus (Bayesian Extensions): Enhancements that use Bayesian methods and Dirichlet process priors to improve robustness, handle diverse data distributions (e.g., count, binary), and automatically estimate the optimal number of clusters (\\(K\\)).\n\n\n\n2. Nonparametric Approaches\nThese methods do not rely on specific distributional assumptions.\n\nSimilarity Network Fusion (SNF): This method constructs a similarity network for each individual omics dataset, then iteratively fuses them into a single, comprehensive patient similarity network, which is then clustered to define the final disease subtypes.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Statistical Methods for Integrative Clustering of Multi-omics Data"
    ]
  },
  {
    "objectID": "multi-omics/chalise_2023_36929074.html#conclusions-and-significance",
    "href": "multi-omics/chalise_2023_36929074.html#conclusions-and-significance",
    "title": "Statistical Methods for Integrative Clustering of Multi-omics Data",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nIntegrative clustering is essential for analyzing heterogeneous multi-omics data. By combining information across multiple molecular layers, these methods produce stable and biologically meaningful disease subtypes and their corresponding molecular signatures. The guide provides the necessary foundation and practical steps for researchers to apply these techniques in cancer biology and precision medicine, including data preprocessing, model selection, and biological interpretation (e.g., using Kaplan-Meier curves to assess survival differences between subtypes).",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Statistical Methods for Integrative Clustering of Multi-omics Data"
    ]
  },
  {
    "objectID": "multi-omics/lin_2020_32691502.html",
    "href": "multi-omics/lin_2020_32691502.html",
    "title": "A General Framework for Integrative Analysis of Incomplete Multi-omics Data",
    "section": "",
    "text": "PubMed: 32691502 DOI: 10.1002/gepi.22328 Overview generated by: Gemini 2.5 Flash, 27/11/2025",
    "crumbs": [
      "multi-omics",
      "Papers",
      "A General Framework for Integrative Analysis of Incomplete Multi-omics Data"
    ]
  },
  {
    "objectID": "multi-omics/lin_2020_32691502.html#background-and-objective",
    "href": "multi-omics/lin_2020_32691502.html#background-and-objective",
    "title": "A General Framework for Integrative Analysis of Incomplete Multi-omics Data",
    "section": "Background and Objective",
    "text": "Background and Objective\nThe analysis of modern multi-omics studies is complicated by two major statistical challenges: 1. Missing Data: Not all omics data types (e.g., protein expression, metabolomics) are measured on every subject, often due to cost constraints, leading to partial or incomplete datasets. 2. Detection Limits (Censoring): Quantitative omics measurements frequently involve values that fall below (or above) the detection limit of the instrument, leading to left- (or right-) censoring of the data.\nFailing to properly account for these issues can lead to biased parameter estimates and reduced statistical power in integrative analyses. This paper proposes a general statistical framework to rigorously and powerfully handle missing values and detection limits in the integrative analysis of multi-omics data.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "A General Framework for Integrative Analysis of Incomplete Multi-omics Data"
    ]
  },
  {
    "objectID": "multi-omics/lin_2020_32691502.html#methods-the-integrative-analysis-framework",
    "href": "multi-omics/lin_2020_32691502.html#methods-the-integrative-analysis-framework",
    "title": "A General Framework for Integrative Analysis of Incomplete Multi-omics Data",
    "section": "Methods: The Integrative Analysis Framework",
    "text": "Methods: The Integrative Analysis Framework\n\nModeling Incomplete Data\nThe framework addresses two main analytic goals common in multi-omics studies: 1. Omics-to-Genetics (xQTL): Relating quantitative omics features (e.g., protein or metabolite levels) to genetic variants (SNPs) and covariates using linear regression models. 2. Omics-to-Phenotype: Relating phenotypes (e.g., disease status) to quantitative omics features and covariates using generalized linear models (GLMs).\n\n\nKey Innovation: Joint Likelihood and EM Algorithm\nThe core innovation is the derivation of a joint likelihood function that formally accounts for the incomplete data structure. This joint likelihood allows for:\n\nArbitrary Missingness: The model is valid even when the pattern of missing omics data is complex (i.e., when data is missing at random).\nCensoring: It directly models the values that are below or above detection limits as censored observations, avoiding the need for simple (and often biased) imputation methods like replacing censored values with \\(\\frac{1}{2}\\) the detection limit.\n\nThe authors use an Expectation-Maximization (EM) algorithm to obtain maximum likelihood estimates of all model parameters, efficiently handling the latent (unobserved) censored and missing values.\n\n\nApplication: Emphysema and Blood Biomarkers\nThe method was applied to data from the SPIROMICS cohort, integrating genetic variants (SNPs), circulating blood biomarkers (proteins), and the phenotype emphysema status.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "A General Framework for Integrative Analysis of Incomplete Multi-omics Data"
    ]
  },
  {
    "objectID": "multi-omics/lin_2020_32691502.html#key-results-and-findings",
    "href": "multi-omics/lin_2020_32691502.html#key-results-and-findings",
    "title": "A General Framework for Integrative Analysis of Incomplete Multi-omics Data",
    "section": "Key Results and Findings",
    "text": "Key Results and Findings\n\nSuperior Performance Over Imputation\nThe proposed method consistently demonstrated superior performance compared to standard practice methods that use ad-hoc imputation (e.g., replacing values below the detection limit with \\(\\frac{1}{2}\\) the limit) or methods that simply remove incomplete samples:\n\nReduced Bias: The likelihood-based approach yielded less biased estimates for the effects of biomarkers on emphysema.\nIncreased Power: The framework substantially increased the statistical power to detect associations, particularly when identifying protein quantitative trait loci (pQTLs), where many variants were discovered that were missed by imputation-based approaches.\nRobustness: The method proved robust in handling high rates of missingness and censoring, which are common in proteomic and metabolomic datasets.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "A General Framework for Integrative Analysis of Incomplete Multi-omics Data"
    ]
  },
  {
    "objectID": "multi-omics/lin_2020_32691502.html#conclusions-and-significance",
    "href": "multi-omics/lin_2020_32691502.html#conclusions-and-significance",
    "title": "A General Framework for Integrative Analysis of Incomplete Multi-omics Data",
    "section": "Conclusions and Significance",
    "text": "Conclusions and Significance\nThis paper presents a rigorous and powerful statistical solution for overcoming the challenges of missing data and detection limits in the integrative analysis of multi-omics datasets .\nBy developing a formal joint likelihood framework and using the EM algorithm, the method accurately models the relationships between genetic variants, omics features, and clinical phenotypes. This advancement is critical for improving the quality and interpretability of findings in large-scale multi-omics studies and accelerating the discovery of genetic determinants and molecular mediators of complex diseases.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "A General Framework for Integrative Analysis of Incomplete Multi-omics Data"
    ]
  },
  {
    "objectID": "multi-omics/garg_2024_39261665.html",
    "href": "multi-omics/garg_2024_39261665.html",
    "title": "Disease prediction with multi-omics and biomarkers empowers case-control genetic discoveries in the UK Biobank",
    "section": "",
    "text": "PubMed: 39261665 DOI: 10.1038/s41588-024-01898-1 Overview generated by: Gemini 2.5 Flash, 27/11/2025",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Disease prediction with multi-omics and biomarkers empowers case-control genetic discoveries in the UK Biobank"
    ]
  },
  {
    "objectID": "multi-omics/garg_2024_39261665.html#background-and-objective",
    "href": "multi-omics/garg_2024_39261665.html#background-and-objective",
    "title": "Disease prediction with multi-omics and biomarkers empowers case-control genetic discoveries in the UK Biobank",
    "section": "Background and Objective",
    "text": "Background and Objective\nBiobank-level datasets, such as the UK Biobank, offer unprecedented opportunities to discover novel biomarkers and develop powerful predictive algorithms for human disease. The challenge lies in effectively integrating diverse, multi-level data (genomics, proteomics, clinical records) to simultaneously improve disease prediction and the statistical power of genetic discovery.\nThis study introduces MILTON (Machine Learning with Phenotype Associations), an ensemble machine-learning framework designed to predict a wide range of diseases using multi-omics and clinical biomarkers. The main objective is to demonstrate how these accurate, biomarker-based predictions can augment case-control genetic association studies.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Disease prediction with multi-omics and biomarkers empowers case-control genetic discoveries in the UK Biobank"
    ]
  },
  {
    "objectID": "multi-omics/garg_2024_39261665.html#methods-the-milton-framework",
    "href": "multi-omics/garg_2024_39261665.html#methods-the-milton-framework",
    "title": "Disease prediction with multi-omics and biomarkers empowers case-control genetic discoveries in the UK Biobank",
    "section": "Methods: The MILTON Framework",
    "text": "Methods: The MILTON Framework\nMILTON is an ensemble machine-learning framework that integrates diverse data types to predict disease status.\n\nData Integration: The framework was developed using the UK Biobank, integrating matched plasma proteomics data (from 46,327 samples) and other biomarkers with genetic data (from 484,230 genome-sequenced samples).\nPrediction Task: MILTON was trained to predict 3,213 incident disease cases—cases that were undiagnosed at the time of recruitment—by leveraging the UK Biobank’s longitudinal health record data.\nAugmenting Genetics: The highly accurate disease predictions from MILTON were then used to refine the case and control definitions in a phenome-wide association study (PheWAS). By improving disease classification through biomarker-based prediction, MILTON effectively enhances the statistical power of the genetic association analyses.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Disease prediction with multi-omics and biomarkers empowers case-control genetic discoveries in the UK Biobank"
    ]
  },
  {
    "objectID": "multi-omics/garg_2024_39261665.html#key-results-and-significance",
    "href": "multi-omics/garg_2024_39261665.html#key-results-and-significance",
    "title": "Disease prediction with multi-omics and biomarkers empowers case-control genetic discoveries in the UK Biobank",
    "section": "Key Results and Significance",
    "text": "Key Results and Significance\nMILTON demonstrated substantial efficacy in both prediction and genetic discovery:\n\nSuperior Prediction: MILTON significantly outperformed available polygenic risk scores (PRSs) in predicting incident disease cases, especially for diseases with strong molecular links.\nEmpowered Genetic Discovery: When applied to the PheWAS, the framework successfully augmented genetic association analyses. This resulted in improved signals for 88 known genetic associations and led to the discovery of 14 novel genetic associations.\nTargeted Improvement: The framework showed the largest improvement in genetic discovery for diseases characterized by lower PRS prediction accuracy but higher biomarker prediction accuracy.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Disease prediction with multi-omics and biomarkers empowers case-control genetic discoveries in the UK Biobank"
    ]
  },
  {
    "objectID": "multi-omics/garg_2024_39261665.html#conclusion",
    "href": "multi-omics/garg_2024_39261665.html#conclusion",
    "title": "Disease prediction with multi-omics and biomarkers empowers case-control genetic discoveries in the UK Biobank",
    "section": "Conclusion",
    "text": "Conclusion\nThe MILTON framework provides a powerful and practical approach to leverage deep molecular phenotyping, including multi-omics data, for disease prediction. Crucially, it demonstrates a successful strategy to empower case-control genetic discoveries by refining disease classification, which will accelerate the understanding of the underlying mechanisms of human diseases.",
    "crumbs": [
      "multi-omics",
      "Papers",
      "Disease prediction with multi-omics and biomarkers empowers case-control genetic discoveries in the UK Biobank"
    ]
  },
  {
    "objectID": "metabolomics/do_2018_30830398.html",
    "href": "metabolomics/do_2018_30830398.html",
    "title": "Characterization of missing values in untargeted MS-based metabolomics data and evaluation of missing data handling strategies",
    "section": "",
    "text": "PubMed: 30830398 DOI: 10.1007/s11306-018-1420-2 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Characterization of missing values in untargeted MS-based metabolomics data and evaluation of missing data handling strategies"
    ]
  },
  {
    "objectID": "metabolomics/do_2018_30830398.html#key-focus-the-problem-of-missing-values-in-untargeted-metabolomics",
    "href": "metabolomics/do_2018_30830398.html#key-focus-the-problem-of-missing-values-in-untargeted-metabolomics",
    "title": "Characterization of missing values in untargeted MS-based metabolomics data and evaluation of missing data handling strategies",
    "section": "Key Focus: The Problem of Missing Values in Untargeted Metabolomics",
    "text": "Key Focus: The Problem of Missing Values in Untargeted Metabolomics\nThis study provides a systematic characterization of missing values (MVs) in untargeted mass spectrometry (MS)-based metabolomics data and evaluates various strategies for handling these MVs. Missing data is a common issue that can severely reduce statistical power and introduce bias in downstream biomedical studies.\n\nCharacterization of Missing Values\nThe study distinguished two primary types of MVs based on their origin:\n\nMissing Due to Limits of Detection (LOD): This is the most prevalent form and is often systematic. It occurs when a compound’s concentration is below the instrument’s detection threshold. This systematic pattern can be further influenced by run day-dependent effects (e.g., changes in instrument performance over time).\nMissing at Random (MAR) or Completely at Random (MCAR): These MVs occur less frequently and are typically a consequence of random technical errors during sample preparation or measurement (e.g., ionization suppression, inconsistent retention time).\n\n\n\nEvaluation of Imputation Strategies\nThe study evaluated several common imputation strategies, including:\n\nFixed-Value Imputation: Replacing MVs with a fixed constant, such as zero, the mean, or a value derived from the limit of detection (LOD) (e.g., half the minimum detected value).\nData-Driven Imputation: Using statistical or machine learning methods based on the observed data, such as Probabilistic Principal Component Analysis (PPCA) or \\(k\\)-Nearest Neighbors (kNN).\n\n\n\nBest Performing Strategy\nThe key finding regarding MV handling was that the best strategy depends on the nature of the missingness:\n\nFor LOD-related MVs (Systematic Missingness): Simple methods like half-of-the-minimum-observed-value imputation performed surprisingly well and often outperformed more complex data-driven methods. This is because LOD-MVs are not truly random but represent a biological value that is near zero.\nFor MAR/MCAR MVs (Random Missingness): Data-driven methods like PPCA and kNN generally performed better than fixed-value methods. However, given that LOD-MVs dominate untargeted metabolomics, the overall benefit of complex methods was limited.",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Characterization of missing values in untargeted MS-based metabolomics data and evaluation of missing data handling strategies"
    ]
  },
  {
    "objectID": "metabolomics/do_2018_30830398.html#conclusions-and-recommendations",
    "href": "metabolomics/do_2018_30830398.html#conclusions-and-recommendations",
    "title": "Characterization of missing values in untargeted MS-based metabolomics data and evaluation of missing data handling strategies",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study emphasizes that the high proportion of systematic, LOD-related missingness is a characteristic feature of untargeted MS-based metabolomics.\nResearchers should: 1. Characterize Missingness: Visually inspect data to distinguish systematic LOD-MVs from random MVs. 2. Apply Targeted Imputation: For the high number of LOD-MVs, use a simple, robust method like imputation with a small value (e.g., half the minimum). 3. Future Development: The authors call for the development of new, tailored imputation methods that can explicitly and simultaneously model both the systematic (LOD) and random (MAR/MCAR) components of missingness in metabolomics data.",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Characterization of missing values in untargeted MS-based metabolomics data and evaluation of missing data handling strategies"
    ]
  },
  {
    "objectID": "metabolomics/sullivan_2016_27658530.html",
    "href": "metabolomics/sullivan_2016_27658530.html",
    "title": "Altered metabolite levels in cancer: implications for tumour biology and cancer therapy",
    "section": "",
    "text": "PubMed: 27658530 DOI: 10.1038/nrc.2016.85 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Altered metabolite levels in cancer: implications for tumour biology and cancer therapy"
    ]
  },
  {
    "objectID": "metabolomics/sullivan_2016_27658530.html#key-focus-metabolite-concentration-as-a-driver-of-cancer-biology",
    "href": "metabolomics/sullivan_2016_27658530.html#key-focus-metabolite-concentration-as-a-driver-of-cancer-biology",
    "title": "Altered metabolite levels in cancer: implications for tumour biology and cancer therapy",
    "section": "Key Focus: Metabolite Concentration as a Driver of Cancer Biology",
    "text": "Key Focus: Metabolite Concentration as a Driver of Cancer Biology\nThis review explores how altered intracellular metabolite concentrations—a fundamental characteristic of cancer cells (metabolic reprogramming)—can actively promote tumor initiation, progression, and survival, rather than merely being a consequence of altered metabolism.\n\nThe Oncogenic Role of Metabolites\nThe core concept is that changes in the concentration of specific metabolites, often driven by genetic mutations or cancer-associated protein modifications, can act as oncometabolites or signaling molecules. These altered levels can then directly impact cell fate by modifying proteins, regulating gene expression, and altering redox balance.",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Altered metabolite levels in cancer: implications for tumour biology and cancer therapy"
    ]
  },
  {
    "objectID": "metabolomics/sullivan_2016_27658530.html#major-oncometabolites-and-their-mechanisms",
    "href": "metabolomics/sullivan_2016_27658530.html#major-oncometabolites-and-their-mechanisms",
    "title": "Altered metabolite levels in cancer: implications for tumour biology and cancer therapy",
    "section": "Major Oncometabolites and Their Mechanisms",
    "text": "Major Oncometabolites and Their Mechanisms\nThe review details several key metabolites whose altered levels have profound implications for cancer:\n\n1. 2-Hydroxyglutarate (2-HG)\n\nSource: Produced at high levels by mutations in Isocitrate Dehydrogenase 1 (IDH1) or IDH2.\nMechanism: 2-HG is an “oncometabolite” that functions as a potent competitive inhibitor of several \\(\\alpha\\)-ketoglutarate (\\(\\alpha\\)-KG)-dependent dioxygenases, including epigenetic regulators like TET DNA demethylases and histone demethylases.\nEffect: This inhibition leads to a hypermethylation phenotype and globally altered gene expression, promoting oncogenesis.\n\n\n\n2. Fumarate and Succinate\n\nSource: Accumulate due to mutations in the tricarboxylic acid (TCA) cycle enzymes Fumarate Hydratase (FH) and Succinate Dehydrogenase (SDH).\nMechanism: Similar to 2-HG, these two metabolites are also \\(\\alpha\\)-KG competitive inhibitors. They inhibit \\(\\alpha\\)-KG-dependent prolyl hydroxylase (PHD) enzymes.\nEffect: Inhibition of PHDs stabilizes the transcription factor Hypoxia-Inducible Factor 1\\(\\alpha\\) (HIF-\\(1\\alpha\\)). This leads to the activation of the Warburg effect and promotes cell proliferation and angiogenesis, even in normoxic conditions (pseudohypoxia).\n\n\n\n3. Aspartate\n\nSource: Can be limited in tumor environments, leading to reduced cell proliferation.\nEffect: Aspartate is a critical precursor for the synthesis of nucleotides (purines and pyrimidines). Its availability links the rate of mitochondrial metabolism (TCA cycle) directly to cell proliferation, acting as a metabolic checkpoint.\n\n\n\n4. Reactive Oxygen Species (ROS)\n\nSource: Increased ROS production due to altered mitochondrial function and high metabolic flux.\nDual Role: While high ROS levels can induce cell death, moderate, sustained increases in ROS can promote tumorigenesis by activating pro-survival signaling pathways and contributing to DNA damage.",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Altered metabolite levels in cancer: implications for tumour biology and cancer therapy"
    ]
  },
  {
    "objectID": "metabolomics/sullivan_2016_27658530.html#therapeutic-implications",
    "href": "metabolomics/sullivan_2016_27658530.html#therapeutic-implications",
    "title": "Altered metabolite levels in cancer: implications for tumour biology and cancer therapy",
    "section": "Therapeutic Implications",
    "text": "Therapeutic Implications\nUnderstanding the altered metabolome provides clear therapeutic vulnerabilities:\n\nTargeting Metabolite Effects: Drugs can be developed to counteract the downstream effects of oncometabolites (e.g., targeting the epigenetic readers or writers whose activity is modified by 2-HG).\nExploiting Dependencies: Cancer cells often become dependent on specific nutrients or pathways due to metabolic constraints (e.g., relying on external aspartate). Inhibiting the transport or synthesis of these essential metabolites could selectively kill tumor cells.",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Altered metabolite levels in cancer: implications for tumour biology and cancer therapy"
    ]
  },
  {
    "objectID": "metabolomics/sullivan_2016_27658530.html#conclusion",
    "href": "metabolomics/sullivan_2016_27658530.html#conclusion",
    "title": "Altered metabolite levels in cancer: implications for tumour biology and cancer therapy",
    "section": "Conclusion",
    "text": "Conclusion\nThe review concludes that changes in intracellular metabolite concentrations are a central feature of cancer cell biology, acting as effector molecules that dictate cancer phenotype. Metabolomics is thus vital for identifying new therapeutic targets and understanding the underlying mechanisms of malignancy.",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Altered metabolite levels in cancer: implications for tumour biology and cancer therapy"
    ]
  },
  {
    "objectID": "metabolomics/amara_2022_35350714.html",
    "href": "metabolomics/amara_2022_35350714.html",
    "title": "Networks and Graphs Discovery in Metabolomics Data Analysis and Interpretation",
    "section": "",
    "text": "PubMed: 35350714 DOI: 10.3389/fmolb.2022.841373 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Networks and Graphs Discovery in Metabolomics Data Analysis and Interpretation"
    ]
  },
  {
    "objectID": "metabolomics/amara_2022_35350714.html#key-focus-the-application-of-graph-theory-in-metabolomics",
    "href": "metabolomics/amara_2022_35350714.html#key-focus-the-application-of-graph-theory-in-metabolomics",
    "title": "Networks and Graphs Discovery in Metabolomics Data Analysis and Interpretation",
    "section": "Key Focus: The Application of Graph Theory in Metabolomics",
    "text": "Key Focus: The Application of Graph Theory in Metabolomics\nThis review article provides a comprehensive overview of how networks and graph theory are used as analytical and interpretive tools in metabolomics data analysis. It focuses on the shift from viewing the metabolome as a list of molecules to a structured system of interactions, which is crucial for making biological sense of complex high-throughput data.\n\nThe Role of Graph Theory\nIn metabolomics, graphs (or networks) are mathematical structures used to represent relationships between two entities: * Nodes (Vertices): Represent the metabolites, genes, proteins, samples, or analytical features (e.g., MS ions). * Edges (Links): Represent the connections or relationships between the nodes, which can be chemical similarity, biological correlation, co-occurrence, or a known reaction.",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Networks and Graphs Discovery in Metabolomics Data Analysis and Interpretation"
    ]
  },
  {
    "objectID": "metabolomics/amara_2022_35350714.html#network-types-and-their-applications",
    "href": "metabolomics/amara_2022_35350714.html#network-types-and-their-applications",
    "title": "Networks and Graphs Discovery in Metabolomics Data Analysis and Interpretation",
    "section": "Network Types and Their Applications",
    "text": "Network Types and Their Applications\nThe review categorizes network applications into two main areas based on the type of data they analyze:\n\n1. Analytical/Chemical Networks\nThese networks are derived directly from mass spectrometry (MS) data and are primarily used for compound identification and annotation.\n\nMolecular Networking (MN): This is the most prominent example. It connects MS/MS spectra based on the similarity of their fragmentation patterns. This allows researchers to group structurally related metabolites (e.g., compounds in the same chemical family) into clusters, enabling the identification of unknown members once one member of the cluster is known\n\n[Image of Molecular Networking Graph] . * Feature-Based Molecular Networking (FBMN): An extension that integrates chromatographic and quantitative data for more robust connections.\n\n\n2. Biological/Correlation Networks\nThese networks are derived from quantitative abundance data and are primarily used for biological interpretation and pathway discovery.\n\nMetabolite-Metabolite Correlation Networks: Nodes are metabolites, and edges represent a significant statistical correlation in their concentration changes across different samples or conditions. These correlations can indicate shared regulation, sequential reactions in a metabolic pathway, or common transporters.\nMetabolite-Pathway Networks: These map identified metabolites onto established biochemical pathways (e.g., KEGG, MetaCyc) to visualize which pathways are perturbed in a given experiment.\nIntegrated Omics Networks: Graphs are essential for multi-omics integration, connecting metabolites to other molecular entities like transcripts (mRNA) and proteins. The edges often represent genetic co-expression, shared regulation, or enzyme-substrate relationships.",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Networks and Graphs Discovery in Metabolomics Data Analysis and Interpretation"
    ]
  },
  {
    "objectID": "metabolomics/amara_2022_35350714.html#key-advantages",
    "href": "metabolomics/amara_2022_35350714.html#key-advantages",
    "title": "Networks and Graphs Discovery in Metabolomics Data Analysis and Interpretation",
    "section": "Key Advantages",
    "text": "Key Advantages\nThe application of graph theory offers significant advantages for metabolomics: * Discovery of Hidden Relationships: Networks can reveal complex, non-linear relationships that are missed by traditional univariate statistics. * Hypothesis Generation: Network hubs (highly connected nodes) often represent key regulatory or rate-limiting enzymes and metabolites, pointing to critical biological control points. * Visualization: They provide an intuitive and powerful way to visualize and communicate complex, high-dimensional data.",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Networks and Graphs Discovery in Metabolomics Data Analysis and Interpretation"
    ]
  },
  {
    "objectID": "metabolomics/amara_2022_35350714.html#conclusion",
    "href": "metabolomics/amara_2022_35350714.html#conclusion",
    "title": "Networks and Graphs Discovery in Metabolomics Data Analysis and Interpretation",
    "section": "Conclusion",
    "text": "Conclusion\nThe review concludes that network and graph-based approaches are fundamental to modern metabolomics, bridging the gap between raw analytical data and comprehensive biological understanding. They are crucial for moving the field forward, especially in the context of integrating data from multiple omics layers.",
    "crumbs": [
      "metabolomics",
      "Papers",
      "Networks and Graphs Discovery in Metabolomics Data Analysis and Interpretation"
    ]
  },
  {
    "objectID": "mr/cupido_2024_39147365.html",
    "href": "mr/cupido_2024_39147365.html",
    "title": "Specific approaches and limitations in (multi)-omic Mendelian randomization",
    "section": "",
    "text": "PubMed: 39147365 DOI: 10.1016/j.jlr.2024.100619 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "MR",
      "Papers",
      "Specific approaches and limitations in (multi)-omic Mendelian randomization"
    ]
  },
  {
    "objectID": "mr/cupido_2024_39147365.html#introduction-to-mendelian-randomization-in-the-multi-omics-era",
    "href": "mr/cupido_2024_39147365.html#introduction-to-mendelian-randomization-in-the-multi-omics-era",
    "title": "Specific approaches and limitations in (multi)-omic Mendelian randomization",
    "section": "Introduction to Mendelian Randomization in the Multi-Omics Era",
    "text": "Introduction to Mendelian Randomization in the Multi-Omics Era\nMendelian Randomization (MR) is a statistical technique that uses genetic variants as instrumental variables (IVs) to assess the causal effect of an exposure (X) on an outcome (Y). This approach minimizes confounding and reverse causation inherent in traditional observational studies. Given the explosion of multi-omics data (proteomics, metabolomics, transcriptomics), MR is increasingly applied, making it crucial to understand the specific limitations and necessary approaches to ensure valid causal inference.",
    "crumbs": [
      "MR",
      "Papers",
      "Specific approaches and limitations in (multi)-omic Mendelian randomization"
    ]
  },
  {
    "objectID": "mr/cupido_2024_39147365.html#specific-approaches-to-address-key-mr-limitations",
    "href": "mr/cupido_2024_39147365.html#specific-approaches-to-address-key-mr-limitations",
    "title": "Specific approaches and limitations in (multi)-omic Mendelian randomization",
    "section": "Specific Approaches to Address Key MR Limitations",
    "text": "Specific Approaches to Address Key MR Limitations\nThe core challenge in MR is validating the IV assumptions, particularly the assumption of no pleiotropy (the IV affects the outcome only through the exposure). The authors review strategies to enhance the reliability of MR studies, particularly in the multi-omics context.\n\n1. Consideration of Biological Pathways (Addressing Pleiotropy)\nResearchers must have a clear research question and deeply consider the biological relevance of their exposure variables and the selected genetic instruments.\n\nBiologically Motivated IV Selection: A strategy focusing on variants in or near the gene encoding the exposure (e.g., a protein or a regulator of that protein) is generally preferred over a broad, genome-wide approach. This helps limit the potential for the IV to affect the outcome through separate pathways.\nExample: IL-6 Signaling: The effect of inhibiting Interleukin-6 (IL-6) signaling on Cardiovascular Disease (CVD) can be studied using variants in the IL6R gene (encoding the receptor). However, these variants not only lower IL-6 signaling but also increase systemic IL-6 levels. When using a GWAS of IL-6 levels as the exposure, researchers must exclude the IL6R region to avoid spurious results caused by this type of vertical pleiotropy.\n\n\n\n2. Consideration of Measurement Techniques (Addressing Data Quality)\nThe reliability of MR findings is tied directly to the quality and resolution of the data used for the exposure GWAS.\n\nMicrobiome Data: MR studies using 16S genomic data for the microbiome must be cautious, as this sequencing technique often lacks the depth to reliably quantify bacteria down to the species level, potentially invalidating basic MR assumptions.\nLipidomics/Metabolomics Data: GWAS data derived from lipidomic or metabolomic techniques that are unable to separate isomers can introduce biases and increase the risk of pleiotropy.\n\n\n\n3. Addressing Weak Instrument Bias (The Relevance Assumption)\nThe relevance assumption states that the genetic IV must be robustly associated with the exposure. Violation of this assumption leads to weak instrument bias, which pulls the causal estimate toward the null.\n\nAssessment: The strength of the instrument is appraised using the F-statistic and the proportion of variance explained (\\(r^2\\)).\nMitigation: To increase power and minimize bias, researchers often combine multiple independent genetic variants (Single Nucleotide Polymorphisms or SNPs) into genetic risk scores or use multi-variant meta-analysis approaches.",
    "crumbs": [
      "MR",
      "Papers",
      "Specific approaches and limitations in (multi)-omic Mendelian randomization"
    ]
  },
  {
    "objectID": "mr/cupido_2024_39147365.html#conclusion",
    "href": "mr/cupido_2024_39147365.html#conclusion",
    "title": "Specific approaches and limitations in (multi)-omic Mendelian randomization",
    "section": "Conclusion",
    "text": "Conclusion\nThe review emphasizes that while MR remains a powerful tool for causal inference, its application in the multi-omics field requires rigorous attention to the biological and technical details of the data and the genetic instruments chosen. Adopting specific, biologically-motivated approaches to IV selection and critically appraising the limitations of measurement techniques are essential steps for generating reliable and interpretable results.",
    "crumbs": [
      "MR",
      "Papers",
      "Specific approaches and limitations in (multi)-omic Mendelian randomization"
    ]
  },
  {
    "objectID": "mr/leyden_2022_35090585.html",
    "href": "mr/leyden_2022_35090585.html",
    "title": "Harnessing tissue-specific genetic variation to dissect putative causal pathways between body mass index and cardiometabolic phenotypes",
    "section": "",
    "text": "PubMed: 35090585 DOI: 10.1016/j.ajhg.2021.12.013 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "MR",
      "Papers",
      "Harnessing tissue-specific genetic variation to dissect putative causal pathways between body mass index and cardiometabolic phenotypes"
    ]
  },
  {
    "objectID": "mr/leyden_2022_35090585.html#key-findings-tissue-specific-genetic-effects-of-bmi",
    "href": "mr/leyden_2022_35090585.html#key-findings-tissue-specific-genetic-effects-of-bmi",
    "title": "Harnessing tissue-specific genetic variation to dissect putative causal pathways between body mass index and cardiometabolic phenotypes",
    "section": "Key Findings: Tissue-Specific Genetic Effects of BMI",
    "text": "Key Findings: Tissue-Specific Genetic Effects of BMI\nThis two-sample Mendelian randomization (MR) study aimed to distinguish the effects of BMI-associated genetic variants that act via metabolic pathways (proxied by subcutaneous adipose tissue expression) from those that act via appetite regulation (proxied by brain tissue expression) on various cardiometabolic phenotypes.\n\nDistinct Gene Sets Identified: The study successfully identified two distinct sets of BMI-associated genetic instruments:\n\nAdipose-Mediated Set (Metabolic): 8 variants (e.g., near IRS1, IGF2BP2) associated with gene expression in subcutaneous adipose tissue.\nBrain-Mediated Set (Appetite): 11 variants (e.g., near BDNF, FTO) associated with gene expression in the brain.\n\nBrain-Mediated Pathway and T2DM: The brain-mediated genetic variants were found to be more strongly associated with Type 2 Diabetes Mellitus (T2DM) risk compared to the adipose-mediated variants. This suggests that the genetic propensity for higher BMI driven by appetite regulation in the brain is a major driver of T2DM risk.\n\nThe brain-mediated set resulted in an Odds Ratio (OR) of 1.43 (95% CI 1.30; 1.57) per standard deviation (SD) increase in BMI, whereas the adipose-mediated set OR was 1.15 (95% CI 1.05; 1.25).\n\nSimilar Effects on Lipids and Blood Pressure: Both the adipose-mediated and brain-mediated genetic sets were similarly associated with detrimental effects on blood lipids (LDL-cholesterol, triglycerides) and blood pressure, suggesting these consequences of BMI are likely shared downstream effects regardless of the initial genetic pathway (appetite vs. metabolism).",
    "crumbs": [
      "MR",
      "Papers",
      "Harnessing tissue-specific genetic variation to dissect putative causal pathways between body mass index and cardiometabolic phenotypes"
    ]
  },
  {
    "objectID": "mr/leyden_2022_35090585.html#study-design-and-methods",
    "href": "mr/leyden_2022_35090585.html#study-design-and-methods",
    "title": "Harnessing tissue-specific genetic variation to dissect putative causal pathways between body mass index and cardiometabolic phenotypes",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\n\nStudy Design\nThis was a two-sample Mendelian randomization (MR) study. The analysis partitioned the overall genetic liability for BMI into two hypothesized causal pathways (adipose-mediated and brain-mediated) to examine their downstream effects on specific cardiometabolic outcomes.\n\n\nData and Genetic Instrument Selection\n\nExposure GWAS (BMI): Summary statistics for BMI were taken from a large-scale GWAS meta-analysis (Locke et al., 2015).\nTissue-Specific eQTL Data (Mediators): Genetic variants were prioritized based on their association with gene expression (eQTLs) in two relevant tissues:\n\nSubcutaneous Adipose Tissue (Metabolic): Expression data from a meta-analysis (n=1257).\nBrain Tissue (Appetite/Central): Expression data from the GTEx consortium (n=1194).\n\nInstrument Selection: Instruments were selected as cis-eQTLs associated with both BMI and gene expression in a specific tissue. Strict quality control and linkage disequilibrium (LD) clumping were applied. This procedure resulted in the final, mostly non-overlapping 8 adipose-mediated and 11 brain-mediated genetic instruments.\n\n\n\nOutcome GWAS Data\nOutcome data were sourced from large-scale consortia GWAS for:\n\nType 2 Diabetes Mellitus (T2DM)\nCoronary Artery Disease (CAD)\nLipid traits (LDL-C, HDL-C, Triglycerides)\nBlood pressure (systolic and diastolic)\n\n\n\nStatistical Analysis\n\nPrimary MR Method: The Inverse-Variance Weighted (IVW) method was used to combine the effects of the variants within each tissue-specific set (adipose vs. brain).\nSensitivity Analyses: MR-Egger and Weighted Median methods were used to test for horizontal pleiotropy. MR-PRESSO was used to detect and correct for outliers. The results were robust across these sensitivity methods.\nTesting for Difference in Effects: A formal statistical test was applied to compare the strength of the causal effect estimates between the adipose-mediated and brain-mediated gene sets on each outcome.",
    "crumbs": [
      "MR",
      "Papers",
      "Harnessing tissue-specific genetic variation to dissect putative causal pathways between body mass index and cardiometabolic phenotypes"
    ]
  },
  {
    "objectID": "mr/leyden_2022_35090585.html#conclusions-and-implications",
    "href": "mr/leyden_2022_35090585.html#conclusions-and-implications",
    "title": "Harnessing tissue-specific genetic variation to dissect putative causal pathways between body mass index and cardiometabolic phenotypes",
    "section": "Conclusions and Implications",
    "text": "Conclusions and Implications\nThe study provides compelling evidence that the genetic pathways influencing BMI have differential downstream causal effects on cardiometabolic outcomes, particularly T2DM.\n\nTargeted Interventions: The strong link between brain-mediated genetic risk (appetite regulation) and T2DM suggests that therapeutic or preventative interventions targeting central regulation of appetite may be particularly effective in reducing T2DM risk among genetically predisposed individuals.\nBiological Dissection: The methodology effectively dissects the genetic architecture of a complex trait (BMI), linking tissue-specific molecular effects to disease risk. This approach offers a powerful strategy for prioritizing therapeutic targets based on biological mechanism.\nFuture Work: The authors recommend expanding the tissue types analyzed to include visceral adipose tissue and other relevant metabolic organs (e.g., liver) to provide a more complete picture of the causal pathways.",
    "crumbs": [
      "MR",
      "Papers",
      "Harnessing tissue-specific genetic variation to dissect putative causal pathways between body mass index and cardiometabolic phenotypes"
    ]
  },
  {
    "objectID": "mr/swanson_2017_28379526.html",
    "href": "mr/swanson_2017_28379526.html",
    "title": "The challenging interpretation of instrumental variable estimates under monotonicity",
    "section": "",
    "text": "PubMed: 28379526 DOI: 10.1093/ije/dyx038 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "MR",
      "Papers",
      "The challenging interpretation of instrumental variable estimates under monotonicity"
    ]
  },
  {
    "objectID": "mr/swanson_2017_28379526.html#background-the-local-average-treatment-effect-late",
    "href": "mr/swanson_2017_28379526.html#background-the-local-average-treatment-effect-late",
    "title": "The challenging interpretation of instrumental variable estimates under monotonicity",
    "section": "Background: The Local Average Treatment Effect (LATE)",
    "text": "Background: The Local Average Treatment Effect (LATE)\nInstrumental Variable (IV) methods are widely used in epidemiology and causal inference to estimate causal effects in the presence of unmeasured confounding. When combined with the necessary assumptions, including the monotonicity assumption (the instrument affects the treatment in only one direction), IV methods typically identify the Local Average Treatment Effect (LATE). The LATE represents the average causal effect of treatment only in the specific subgroup called ‘compliers’—individuals whose treatment status is changed by the instrument.",
    "crumbs": [
      "MR",
      "Papers",
      "The challenging interpretation of instrumental variable estimates under monotonicity"
    ]
  },
  {
    "objectID": "mr/swanson_2017_28379526.html#methods-non-causal-instruments",
    "href": "mr/swanson_2017_28379526.html#methods-non-causal-instruments",
    "title": "The challenging interpretation of instrumental variable estimates under monotonicity",
    "section": "Methods: Non-Causal Instruments",
    "text": "Methods: Non-Causal Instruments\nThis paper provides a detailed framework for interpreting LATE under monotonicity, paying particular attention to instruments that are non-causal for the exposure.\n\nNon-Causal Instrument Definition: A non-causal instrument is one that affects the outcome only through the exposure, but does not itself cause the exposure. Examples often include proxies for physician preference or certain genetic variants used in Mendelian Randomization (MR) studies.\nThe Problem: When the instrument is non-causal, the interpretation of the complier population—the subgroup to which the LATE applies—becomes highly complex. Little attention has historically been paid to the difficulty of interpreting the LATE when it is defined by adherence to an instrument that doesn’t actually cause the exposure.",
    "crumbs": [
      "MR",
      "Papers",
      "The challenging interpretation of instrumental variable estimates under monotonicity"
    ]
  },
  {
    "objectID": "mr/swanson_2017_28379526.html#results-challenges-to-interpretation",
    "href": "mr/swanson_2017_28379526.html#results-challenges-to-interpretation",
    "title": "The challenging interpretation of instrumental variable estimates under monotonicity",
    "section": "Results: Challenges to Interpretation",
    "text": "Results: Challenges to Interpretation\nThe paper clarifies the difficulties arising from the LATE’s reliance on the non-causal instrument:\n\nLimited Generalizability: Since the LATE applies only to the compliers, it represents a ‘local’ effect that may not be directly transferable to the entire population. This limits its utility for informing broad clinical or public health policy decisions.\nAmbiguous Subgroup: When the instrument is non-causal, the defining characteristics of the complier population are often unknown or difficult to characterize in a clinically meaningful way. This ambiguity makes the LATE difficult to translate into practical recommendations.\nRisk of Misleading Inference: Failure to fully account for the LATE’s limited applicability, especially when using a non-causal instrument, risks drawing misleading causal conclusions that overestimate the scope of the treatment effect. This is particularly relevant in MR studies where genetic variants are used as non-causal instruments for environmental or behavioral exposures.",
    "crumbs": [
      "MR",
      "Papers",
      "The challenging interpretation of instrumental variable estimates under monotonicity"
    ]
  },
  {
    "objectID": "mr/swanson_2017_28379526.html#conclusions-and-recommendations",
    "href": "mr/swanson_2017_28379526.html#conclusions-and-recommendations",
    "title": "The challenging interpretation of instrumental variable estimates under monotonicity",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe authors urge researchers using IV methods to exercise greater caution in the interpretation of their estimates, especially in observational settings where instruments are often non-causal. The primary recommendation is to explicitly define the complier population and clearly state the limited scope of the LATE estimate. The paper emphasizes the need for future research to develop methods that can identify causal effects that are more broadly relevant to clinical practice and policy.",
    "crumbs": [
      "MR",
      "Papers",
      "The challenging interpretation of instrumental variable estimates under monotonicity"
    ]
  },
  {
    "objectID": "mr/burgess_2011_21414999.html",
    "href": "mr/burgess_2011_21414999.html",
    "title": "Avoiding bias from weak instruments in Mendelian randomization studies",
    "section": "",
    "text": "PubMed: 21414999 DOI: 10.1093/ije/dyr036 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "MR",
      "Papers",
      "Avoiding bias from weak instruments in Mendelian randomization studies"
    ]
  },
  {
    "objectID": "mr/burgess_2011_21414999.html#key-finding-the-bias-from-weak-instruments",
    "href": "mr/burgess_2011_21414999.html#key-finding-the-bias-from-weak-instruments",
    "title": "Avoiding bias from weak instruments in Mendelian randomization studies",
    "section": "Key Finding: The Bias from Weak Instruments",
    "text": "Key Finding: The Bias from Weak Instruments\nThis paper provides crucial methodological guidance on weak instrument bias in Mendelian Randomization (MR) studies. It confirms that when the genetic instrumental variables (IVs) are only weakly associated with the exposure phenotype, the causal estimate derived from IV analysis is biased in the direction of the confounded, observational association between the phenotype and the outcome. The magnitude of this bias is directly dependent on the strength of the IVs, as measured by the F-statistic.",
    "crumbs": [
      "MR",
      "Papers",
      "Avoiding bias from weak instruments in Mendelian randomization studies"
    ]
  },
  {
    "objectID": "mr/burgess_2011_21414999.html#background-and-mechanism-of-bias",
    "href": "mr/burgess_2011_21414999.html#background-and-mechanism-of-bias",
    "title": "Avoiding bias from weak instruments in Mendelian randomization studies",
    "section": "Background and Mechanism of Bias",
    "text": "Background and Mechanism of Bias\nMendelian Randomization uses genetic variants to mimic the random allocation of a randomized controlled trial (RCT) to estimate the causal effect of an exposure (\\(X\\)) on an outcome (\\(Y\\)).\n\nThe Ideal: Valid IV analysis yields a causal estimate (\\(\\beta_{IV}\\)) that is independent of unmeasured confounding (\\(U\\)).\nThe Reality (Weak Instruments): When the instruments are weak (i.e., the F-statistic is low), the IV estimate no longer behaves like a purely causal estimate. Instead, the sampling distribution of the IV estimate is centered closer to the biased observational estimate (\\(\\beta_{OBS}\\)) rather than the true causal effect (\\(\\beta_{C}\\)). This happens because the genetic instrument’s association with the exposure is estimated with large uncertainty, making the IV estimate unstable and vulnerable to finite sample bias.",
    "crumbs": [
      "MR",
      "Papers",
      "Avoiding bias from weak instruments in Mendelian randomization studies"
    ]
  },
  {
    "objectID": "mr/burgess_2011_21414999.html#guidelines-for-minimizing-bias",
    "href": "mr/burgess_2011_21414999.html#guidelines-for-minimizing-bias",
    "title": "Avoiding bias from weak instruments in Mendelian randomization studies",
    "section": "Guidelines for Minimizing Bias",
    "text": "Guidelines for Minimizing Bias\nThe authors develop and advocate for guidelines aimed at the design and analysis stages of MR to minimize weak instrument bias:\n\n1. Instrument Selection\n\nF-statistic Threshold: Researchers should select genetic instruments with an F-statistic greater than 10 in the exposure sample to ensure adequate strength. The F-statistic is a measure of the instrument’s relevance and power.\nAvoidance of Small Studies: MR studies based on small sample sizes for the genetic-exposure association are highly susceptible to weak instrument bias and should generally be avoided unless a very strong, validated instrument is used.\n\n\n\n2. Analytical Strategies\n\nMeta-Analysis and Combining Effects: The authors discuss methods for combining data from multiple genetic variants and studies, such as the Inverse Variance Weighted (IVW) method and Bayesian meta-analysis, which can improve the overall strength and reduce bias when individual instruments are weak.\nFirst-Stage \\(R^2\\): In addition to the F-statistic, the proportion of variance in the exposure explained by the genetic instrument (\\(R^2\\)) should be reported, as this provides a complementary measure of instrument strength.",
    "crumbs": [
      "MR",
      "Papers",
      "Avoiding bias from weak instruments in Mendelian randomization studies"
    ]
  },
  {
    "objectID": "mr/burgess_2011_21414999.html#conclusion",
    "href": "mr/burgess_2011_21414999.html#conclusion",
    "title": "Avoiding bias from weak instruments in Mendelian randomization studies",
    "section": "Conclusion",
    "text": "Conclusion\nThe paper establishes the critical link between the strength of genetic instruments and the magnitude and direction of bias in MR. By providing a clear F-statistic threshold and promoting best practices for instrument selection and analysis, the study has been foundational in promoting rigorous quality control standards, ensuring that MR estimates remain as close as possible to the true causal effect and do not regress toward the confounded observational association.",
    "crumbs": [
      "MR",
      "Papers",
      "Avoiding bias from weak instruments in Mendelian randomization studies"
    ]
  },
  {
    "objectID": "mr/stender_2024_39244551.html",
    "href": "mr/stender_2024_39244551.html",
    "title": "Reclaiming mendelian randomization from the deluge of papers and misleading findings",
    "section": "",
    "text": "PubMed: 39244551 DOI: 10.1186/s12944-024-02284-w Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "MR",
      "Papers",
      "Reclaiming mendelian randomization from the deluge of papers and misleading findings"
    ]
  },
  {
    "objectID": "mr/stender_2024_39244551.html#key-findings-and-statement-of-concern",
    "href": "mr/stender_2024_39244551.html#key-findings-and-statement-of-concern",
    "title": "Reclaiming mendelian randomization from the deluge of papers and misleading findings",
    "section": "Key Findings and Statement of Concern",
    "text": "Key Findings and Statement of Concern\nThis comment piece addresses the burgeoning crisis of low-quality papers in Mendelian Randomization (MR) that is jeopardizing the method’s reputation as a robust causal inference tool. The authors identify two primary concerning trends driven by the availability of large genetic datasets:\n\nDeluge of Two-Sample MR (2SMR) Studies: An explosion of studies that simply apply standard 2SMR to publicly available Genome-Wide Association Study (GWAS) summary data, yielding results that often lack novelty or sufficient rigor.\nPremature Application of Complex Methods: The rapid development and use of novel, highly complex MR methods (e.g., non-linear MR, multi-variable MR) without adequate testing, leading to misleading or spurious findings, some of which have resulted in retractions or corrections (e.g., relating to vitamin D).",
    "crumbs": [
      "MR",
      "Papers",
      "Reclaiming mendelian randomization from the deluge of papers and misleading findings"
    ]
  },
  {
    "objectID": "mr/stender_2024_39244551.html#recommendations-for-journal-editors-and-peer-reviewers",
    "href": "mr/stender_2024_39244551.html#recommendations-for-journal-editors-and-peer-reviewers",
    "title": "Reclaiming mendelian randomization from the deluge of papers and misleading findings",
    "section": "Recommendations for Journal Editors and Peer Reviewers",
    "text": "Recommendations for Journal Editors and Peer Reviewers\nTo “reclaim” the methodology, the authors propose strict guidelines for editorial and peer review processes aimed at improving the quality and impact of published MR research.\n\nHandling Two-Sample MR (2SMR) Studies\nThe authors advise a heightened threshold for acceptance of standard 2SMR studies:\n\nRecommendation for Editors: Editors are advised to simply reject papers that only report standard 2SMR findings with no additional supporting evidence (e.g., individual-level data validation, unique data resources, or advanced methodological innovation).\nRecommendation for Reviewers: Reviewers should use a template for rejection that flags studies lacking sufficient rigor, such as those that fail to:\n\nConduct comprehensive sensitivity analyses (MR-Egger, Weighted Median, Steiger filtering).\nDemonstrate clear novelty or clinical/biological plausibility.\nAdequately harmonize data or check for sample overlap.\n\n\n\n\nHandling Novel and Complex MR Methods\nFor studies using new or highly complex MR methods, the review process must ensure robustness:\n\nRigorous Testing: Reviewers should demand evidence that the complex MR methods (e.g., non-linear MR) have been properly tested and validated by the authors, particularly for the specific exposure and outcome under investigation.\nBiological Plausibility: Findings must be scrutinized for biological plausibility. Results that challenge established biology must be exceptionally well-supported by evidence.\nComparison to Simpler Methods: Results from complex methods should be compared against robust, well-established methods (e.g., standard Inverse Variance Weighted MR) to ensure consistency, if appropriate.",
    "crumbs": [
      "MR",
      "Papers",
      "Reclaiming mendelian randomization from the deluge of papers and misleading findings"
    ]
  },
  {
    "objectID": "mr/stender_2024_39244551.html#conclusion",
    "href": "mr/stender_2024_39244551.html#conclusion",
    "title": "Reclaiming mendelian randomization from the deluge of papers and misleading findings",
    "section": "Conclusion",
    "text": "Conclusion\nThe article concludes that the current publishing landscape threatens to damage the credibility of MR. The burden is placed on the scientific community—especially editors and peer reviewers—to enforce stricter standards of rigor and novelty to ensure that MR continues to be a trustworthy and valuable tool for causal inference in epidemiology.",
    "crumbs": [
      "MR",
      "Papers",
      "Reclaiming mendelian randomization from the deluge of papers and misleading findings"
    ]
  },
  {
    "objectID": "mr/sanderson_2023_37634227.html",
    "href": "mr/sanderson_2023_37634227.html",
    "title": "Reappraising the role of instrumental inequalities for mendelian randomization studies in the mega Biobank era",
    "section": "",
    "text": "PubMed: 37634227 DOI: 10.1007/s10654-023-01035-y Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "MR",
      "Papers",
      "Reappraising the role of instrumental inequalities for mendelian randomization studies in the mega Biobank era"
    ]
  },
  {
    "objectID": "mr/sanderson_2023_37634227.html#key-findings-reappraisal-of-instrumental-inequalities-in-mr",
    "href": "mr/sanderson_2023_37634227.html#key-findings-reappraisal-of-instrumental-inequalities-in-mr",
    "title": "Reappraising the role of instrumental inequalities for mendelian randomization studies in the mega Biobank era",
    "section": "Key Findings: Reappraisal of Instrumental Inequalities in MR",
    "text": "Key Findings: Reappraisal of Instrumental Inequalities in MR\nThis commentary discusses the role and utility of instrumental inequalities (IIs)—a set of mathematical conditions that must hold true if the core Instrumental Variable (IV) assumptions of Mendelian Randomization (MR) are satisfied—especially in the context of the large datasets available in the “mega Biobank era.”\n\nInstrumental Inequalities as a Falsification Tool: Instrumental inequalities provide a valuable test of the validity of the instrumental variable (IV) assumptions. If the data violate the instrumental inequalities, it logically implies that at least one of the core IV assumptions is not met, and the instruments are therefore invalid.\nIncreased Utility in Large Biobanks: The authors argue that the use of instrumental inequalities has become more relevant and powerful in the era of mega-Biobanks (like the UK Biobank), due to the massive increase in sample size.\n\nStatistical Power: Large datasets provide the necessary statistical power to detect small violations of the inequalities, which may be missed in smaller studies. Detecting a violation strengthens the case for rejecting the IV assumptions.\n\nLimitations and Interpretation: The authors emphasize that while violating the inequalities proves the invalidity of the instruments, satisfying the inequalities does not prove validity (similar to how satisfying sensitivity analyses doesn’t guarantee the MR assumptions are met). However, IIs provide a unique way to flag potentially unreliable instruments.\nComplementary to Existing MR Methods: Instrumental inequalities should be viewed as a complementary tool to existing MR sensitivity analyses (like MR-Egger or MR-PRESSO). They offer an alternative perspective on bias detection, specifically testing the logical consequences of the IV assumptions themselves.",
    "crumbs": [
      "MR",
      "Papers",
      "Reappraising the role of instrumental inequalities for mendelian randomization studies in the mega Biobank era"
    ]
  },
  {
    "objectID": "mr/sanderson_2023_37634227.html#study-design-and-methods-commentary",
    "href": "mr/sanderson_2023_37634227.html#study-design-and-methods-commentary",
    "title": "Reappraising the role of instrumental inequalities for mendelian randomization studies in the mega Biobank era",
    "section": "Study Design and Methods (Commentary)",
    "text": "Study Design and Methods (Commentary)\nThis paper is a Commentary and does not report new empirical data but rather provides a methodological perspective on the role of existing statistical tools in modern genetic epidemiology.\n\nCore Concepts\n\nMR Assumptions: MR relies on three core Instrumental Variable (IV) assumptions :\n\nRelevance: The genetic instrument (\\(G\\)) is associated with the exposure (\\(X\\)).\nExclusion Restriction: The instrument affects the outcome (\\(Y\\)) only through the exposure (\\(X\\)).\nNo Confounding: The instrument (\\(G\\)) is independent of all confounders of the exposure-outcome relationship.\n\nInstrumental Inequalities: Instrumental inequalities are a set of mathematical constraints on the observable correlations (or effects) between \\(G\\), \\(X\\), and \\(Y\\) that must be satisfied if the three IV assumptions hold true. Violation of these constraints is a definitive way to demonstrate that the IV assumptions are false.\n\n\n\nRelevance in Mega-Biobanks\nThe commentary highlights that the ability to test IIs hinges on the power to accurately measure the associations between the instrument and the exposure, the instrument and the outcome, and the exposure and the outcome, which is significantly enhanced by the large sample sizes of modern Biobanks.",
    "crumbs": [
      "MR",
      "Papers",
      "Reappraising the role of instrumental inequalities for mendelian randomization studies in the mega Biobank era"
    ]
  },
  {
    "objectID": "mr/sanderson_2023_37634227.html#conclusions-and-recommendations",
    "href": "mr/sanderson_2023_37634227.html#conclusions-and-recommendations",
    "title": "Reappraising the role of instrumental inequalities for mendelian randomization studies in the mega Biobank era",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe authors recommend that researchers working with large datasets should reappraise and routinely incorporate the testing of instrumental inequalities into their Mendelian Randomization analyses.\n\nEnhanced Rigor: Testing instrumental inequalities adds a layer of statistical rigor by providing a formal test for falsifying the fundamental assumptions upon which MR is built.\nCautionary Note: The authors caution that since IIs rely on observed data, a failure to reject the inequalities is not proof of instrument validity, and therefore, they should not replace robust sensitivity analyses. They are best used as an initial filter or a final check on the proposed instruments.",
    "crumbs": [
      "MR",
      "Papers",
      "Reappraising the role of instrumental inequalities for mendelian randomization studies in the mega Biobank era"
    ]
  },
  {
    "objectID": "mr/merino_2022_34980908.html",
    "href": "mr/merino_2022_34980908.html",
    "title": "The unique challenges of studying the genetics of diet and nutrition",
    "section": "",
    "text": "PubMed: 34980908 DOI: 10.1038/s41591-021-01626-w Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "MR",
      "Papers",
      "The unique challenges of studying the genetics of diet and nutrition"
    ]
  },
  {
    "objectID": "mr/merino_2022_34980908.html#key-findings-the-challenge-of-nutritional-genomics",
    "href": "mr/merino_2022_34980908.html#key-findings-the-challenge-of-nutritional-genomics",
    "title": "The unique challenges of studying the genetics of diet and nutrition",
    "section": "Key Findings: The Challenge of Nutritional Genomics",
    "text": "Key Findings: The Challenge of Nutritional Genomics\nThis article discusses the specific methodological challenges faced when using genetic methods, particularly Mendelian Randomization (MR), to establish causal relationships between dietary and nutritional exposures and health outcomes. While MR is powerful for inferring causality in the presence of confounding, the inherent characteristics of diet and nutrition make applying MR uniquely problematic, risking violation of its core assumptions.",
    "crumbs": [
      "MR",
      "Papers",
      "The unique challenges of studying the genetics of diet and nutrition"
    ]
  },
  {
    "objectID": "mr/merino_2022_34980908.html#unique-challenges-of-dietary-exposures",
    "href": "mr/merino_2022_34980908.html#unique-challenges-of-dietary-exposures",
    "title": "The unique challenges of studying the genetics of diet and nutrition",
    "section": "Unique Challenges of Dietary Exposures",
    "text": "Unique Challenges of Dietary Exposures\nThe authors highlight several factors that make diet fundamentally different from other exposures typically studied using MR:\n\n1. Measurement and Quantifying Intake\nThere is a difficulty in accurately quantifying intake due to high measurement error and reliance on self-reporting. Furthermore, MR analyses often use genetic instruments based on single measures of diet (e.g., midlife assessment) and must assume this single measure is representative of long-term habitual intake, which may not be accurate for a time-varying exposure.\n\n\n2. Compositional and Time-Varying Nature\n\nCompositional Constraints: Diet is compositional; by definition, if an individual increases the intake of one macronutrient (e.g., protein), the intake of another (e.g., carbohydrate or fat) is necessarily altered. This complex intercorrelation makes it difficult to isolate the effect of a single nutrient.\nTime Variation: Dietary habits vary across time (life stages, seasons, etc.), making a single genetic instrument less likely to accurately capture lifetime exposure.\n\n\n\n3. Confounding and Assumption Violations\nThe most critical challenge is the risk of violating Instrumental Variable (IV) assumptions (Independence and Exclusion Restriction). Nutrition is notoriously correlated with numerous other lifestyle and environmental factors (e.g., socioeconomic status, education, exercise).\n\nThis extensive correlation suggests that a genetic variant associated with a dietary exposure might also affect the outcome through these related behaviors (pleiotropy), thereby biasing the MR estimate toward the confounded observational association.",
    "crumbs": [
      "MR",
      "Papers",
      "The unique challenges of studying the genetics of diet and nutrition"
    ]
  },
  {
    "objectID": "mr/merino_2022_34980908.html#conclusion",
    "href": "mr/merino_2022_34980908.html#conclusion",
    "title": "The unique challenges of studying the genetics of diet and nutrition",
    "section": "Conclusion",
    "text": "Conclusion\nThe authors underscore that the time-varying, compositional, and intercorrelated nature of diet and exercise makes instrumenting these behavioral exposures particularly problematic. To advance nutritional genomics, researchers need to develop and apply more sophisticated MR methods and leverage more detailed, repeated dietary assessment data to address these unique challenges and produce reliable causal inferences.",
    "crumbs": [
      "MR",
      "Papers",
      "The unique challenges of studying the genetics of diet and nutrition"
    ]
  },
  {
    "objectID": "mr/verkouter_2020_32500151.html",
    "href": "mr/verkouter_2020_32500151.html",
    "title": "The contribution of tissue-grouped BMI-associated gene sets to cardiometabolic-disease risk: a Mendelian randomization study",
    "section": "",
    "text": "PubMed: 32500151 DOI: 10.1093/ije/dyaa070 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "MR",
      "Papers",
      "The contribution of tissue-grouped BMI-associated gene sets to cardiometabolic-disease risk: a Mendelian randomization study"
    ]
  },
  {
    "objectID": "mr/verkouter_2020_32500151.html#key-findings",
    "href": "mr/verkouter_2020_32500151.html#key-findings",
    "title": "The contribution of tissue-grouped BMI-associated gene sets to cardiometabolic-disease risk: a Mendelian randomization study",
    "section": "Key Findings",
    "text": "Key Findings\nThe study aimed to test a biology-based approach by grouping BMI-associated genetic variants according to the tissue in which they are differentially expressed, expecting to find evidence for distinct pathways leading to cardiometabolic disease risk.\n\nNo Differential Risk based on Tissue Grouping: The central finding was that all identified tissue-grouped BMI-associated gene sets were similarly associated with increased risks of Type 2 Diabetes Mellitus (T2DM) and Coronary Artery Disease (CAD), despite being biologically distinct.\nSimilarity to Random Samples: The distribution of effect estimates (Odds Ratios, ORs) for T2DM and CAD derived from the tissue-grouped sets was similar to the distribution obtained from randomly sampled sets of BMI-associated genetic variants.\nLack of Additional Insight: The conclusion was that this novel, biology-based approach did not provide additional insight into the role of specific tissues in the genetic risk for specific cardiometabolic diseases (T2DM or CAD) due to overweight or obesity.\nOverall Risk Confirmed: Considering all BMI genetic instruments, an increase of 1 Standard Deviation (SD) in BMI was associated with an increased risk of T2DM (OR 2.71, 95% CI 2.49; 2.94) and CAD (OR 1.48, 95% CI 1.37; 1.59).",
    "crumbs": [
      "MR",
      "Papers",
      "The contribution of tissue-grouped BMI-associated gene sets to cardiometabolic-disease risk: a Mendelian randomization study"
    ]
  },
  {
    "objectID": "mr/verkouter_2020_32500151.html#study-design-and-methods",
    "href": "mr/verkouter_2020_32500151.html#study-design-and-methods",
    "title": "The contribution of tissue-grouped BMI-associated gene sets to cardiometabolic-disease risk: a Mendelian randomization study",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\n\nStudy Design\nThis was a two-sample Mendelian randomization (MR) study that used publicly available summary-level data from large-scale Genome-Wide Association Studies (GWAS) to assess causal relationships.\n\n\nSelection and Grouping of Genetic Instruments\n\nSelection of Exposure Variants: The exposure was Body Mass Index (BMI). Genetic variants were selected from the GWAS meta-analysis by Yengo et al. (2018), which initially identified 656 independent single nucleotide polymorphisms (SNPs) associated with BMI. After filtering and gene annotation, 634 mapped BMI-associated genes were used.\nTissue-Grouped Gene Set Identification: Overrepresentation tests were performed using Genotype-Tissue Expression (GTEx) version 8 data to identify genes that were differentially expressed (significantly up- or downregulated) in a given tissue compared with other tissues. This resulted in the identification of 17 partly overlapping tissue-grouped gene sets. Twelve of these sets were derived from brain areas (e.g., hypothalamus, amygdala), while others included the artery tibial, kidney cortex, spleen, and digestive system tissues.\nMR Instrumental Variables: The individual SNPs corresponding to the genes within these 17 tissue-grouped sets were used as instrumental variables for the MR analyses. The genetic instrument mapped to TCF7L2 (rs7903146) was removed from all gene sets due to its known pleiotropic effect.\n\n\n\nOutcome Data\n\nT2DM: Summary statistics from the DIAbetes Genetics Replication and Meta-analysis (DIAGRAM) consortium (74,124 cases and 824,006 controls).\nCAD: Summary statistics from the CARDIOGRAMplusC4D consortium (60,801 cases and 123,504 controls).\nAnthropometric Traits: Waist circumference and total body fat were also used as outcomes to explore underlying mechanisms.\n\n\n\nStatistical Analysis\n\nPrimary MR Method: Inverse-Variance-Weighted (IVW) analyses were used to combine the effects of individual genetic instruments. The results were reported as the change in Odds Ratio (OR) per 1 SD increase in BMI (which corresponds to 4.8 kg/m\\(^2\\)).\nSensitivity Analyses: MR-Egger regression and Weighted Median-Estimator (WME) analyses were performed to test for and account for potential pleiotropy.",
    "crumbs": [
      "MR",
      "Papers",
      "The contribution of tissue-grouped BMI-associated gene sets to cardiometabolic-disease risk: a Mendelian randomization study"
    ]
  },
  {
    "objectID": "mr/verkouter_2020_32500151.html#conclusions-and-recommendations",
    "href": "mr/verkouter_2020_32500151.html#conclusions-and-recommendations",
    "title": "The contribution of tissue-grouped BMI-associated gene sets to cardiometabolic-disease risk: a Mendelian randomization study",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe investigation demonstrated that grouping BMI-associated genes based on tissue-specific expression (a biological mechanism) did not yield groups of variants with distinct causal effects on T2DM or CAD risk.\n\nImplication for Etiological Studies: The findings suggest that the genetic predisposition to cardiometabolic disease driven by general BMI is largely independent of the tissue-specific expression profiles examined, or that the current methodology for tissue-grouping is insufficient to detect fine-scale differences in downstream risk pathways.\nLimitation: A potential limitation noted by the authors is that the GTEx overrepresentation test does not account for the effect size of the genetic instruments on the exposure (BMI), meaning large and small effect variants are weighted equally in the clustering, which may have masked subtle differences. The substantial overlap between genes within the tissue-grouped gene sets was also cited as a limitation that may reduce specificity.",
    "crumbs": [
      "MR",
      "Papers",
      "The contribution of tissue-grouped BMI-associated gene sets to cardiometabolic-disease risk: a Mendelian randomization study"
    ]
  },
  {
    "objectID": "mr/labrecque_2018_30239571.html",
    "href": "mr/labrecque_2018_30239571.html",
    "title": "Interpretation and Potential Biases of Mendelian Randomization Estimates With Time-Varying Exposures",
    "section": "",
    "text": "PubMed: 30239571 DOI: 10.1093/aje/kwy204 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "MR",
      "Papers",
      "Interpretation and Potential Biases of Mendelian Randomization Estimates With Time-Varying Exposures"
    ]
  },
  {
    "objectID": "mr/labrecque_2018_30239571.html#key-findings-mr-and-time-varying-exposures",
    "href": "mr/labrecque_2018_30239571.html#key-findings-mr-and-time-varying-exposures",
    "title": "Interpretation and Potential Biases of Mendelian Randomization Estimates With Time-Varying Exposures",
    "section": "Key Findings: MR and Time-Varying Exposures",
    "text": "Key Findings: MR and Time-Varying Exposures\nThis paper explores the interpretation and potential biases of Mendelian Randomization (MR) estimates when the exposure of interest is time-varying (changes over an individual’s life course). The often-cited advantage of MR is that it estimates a “lifetime effect,” but this term is frequently vague. The authors clarify that standard MR estimates typically reflect the effect of a time-fixed intervention—specifically, an intervention on the long-term average of the exposure—rather than the effect of an intervention at a specific time point. This interpretation is often inconsistent with the target of clinical or public health interventions.",
    "crumbs": [
      "MR",
      "Papers",
      "Interpretation and Potential Biases of Mendelian Randomization Estimates With Time-Varying Exposures"
    ]
  },
  {
    "objectID": "mr/labrecque_2018_30239571.html#methods-and-study-design",
    "href": "mr/labrecque_2018_30239571.html#methods-and-study-design",
    "title": "Interpretation and Potential Biases of Mendelian Randomization Estimates With Time-Varying Exposures",
    "section": "Methods and Study Design",
    "text": "Methods and Study Design\nThe study used an empirical example with data from the Rotterdam Study to illustrate the problem. They applied standard MR to examine the effect of smoking (a classic time-varying exposure) on blood pressure.\n\nStandard IV/MR Model: The traditional Instrumental Variable (IV) and MR framework is designed for estimating the effect of a point or time-fixed exposure, not a dynamic, time-varying exposure.\nInterpretation of Standard MR: When applied to a time-varying exposure (like smoking duration or intensity), the resulting MR estimate is interpreted as the causal effect of an intervention that fixes the exposure status to be constantly high (or low) from birth. This is equivalent to estimating the effect of an intervention on the cumulative or long-term average of the exposure.",
    "crumbs": [
      "MR",
      "Papers",
      "Interpretation and Potential Biases of Mendelian Randomization Estimates With Time-Varying Exposures"
    ]
  },
  {
    "objectID": "mr/labrecque_2018_30239571.html#potential-biases",
    "href": "mr/labrecque_2018_30239571.html#potential-biases",
    "title": "Interpretation and Potential Biases of Mendelian Randomization Estimates With Time-Varying Exposures",
    "section": "Potential Biases",
    "text": "Potential Biases\nThe authors identify two key sources of bias when standard MR methods are applied to time-varying exposures without proper consideration:\n\n1. Misalignment with Target Causal Effect\nThe primary issue is the misalignment between what standard MR estimates and what is clinically relevant. * Clinical Target: Policymakers and clinicians are usually interested in the causal effect of intervening at a specific time (e.g., quitting smoking at age 40) or intervening on the intensity (e.g., reducing alcohol intake by one drink per day). * MR Estimate: Standard MR estimates the effect of a lifetime difference in exposure average, which may not be the relevant policy parameter, potentially leading to misleading conclusions about the impact of a feasible intervention.\n\n\n2. Time-Varying Confounding\nIf there are time-varying confounders that are themselves affected by prior exposure, standard MR estimates can be biased. For example, a genetic variant associated with smoking might also be associated with factors that influence both later-life smoking and the outcome (blood pressure). The standard MR assumptions do not adequately account for this complex, time-dependent structure.",
    "crumbs": [
      "MR",
      "Papers",
      "Interpretation and Potential Biases of Mendelian Randomization Estimates With Time-Varying Exposures"
    ]
  },
  {
    "objectID": "mr/labrecque_2018_30239571.html#conclusions-and-recommendations",
    "href": "mr/labrecque_2018_30239571.html#conclusions-and-recommendations",
    "title": "Interpretation and Potential Biases of Mendelian Randomization Estimates With Time-Varying Exposures",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe paper concludes that while MR remains a powerful tool, researchers must be more explicit and cautious when applying it to time-varying exposures.\n\nExplicit Interpretation: Researchers should clearly state that standard MR estimates for time-varying exposures reflect the causal effect of an intervention on the lifetime average or cumulative exposure, not the effect of an intervention at a specific point in time.\nCausal Models: Future methodological work should focus on leveraging the causal inference tools developed for time-varying treatments (e.g., g-methods) to create MR-based methods that can estimate more policy-relevant parameters, such as the effect of initiating or stopping an exposure at a specific age.",
    "crumbs": [
      "MR",
      "Papers",
      "Interpretation and Potential Biases of Mendelian Randomization Estimates With Time-Varying Exposures"
    ]
  },
  {
    "objectID": "mr/ray_2025_40555237.html",
    "href": "mr/ray_2025_40555237.html",
    "title": "Single-cell transcriptome-wide Mendelian randomization and colocalization analyses uncover cell-specific mechanisms in atherosclerotic cardiovascular disease",
    "section": "",
    "text": "PubMed: 40555237 DOI: 10.1016/j.ajhg.2025.06.001 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "MR",
      "Papers",
      "Single-cell transcriptome-wide Mendelian randomization and colocalization analyses uncover cell-specific mechanisms in atherosclerotic cardiovascular disease"
    ]
  },
  {
    "objectID": "mr/ray_2025_40555237.html#key-findings-cell-specific-causal-genes-in-atherosclerosis",
    "href": "mr/ray_2025_40555237.html#key-findings-cell-specific-causal-genes-in-atherosclerosis",
    "title": "Single-cell transcriptome-wide Mendelian randomization and colocalization analyses uncover cell-specific mechanisms in atherosclerotic cardiovascular disease",
    "section": "Key Findings: Cell-Specific Causal Genes in Atherosclerosis",
    "text": "Key Findings: Cell-Specific Causal Genes in Atherosclerosis\nThis study introduced a novel, stringent single-cell transcriptome-wide Mendelian randomization (scTWMR) framework to leverage cell-type-specific gene expression data and identify causal genes for Atherosclerotic Cardiovascular Disease (ASCVD) risk, a level of resolution previously inaccessible with bulk-tissue MR.\n\nCell-Specific Mechanisms Uncovered: The scTWMR approach successfully resolved the genetic signals influencing ASCVD risk to specific cell types within the atherosclerotic plaque, leading to the prioritization of 23 causal genes.\nPrioritization of Immune Cell Genes: The strongest and most frequent causal signals were found in immune cells, particularly macrophages and T cells, which are central to the inflammatory process of atherosclerosis:\n\nMacrophages: Genetically predicted increased expression of HSH2D in macrophages was causally associated with an increased risk of ASCVD.\nT Cells: Genetically predicted increased expression of PRRC2A and ARHGAP26 in T cells was causally associated with increased ASCVD risk.\n\nVascular Smooth Muscle Cell (VSMC) Genes: Genes with causal effects were also identified in VSMCs, highlighting their role in plaque stability:\n\nSCARB1 and CD33 expression in VSMCs were causally associated with ASCVD risk.\n\nImproved Gene Prioritization: The scTWMR approach significantly improved the precision of causal gene prioritization compared to traditional bulk-tissue MR. Many signals that appeared ambiguous or lacked a confirmed causal gene in bulk tissue analysis were successfully linked to a specific cell type and gene using the single-cell resolution framework.",
    "crumbs": [
      "MR",
      "Papers",
      "Single-cell transcriptome-wide Mendelian randomization and colocalization analyses uncover cell-specific mechanisms in atherosclerotic cardiovascular disease"
    ]
  },
  {
    "objectID": "mr/ray_2025_40555237.html#study-design-and-methods",
    "href": "mr/ray_2025_40555237.html#study-design-and-methods",
    "title": "Single-cell transcriptome-wide Mendelian randomization and colocalization analyses uncover cell-specific mechanisms in atherosclerotic cardiovascular disease",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\n\nStudy Design\nThis was a two-sample single-cell transcriptome-wide Mendelian randomization (scTWMR) study. The framework integrated publicly available GWAS summary statistics for ASCVD with cell-type-specific gene expression data derived from single-cell RNA sequencing (scRNA-seq) of human atherosclerotic plaques.\n\n\nData Sources and Instrumental Variables\n\nOutcome GWAS Data (ASCVD): Summary statistics for Atherosclerotic Cardiovascular Disease (ASCVD), Coronary Artery Disease (CAD), and Stroke were sourced from large-scale consortia (e.g., CARDIOGRAMplusC4D, ISGC).\nExposure Data (Cell-Specific Expression): Expression quantitative trait loci (eQTLs) were derived from an analysis of human atherosclerotic plaque scRNA-seq data, providing a high-resolution map of genetic effects on gene expression within specific cell types, including macrophages, T cells, endothelial cells, and smooth muscle cells.\nInstrument Selection: Genetic variants (SNPs) acting as cell-type-specific eQTLs were used as instrumental variables (IVs). This ensured that the instruments reflected the genetic regulation of gene expression specifically in the cell type of interest.\n\n\n\nStatistical Analysis\n\nMendelian Randomization (MR): The Inverse-Variance Weighted (IVW) method was used as the primary MR approach to estimate the causal effect (Odds Ratio, OR) of genetically predicted cell-type-specific gene expression on ASCVD risk.\nSensitivity Analyses: Robust sensitivity methods were performed, including:\n\nMR-Egger regression.\nWeighted Median and Weighted Mode estimators.\nMR-PRESSO (to detect and remove pleiotropic outliers).\n\nColocalization Analysis: Bayesian colocalization (moloc) was used to provide statistical evidence that the same underlying genetic variant was responsible for both the change in cell-specific gene expression and the change in ASCVD risk, distinguishing true causal links from spurious associations due to linkage disequilibrium.\nNovelty of Framework: The study designed its framework to be stringent and scalable, providing a powerful resource for future studies aimed at dissecting complex disease mechanisms at cellular resolution.",
    "crumbs": [
      "MR",
      "Papers",
      "Single-cell transcriptome-wide Mendelian randomization and colocalization analyses uncover cell-specific mechanisms in atherosclerotic cardiovascular disease"
    ]
  },
  {
    "objectID": "mr/ray_2025_40555237.html#conclusions-and-recommendations",
    "href": "mr/ray_2025_40555237.html#conclusions-and-recommendations",
    "title": "Single-cell transcriptome-wide Mendelian randomization and colocalization analyses uncover cell-specific mechanisms in atherosclerotic cardiovascular disease",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study concludes that integrating single-cell transcriptomic data with MR provides a necessary leap forward in genetic epidemiology, enabling the dissection of complex disease risk at a cellular level.\n\nHigh-Resolution Drug Targets: By resolving causal genes to specific cell types (e.g., HSH2D in macrophages), the scTWMR framework directly identifies highly specific molecular targets for therapeutic development. This could lead to the design of drugs that modulate ASCVD risk by specifically affecting the function of key immune cells in the plaque, potentially minimizing off-target effects.\nMechanistic Understanding: The prioritization of genes in immune cells reinforces the central role of inflammation in ASCVD aetiology and provides a refined list of genes for functional validation studies.\nFuture Work: The authors advocate for the widespread application of this scTWMR framework to other complex diseases, such as neurological disorders and cancer, where cell-type heterogeneity is known to play a crucial role.",
    "crumbs": [
      "MR",
      "Papers",
      "Single-cell transcriptome-wide Mendelian randomization and colocalization analyses uncover cell-specific mechanisms in atherosclerotic cardiovascular disease"
    ]
  },
  {
    "objectID": "mr/burgess_2024_38362310.html",
    "href": "mr/burgess_2024_38362310.html",
    "title": "Incorporating biological and clinical insights into variant choice for Mendelian randomisation: examples and principles",
    "section": "",
    "text": "PubMed: 38362310 DOI: 10.1136/egastro-2023-100042 Overview generated by: Gemini 2.5 Flash, 28-11-2025",
    "crumbs": [
      "MR",
      "Papers",
      "Incorporating biological and clinical insights into variant choice for Mendelian randomisation: examples and principles"
    ]
  },
  {
    "objectID": "mr/burgess_2024_38362310.html#key-findings-prioritizing-biology-for-instrument-validity",
    "href": "mr/burgess_2024_38362310.html#key-findings-prioritizing-biology-for-instrument-validity",
    "title": "Incorporating biological and clinical insights into variant choice for Mendelian randomisation: examples and principles",
    "section": "Key Findings: Prioritizing Biology for Instrument Validity",
    "text": "Key Findings: Prioritizing Biology for Instrument Validity\nThis review emphasizes that the validity of Mendelian Randomisation (MR) analyses is fundamentally determined by the choice of genetic variants used as instrumental variables (IVs). The most critical risk is horizontal pleiotropy, where a genetic variant influences the outcome through a pathway separate from the exposure of interest.\n\nThe central argument is that a biologically motivated strategy for variant selection is generally preferred over a comprehensive genome-wide approach, as it increases the plausibility that the core IV assumptions are met.\nGenome-wide analyses, while potentially offering greater power, often introduce invalid instruments and should be viewed as complementary evidence, particularly when a clear biological signal is lacking or when assessing the robustness of findings.",
    "crumbs": [
      "MR",
      "Papers",
      "Incorporating biological and clinical insights into variant choice for Mendelian randomisation: examples and principles"
    ]
  },
  {
    "objectID": "mr/burgess_2024_38362310.html#study-design-and-instrumental-variable-assumptions",
    "href": "mr/burgess_2024_38362310.html#study-design-and-instrumental-variable-assumptions",
    "title": "Incorporating biological and clinical insights into variant choice for Mendelian randomisation: examples and principles",
    "section": "Study Design and Instrumental Variable Assumptions",
    "text": "Study Design and Instrumental Variable Assumptions\nThis paper is a Review that provides a principled discussion of IV selection in MR, an epidemiological method used to infer causal relationships using genetic variants. The validity of MR relies on three core IV assumptions:\n\nRelevance: The genetic variant must be strongly associated with the exposure.\nIndependence (No Confounding): The genetic variant must not share common causes with the outcome.\nExclusion Restriction (No Pleiotropy): The genetic variant must not affect the outcome except through the exposure.\n\nThe paper focuses heavily on strategies to satisfy the third, most vulnerable, assumption, stating that the plausibility of meeting the assumptions is greatest when the functional relevance of the genetic variants to the exposure is clearly understood.",
    "crumbs": [
      "MR",
      "Papers",
      "Incorporating biological and clinical insights into variant choice for Mendelian randomisation: examples and principles"
    ]
  },
  {
    "objectID": "mr/burgess_2024_38362310.html#methods-for-biologically-motivated-selection",
    "href": "mr/burgess_2024_38362310.html#methods-for-biologically-motivated-selection",
    "title": "Incorporating biological and clinical insights into variant choice for Mendelian randomisation: examples and principles",
    "section": "Methods for Biologically Motivated Selection",
    "text": "Methods for Biologically Motivated Selection\nThe authors detail practical ways to implement a biologically informed strategy, ensuring the genetic instrument acts primarily through the exposure:\n\nCis-Mendelian Randomisation (Cis-MR): Restricting variants to the coding gene region of the exposure itself (e.g., a specific protein). This is considered the most reliable approach due to the clear, short-distance functional link.\nRegulatory Variants: Using variants in a gene region that encodes a key regulator of the exposure levels, rather than the exposure itself, but still with a clear biological link.\nBiomarker Selection: Choosing instruments based on their association with a circulating biomarker (e.g., plasma caffeine levels) rather than a broader behavioral phenotype (e.g., self-reported coffee consumption), to isolate the biological effect of the circulating factor.\nConsistency Assessment: When using multiple variants, evaluating the consistency of the effects of those variants on the outcome is crucial, as inconsistent effects (some raising risk, some lowering) may point to mechanism-specific effects or horizontal pleiotropy.",
    "crumbs": [
      "MR",
      "Papers",
      "Incorporating biological and clinical insights into variant choice for Mendelian randomisation: examples and principles"
    ]
  },
  {
    "objectID": "mr/burgess_2024_38362310.html#validation-and-sensitivity-analyses",
    "href": "mr/burgess_2024_38362310.html#validation-and-sensitivity-analyses",
    "title": "Incorporating biological and clinical insights into variant choice for Mendelian randomisation: examples and principles",
    "section": "Validation and Sensitivity Analyses",
    "text": "Validation and Sensitivity Analyses\nBiological and clinical insights should inform sensitivity checks to validate the instruments:\n\nPositive Controls: Testing the selected IVs on an outcome where a causal link with the exposure is already well-established. This confirms the IVs are appropriately capturing the exposure’s effect.\nNegative Controls: Testing for associations in populations or with outcomes where a causal effect is biologically or clinically not expected. Finding a null association in these controls strengthens confidence in the primary analysis.\nMultivariable MR (MVMR): This method should be used to adjust for genetically predicted levels of related traits, helping to distinguish the direct effect of the exposure from the indirect effects of correlated risk factors (i.e., known pleiotropy).",
    "crumbs": [
      "MR",
      "Papers",
      "Incorporating biological and clinical insights into variant choice for Mendelian randomisation: examples and principles"
    ]
  },
  {
    "objectID": "mr/burgess_2024_38362310.html#conclusions-and-recommendations",
    "href": "mr/burgess_2024_38362310.html#conclusions-and-recommendations",
    "title": "Incorporating biological and clinical insights into variant choice for Mendelian randomisation: examples and principles",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe authors conclude that an optimal MR investigation requires combining statistical methods with critical biological and clinical thinking. Recommendations for researchers include:\n\nPrioritizing the use of biologically plausible instruments.\nUsing sensitivity analyses (MVMR, positive/negative controls) to rigorously test the IV assumptions.\nReporting estimates derived from all plausible instrument selection strategies (e.g., both biologically motivated and genome-wide) to demonstrate the robustness and consistency of the causal estimate.",
    "crumbs": [
      "MR",
      "Papers",
      "Incorporating biological and clinical insights into variant choice for Mendelian randomisation: examples and principles"
    ]
  },
  {
    "objectID": "mr/burgess_2023_32760811.html",
    "href": "mr/burgess_2023_32760811.html",
    "title": "Guidelines for performing Mendelian randomization investigations: update for summer 2023",
    "section": "",
    "text": "PubMed: 32760811 DOI: 10.12688/wellcomeopenres.15555.3 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "MR",
      "Papers",
      "Guidelines for performing Mendelian randomization investigations: update for summer 2023"
    ]
  },
  {
    "objectID": "mr/burgess_2023_32760811.html#key-findings-and-purpose",
    "href": "mr/burgess_2023_32760811.html#key-findings-and-purpose",
    "title": "Guidelines for performing Mendelian randomization investigations: update for summer 2023",
    "section": "Key Findings and Purpose",
    "text": "Key Findings and Purpose\nThis article provides an updated set of guidelines and best practices for conducting and reporting Mendelian Randomization (MR) studies. As MR methodology rapidly advances, this iteration addresses new statistical methods, common methodological challenges, and the need for standardized reporting. The core purpose is to help researchers rigorously apply MR to infer causality between an exposure and an outcome using genetic instruments.",
    "crumbs": [
      "MR",
      "Papers",
      "Guidelines for performing Mendelian randomization investigations: update for summer 2023"
    ]
  },
  {
    "objectID": "mr/burgess_2023_32760811.html#the-core-assumptions-and-validity-checks",
    "href": "mr/burgess_2023_32760811.html#the-core-assumptions-and-validity-checks",
    "title": "Guidelines for performing Mendelian randomization investigations: update for summer 2023",
    "section": "The Core Assumptions and Validity Checks",
    "text": "The Core Assumptions and Validity Checks\nMR relies on three key Instrumental Variable (IV) assumptions. The guidelines emphasize that MR studies must systematically test and address potential violations of these assumptions, which are often the main sources of bias:\n\nRelevance: The genetic instrument must be associated with the exposure. (Checked via F-statistic, strong instruments are required).\nIndependence (No Confounding): The genetic instrument must not be associated with confounders of the exposure-outcome relationship. (Addressed by excluding associations with measured confounders and assuming sufficient randomization by meiosis).\nExclusion Restriction (No Pleiotropy): The genetic instrument must affect the outcome only through the exposure. (Violated by pleiotropy, where a single variant affects multiple traits independently).\n\n\nFalsification and Sensitivity Analyses\nThe guidelines strongly recommend using multiple sensitivity analyses to test the untestable Exclusion Restriction assumption, including:\n\nMR-Egger: Used to detect and adjust for directional pleiotropy.\nWeighted Median and Weighted Mode: Provide consistent causal estimates even if up to 50% of the information comes from invalid variants.\nCochran’s Q Statistic: Tests for heterogeneity, suggesting potential pleiotropy or violation of the “no confounding” assumption.\nSteiger Filtering: Confirms that the genetic instrument influences the intended exposure more strongly than the outcome, testing the causal direction.",
    "crumbs": [
      "MR",
      "Papers",
      "Guidelines for performing Mendelian randomization investigations: update for summer 2023"
    ]
  },
  {
    "objectID": "mr/burgess_2023_32760811.html#two-sample-mr-and-data-requirements",
    "href": "mr/burgess_2023_32760811.html#two-sample-mr-and-data-requirements",
    "title": "Guidelines for performing Mendelian randomization investigations: update for summer 2023",
    "section": "Two-Sample MR and Data Requirements",
    "text": "Two-Sample MR and Data Requirements\nThe guidelines detail best practices for Two-Sample MR (2SMR), the most common form of MR, which uses genetic summary statistics from two separate (but ancestrally matched) populations: one for the instrument-exposure association and one for the instrument-outcome association.\n\nData Selection: Genetic variants must be carefully selected to be robustly associated with the exposure (typically \\(P &lt; 5 \\times 10^{-8}\\)) and independent (clumped).\nHarmonization: Data harmonization (aligning the effect allele and its effect size across the two datasets) is a critical step to ensure variant estimates are comparable.\nSample Overlap: The guidelines discuss the risk of bias due to sample overlap (when the two samples share individuals) and recommend specific correction methods (e.g., using robust standard errors) when overlap is unavoidable.",
    "crumbs": [
      "MR",
      "Papers",
      "Guidelines for performing Mendelian randomization investigations: update for summer 2023"
    ]
  },
  {
    "objectID": "mr/burgess_2023_32760811.html#reporting-and-transparency",
    "href": "mr/burgess_2023_32760811.html#reporting-and-transparency",
    "title": "Guidelines for performing Mendelian randomization investigations: update for summer 2023",
    "section": "Reporting and Transparency",
    "text": "Reporting and Transparency\nThe paper stresses the importance of clear, comprehensive reporting to ensure reproducibility and critical appraisal:\n\nClarity on Assumptions: Researchers must explicitly state the assumptions being tested and which sensitivity analyses were used to address them.\nReporting Results: All sensitivity analysis results should be reported, not just the primary MR result. Visual tools, such as scatter plots, funnel plots, and forest plots, should be included to aid interpretation and illustrate heterogeneity/pleiotropy.",
    "crumbs": [
      "MR",
      "Papers",
      "Guidelines for performing Mendelian randomization investigations: update for summer 2023"
    ]
  },
  {
    "objectID": "mr/burgess_2023_32760811.html#conclusion-and-future-directions",
    "href": "mr/burgess_2023_32760811.html#conclusion-and-future-directions",
    "title": "Guidelines for performing Mendelian randomization investigations: update for summer 2023",
    "section": "Conclusion and Future Directions",
    "text": "Conclusion and Future Directions\nThe updated guidelines reflect the growing sophistication of MR, emphasizing the move from simple, single-method MR to a multi-method approach where a range of sensitivity analyses are mandatory to demonstrate the robustness of causal findings. Future research is encouraged to develop methods for dealing with non-linear relationships, time-varying exposures, and the use of multi-omic data in MR frameworks.",
    "crumbs": [
      "MR",
      "Papers",
      "Guidelines for performing Mendelian randomization investigations: update for summer 2023"
    ]
  },
  {
    "objectID": "other/index.html",
    "href": "other/index.html",
    "title": "other",
    "section": "",
    "text": "No matching items",
    "crumbs": [
      "other",
      "Papers"
    ]
  },
  {
    "objectID": "pgs/ma_2021_34243982.html",
    "href": "pgs/ma_2021_34243982.html",
    "title": "Genetic prediction of complex traits with polygenic scores: A statistical review",
    "section": "",
    "text": "PubMed: 34243982\nDOI: 10.1016/j.tig.2021.06.004\nOverview generated by: Gemini 2.5 Flash, 26/11/2025",
    "crumbs": [
      "PGS",
      "Papers",
      "Genetic prediction of complex traits with polygenic scores: A statistical review"
    ]
  },
  {
    "objectID": "pgs/ma_2021_34243982.html#key-findings",
    "href": "pgs/ma_2021_34243982.html#key-findings",
    "title": "Genetic prediction of complex traits with polygenic scores: A statistical review",
    "section": "Key Findings",
    "text": "Key Findings\nThis comprehensive statistical review provides an exhaustive analysis of the landscape of Polygenic Score (PGS) methods, a critical area in human genetics focused on predicting complex traits and disease risk using genetic data. The authors review 46 different methods for PGS construction, establishing a unifying multiple linear regression framework to connect and categorize the majority of these techniques. The core conclusion is that the optimal PGS method is highly dependent on the genetic architecture of the target trait (e.g., polygenicity, effect size distribution) and the nature of the available training and target data. The review serves as an essential reference, providing both the statistical underpinnings for method developers and practical guidance (including a decision tree) for analysts performing PGS analysis in clinical and research settings.",
    "crumbs": [
      "PGS",
      "Papers",
      "Genetic prediction of complex traits with polygenic scores: A statistical review"
    ]
  },
  {
    "objectID": "pgs/ma_2021_34243982.html#categorization-and-statistical-framework",
    "href": "pgs/ma_2021_34243982.html#categorization-and-statistical-framework",
    "title": "Genetic prediction of complex traits with polygenic scores: A statistical review",
    "section": "Categorization and Statistical Framework",
    "text": "Categorization and Statistical Framework\nThe review structures the diverse landscape of PGS methods into a coherent statistical framework, primarily centered on how they estimate the SNP effect sizes, which serve as the weights for the Polygenic Score.\n\nCore PGS Calculation\nIn its simplest form, the PGS for an individual (\\(i\\)) is a weighted sum of their genotypes across \\(M\\) single nucleotide polymorphisms (SNPs): \\[\\text{PGS}_i = \\sum_{j=1}^{M} G_{ij} \\hat{\\beta}_j\\] where \\(G_{ij}\\) is the genotype of individual \\(i\\) at SNP \\(j\\), and \\(\\hat{\\beta}_j\\) is the estimated genetic effect size (the weight) for that SNP.\n\n\nUnifying Multiple Linear Regression Framework\nThe authors classify most modern PGS methods as variants of a general multiple linear regression model: \\[\\mathbf{Y} = \\mathbf{G}\\mathbf{\\beta} + \\mathbf{\\epsilon}\\] where \\(\\mathbf{Y}\\) is the phenotype vector, \\(\\mathbf{G}\\) is the genotype matrix, \\(\\mathbf{\\beta}\\) is the vector of true genetic effects, and \\(\\mathbf{\\epsilon}\\) is the error term. Different PGS methods (e.g., LDpred, S-Bayes) are distinguished by their assumptions about the effect size vector, \\(\\mathbf{\\beta}\\), and how they account for the correlation structure of the genotypes (Linkage Disequilibrium or LD).\n\n\nClassification of PGS Methods\nThe review categorizes the 46 analyzed methods based on their underlying statistical approach:\n\nClassical Methods: These include P-value Thresholding and Clumping (P+T), which selects a subset of independent SNPs based on their p-values. This remains a simple and surprisingly effective baseline method.\nBayesian and Regularization Methods: These methods explicitly model the genetic architecture by incorporating prior distributions on the SNP effect sizes (\\(\\mathbf{\\beta}\\)) and/or regularizing the estimates to account for noise and LD. Examples include:\n\nLDpred/LDpred2: Assumes a fraction of SNPs are causal and uses an LD reference panel to shrink effect sizes.\nBayesC/BayesR: Assumes effect sizes come from a mixture of normal distributions, allowing for a few large effects and many small ones.\nLasso/Ridge Regression: Uses penalization to optimize effect size selection and estimation.\n\nSummary Statistics-Based Methods: Methods that operate entirely on GWAS summary statistics and an external LD reference panel (e.g., LDpred, S-Bayes, PRS-CS). These are the most computationally efficient and widely used due to data sharing conventions.\nMeta-Dimensional Methods: These include methods that incorporate information beyond SNP effects, such as functional annotations or gene expression data (e.g., MetaXcan).",
    "crumbs": [
      "PGS",
      "Papers",
      "Genetic prediction of complex traits with polygenic scores: A statistical review"
    ]
  },
  {
    "objectID": "pgs/ma_2021_34243982.html#practical-considerations-and-performance",
    "href": "pgs/ma_2021_34243982.html#practical-considerations-and-performance",
    "title": "Genetic prediction of complex traits with polygenic scores: A statistical review",
    "section": "Practical Considerations and Performance",
    "text": "Practical Considerations and Performance\n\nFactors Affecting PGS Performance\nThe prediction accuracy of a PGS (typically measured by the coefficient of determination, \\(R^2\\)) is highly sensitive to several factors:\n\nGenetic Architecture: Methods that accurately model the true genetic architecture (e.g., high polygenicity vs. oligenicity) perform best. Bayesian methods often excel because they flexibly model the prior distribution of effect sizes.\nTraining Sample Size: Performance is highly dependent on the size of the GWAS used to estimate \\(\\hat{\\beta}\\). Larger sample sizes generally lead to more accurate effect size estimates and thus better PGS performance.\nAncestral Divergence: Prediction accuracy significantly drops when the target population ancestry differs substantially from the training population (e.g., GWAS conducted primarily in European populations but applied to African populations). This highlights the critical issue of transferability and health equity.\nLinkage Disequilibrium (LD) Modeling: Explicitly accounting for LD, such as in LDpred and PRS-CS, is crucial for improving prediction accuracy compared to simple methods like P+T.\n\n\n\nDecision Tree for PGS Analysis\nThe review provides a practical decision tree to help researchers select the appropriate PGS method based on the available data and research question:\nKey branches include: * Input Data: Is individual-level data or only GWAS summary data available? * Trait Complexity: Analyzing a single trait or multiple correlated traits? * Modeling Approach: Choosing between model-based (e.g., linear mixed models) and algorithm-based (e.g., machine learning) methods.",
    "crumbs": [
      "PGS",
      "Papers",
      "Genetic prediction of complex traits with polygenic scores: A statistical review"
    ]
  },
  {
    "objectID": "pgs/ma_2021_34243982.html#challenges-and-future-directions",
    "href": "pgs/ma_2021_34243982.html#challenges-and-future-directions",
    "title": "Genetic prediction of complex traits with polygenic scores: A statistical review",
    "section": "Challenges and Future Directions",
    "text": "Challenges and Future Directions\nThe authors identify several major challenges that need to be addressed to realize the full clinical potential of PGS:\n\nAddressing Heterogeneity (Ancestry): Developing methods that maintain high predictive accuracy across diverse ancestral populations and that can properly perform multi-ancestry meta-analysis to generate universally accurate PGS.\nNon-Additive and GxE Effects: Incorporating non-additive genetic effects (dominance, epistasis) and Gene-by-Environment (GxE) interactions into the PGS model. Current models are largely additive and thus leave substantial variance unexplained.\nCausal Variants and Fine-Mapping: Moving beyond marker SNPs to accurately identify and weight the true causal variants to improve biological relevance and prediction.\nClinical Implementation: Establishing standardized reporting guidelines, validating PGS in independent clinical cohorts, and addressing the ethical concerns related to using PGS in personalized medicine.",
    "crumbs": [
      "PGS",
      "Papers",
      "Genetic prediction of complex traits with polygenic scores: A statistical review"
    ]
  },
  {
    "objectID": "pgs/mars_2022_35591975.html",
    "href": "pgs/mars_2022_35591975.html",
    "title": "Genome-wide risk prediction of common diseases across ancestries in one million people",
    "section": "",
    "text": "PubMed: 35591975\nDOI: 10.1016/j.xgen.2022.100118\nOverview generated by: Gemini 2.5 Flash, 26/11/2025",
    "crumbs": [
      "PGS",
      "Papers",
      "Genome-wide risk prediction of common diseases across ancestries in one million people"
    ]
  },
  {
    "objectID": "pgs/mars_2022_35591975.html#key-findings",
    "href": "pgs/mars_2022_35591975.html#key-findings",
    "title": "Genome-wide risk prediction of common diseases across ancestries in one million people",
    "section": "Key Findings",
    "text": "Key Findings\nThis study performed a large-scale, cross-ancestry evaluation of Polygenic Risk Scores (PRSs) for four major common diseases—Coronary Artery Disease (CAD), Type 2 Diabetes (T2D), Breast Cancer, and Prostate Cancer—using genome-wide genotype data from six biobanks across Europe, the United States, and Asia, encompassing over one million individuals. The principal finding is a striking disparity in PRS transferability and accuracy: the predictive ability of PRSs remains robust and highly similar across various European populations and local population substructures, suggesting utility in clinical settings for this group. However, the PRSs exhibited significantly poorer transferability and substantially lower effect sizes in individuals of African ancestry, and to a lesser extent, in South Asian and East Asian ancestries. This large-scale empirical evidence underscores the immediate challenge of ancestral bias in genomic data and highlights the potential for the clinical implementation of current PRSs to exacerbate existing health disparities.",
    "crumbs": [
      "PGS",
      "Papers",
      "Genome-wide risk prediction of common diseases across ancestries in one million people"
    ]
  },
  {
    "objectID": "pgs/mars_2022_35591975.html#study-design-and-data",
    "href": "pgs/mars_2022_35591975.html#study-design-and-data",
    "title": "Genome-wide risk prediction of common diseases across ancestries in one million people",
    "section": "Study Design and Data",
    "text": "Study Design and Data\nThe study utilized a combined dataset of approximately one million individuals across six major biobanks: BioBank Japan, Estonian Biobank, FinnGen, HUNT, Mass General Brigham (MGB) Biobank, and UK Biobank. The ancestries evaluated included European, South Asian, East Asian, and African.\n\nPRS Calculation and Evaluation\n\nPRS Method: Genome-wide PRSs were calculated using LDpred, a method that accounts for linkage disequilibrium (LD) and uses a Bayesian approach to estimate SNP effect sizes, incorporating over 6 million variants for each disease.\nInput Data: The input weights were obtained from the largest publicly available, non-overlapping Genome-Wide Association Studies (GWASs) for each of the four diseases.\nTransferability Assessment: Transferability was assessed by comparing the Odds Ratios (OR) per standard deviation (SD) increase in PRS across different global ancestry groups, and also within European populations (including a population isolate, Finland).",
    "crumbs": [
      "PGS",
      "Papers",
      "Genome-wide risk prediction of common diseases across ancestries in one million people"
    ]
  },
  {
    "objectID": "pgs/mars_2022_35591975.html#key-results-on-transferability",
    "href": "pgs/mars_2022_35591975.html#key-results-on-transferability",
    "title": "Genome-wide risk prediction of common diseases across ancestries in one million people",
    "section": "Key Results on Transferability",
    "text": "Key Results on Transferability\n\nGlobal Ancestry Disparities\nA clear gradient of PRS accuracy was observed, directly correlated with the genetic distance from the primary European GWAS training cohorts:\n\nEuropean Ancestry: The PRSs showed consistently high and similar effect sizes (ORs) across various European populations and health-care systems, suggesting good utility for risk stratification.\nAsian Ancestry: Individuals of South Asian and East Asian ancestry exhibited similar or slightly lower effect sizes compared to Europeans.\nAfrican Ancestry: Individuals of African ancestry consistently had the lowest effect sizes and poorest prediction accuracy for all four diseases. For instance, in breast cancer, the association was not statistically significant in women of African ancestry in some cohorts.\n\n\n\nSubstructure and Polygenicity\n\nWithin-European Transferability: The PRSs transferred well even between highly structured European populations, such as various regional substructures within Finland, demonstrating robustness across recent population bottlenecks.\nGenome-wide vs. Sparse PRS: A crucial methodological finding was that the highly polygenic, genome-wide PRSs (using millions of variants) displayed higher effect sizes and better transferability across global ancestries than PRSs containing only a smaller, more stringently selected set of variants (sparse PRSs). This supports the notion that the polygenic nature of these traits is captured across different populations, even if the fine-mapping of causal variants differs.",
    "crumbs": [
      "PGS",
      "Papers",
      "Genome-wide risk prediction of common diseases across ancestries in one million people"
    ]
  },
  {
    "objectID": "pgs/mars_2022_35591975.html#implications-for-clinical-utility",
    "href": "pgs/mars_2022_35591975.html#implications-for-clinical-utility",
    "title": "Genome-wide risk prediction of common diseases across ancestries in one million people",
    "section": "Implications for Clinical Utility",
    "text": "Implications for Clinical Utility\nThe findings provide strong evidence that the current state of PRS technology is not ready for equitable clinical deployment:\n\nClinical Utility: Current PRSs have demonstrated significant potential for clinical screening and prevention in individuals of European ancestry.\nHealth Equity Concern: The low predictive accuracy in individuals of African ancestry, South Asian, and East Asian ancestry—stemming from the lack of diversity in the original GWAS training data—poses a significant challenge to global health equity and personalized medicine. The study stresses the urgent necessity of investing in and executing large-scale GWAS in non-European populations to address this bias.",
    "crumbs": [
      "PGS",
      "Papers",
      "Genome-wide risk prediction of common diseases across ancestries in one million people"
    ]
  },
  {
    "objectID": "pgs/ghatan_2024_38200577.html",
    "href": "pgs/ghatan_2024_38200577.html",
    "title": "Defining type 2 diabetes polygenic risk scores through colocalization and network-based clustering of metabolic trait genetic associations",
    "section": "",
    "text": "PubMed: 38200577 DOI: 10.1186/s13073-023-01255-7 Overview generated by: Gemini 2.5 Flash, 26/11/2025",
    "crumbs": [
      "PGS",
      "Papers",
      "Defining type 2 diabetes polygenic risk scores through colocalization and network-based clustering of metabolic trait genetic associations"
    ]
  },
  {
    "objectID": "pgs/ghatan_2024_38200577.html#key-findings-dissecting-t2d-heterogeneity-via-pleiotropy",
    "href": "pgs/ghatan_2024_38200577.html#key-findings-dissecting-t2d-heterogeneity-via-pleiotropy",
    "title": "Defining type 2 diabetes polygenic risk scores through colocalization and network-based clustering of metabolic trait genetic associations",
    "section": "Key Findings: Dissecting T2D Heterogeneity via Pleiotropy",
    "text": "Key Findings: Dissecting T2D Heterogeneity via Pleiotropy\nThis study addresses the profound genetic and clinical heterogeneity of Type 2 Diabetes (T2D) by developing a refined framework to partition T2D-associated genetic variants into distinct biological pathways. The goal is to move beyond a single, monolithic T2D diagnosis toward stratified risk prediction and targeted therapeutic strategies. The method leverages the pleiotropic nature of genetic variants—their influence on multiple related traits—to define distinct mechanisms of T2D pathogenesis.\n\nColocalization-First Partitioning and Clustering\nThe authors improved upon previous clustering approaches by integrating rigorous statistical checks for shared causality, enhancing the mechanistic interpretability of the resulting risk scores.\n\nColocalization Analysis: They applied colocalization analysis between T2D and 20 related metabolic traits (selected based on established risk factors and genetic correlation) across 243 T2D loci. This step robustly identified 146 T2D loci where the T2D association was likely caused by the same causal variant as the associated metabolic trait.\nNetwork-Based Clustering: A network-based unsupervised hierarchical clustering approach was then performed using the colocalized variant-trait associations. This successfully grouped the T2D risk loci into five distinct clusters, each representing a unique, interconnected set of T2D and metabolic risk factors.\nCausality Check (Mendelian Randomization): The study explicitly assessed the causality and directionality of the variant-trait associations using the Mendelian randomization (MR) Steiger’s Z-test. This confirmed that the genetic associations identified in the clusters were largely causal for the corresponding metabolic phenotypes.\n\n\n\nFive Distinct T2D Pathophysiological Clusters\nThe five identified genetic clusters, which align with distinct T2D pathophysiologies, are: 1. Obesity (High BMI-T2D risk) 2. Lipodystrophic insulin resistance (T2D risk associated with fat distribution/dysfunction) 3. Liver and lipid metabolism 4. Hepatic glucose metabolism 5. Beta-cell dysfunction (Impaired insulin secretion)\n\n\nHeterogeneous Clinical Profiles and “Lean Diabetes”\nPartitioned Polygenic Risk Scores (PRSs) were generated for each cluster and externally validated in 21,742 individuals with T2D across three independent cohorts, demonstrating unique associations with metabolic and clinical outcomes:\n\nOpposite BMI Associations: The Obesity PRS was strongly associated with a higher BMI, as expected. Critically, the Lipodystrophic insulin resistance PRS and Beta-cell dysfunction PRS were both associated with lower BMI.\nSupport for Lean Diabetes: The MR Steiger analysis provided causal evidence that increased T2D risk in the lipodystrophic insulin resistance and beta-cell dysfunction pathways was causally associated with lower BMI. This provides a genetic foundation for the “lean diabetes” or non-obese T2D sub-type, where risk is driven by dysfunctional fat/insulin-secretion rather than overall fat mass.\nComorbidity Stratification: The Lipodystrophic insulin resistance PRS was uniquely and specifically associated with a higher odds of chronic kidney disease (CKD) (Odds Ratio 1.29), suggesting that individuals whose T2D risk is driven by this pathway may require specific monitoring and intervention for renal complications.\n\n\n\nConclusion\nThe colocalization-first, network-based clustering methodology successfully and robustly partitioned the genetic heterogeneity underlying T2D. The resulting pathway-specific PRSs provide valuable tools for risk stratification, sub-type identification, and the development of targeted therapies based on an individual’s distinct genetic mechanism of disease.",
    "crumbs": [
      "PGS",
      "Papers",
      "Defining type 2 diabetes polygenic risk scores through colocalization and network-based clustering of metabolic trait genetic associations"
    ]
  },
  {
    "objectID": "finemap-colocalisation/patel_2024_240212171.html",
    "href": "finemap-colocalisation/patel_2024_240212171.html",
    "title": "A frequentist test of proportional colocalization after selecting relevant genetic variants",
    "section": "",
    "text": "arXiv: 2402.12171 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "finemap-colocalisation",
      "Papers",
      "A frequentist test of proportional colocalization after selecting relevant genetic variants"
    ]
  },
  {
    "objectID": "finemap-colocalisation/patel_2024_240212171.html#key-findings-proportional-colocalization-test",
    "href": "finemap-colocalisation/patel_2024_240212171.html#key-findings-proportional-colocalization-test",
    "title": "A frequentist test of proportional colocalization after selecting relevant genetic variants",
    "section": "Key Findings: Proportional Colocalization Test",
    "text": "Key Findings: Proportional Colocalization Test\nThis paper proposes a novel frequentist test of proportional colocalization, named prop-coloc-cond, designed to assess whether two traits are affected by the same causal genetic variants in a specific genomic region. This method serves as a crucial complement to the widely-used Bayesian colocalization tests.\n\nNovel Frequentist Approach: The test is based on the concept of proportional colocalization, which posits that if two traits share the same causal variant(s), the vector of genetic associations for the first trait should be proportional to the vector of associations for the second trait in that region.\nAddressing Selection Uncertainty: A key methodological contribution is that the prop-coloc-cond test explicitly accounts for the uncertainty introduced by selecting the relevant genetic variants (e.g., fine-mapping or selecting the top SNPs) that are included in the test. By conditioning on the selection, the test achieves accurate Type I error control, ensuring reliable results in real-world applications.\nComplementary to Bayesian Methods: The prop-coloc-cond approach uses assumptions markedly different from those of Bayesian colocalization methods (like coloc). This offers a valuable alternative for obtaining complementary evidence, especially when Bayesian results are ambiguous, sensitive to prior distributions, or when the underlying assumptions of the Bayesian test are questionable.\nSimulations and Empirical Validation: Simulation studies demonstrated that the new method correctly controls the Type I error rate. The empirical investigation into the GLP1R gene expression locus confirmed the utility of the test in providing robust evidence of colocalization.\nSimplicity of Implementation: The test requires only summary data on genetic associations for the two traits, making it straightforward to implement using existing Genome-Wide Association Study (GWAS) results.",
    "crumbs": [
      "finemap-colocalisation",
      "Papers",
      "A frequentist test of proportional colocalization after selecting relevant genetic variants"
    ]
  },
  {
    "objectID": "finemap-colocalisation/patel_2024_240212171.html#study-design-and-methods",
    "href": "finemap-colocalisation/patel_2024_240212171.html#study-design-and-methods",
    "title": "A frequentist test of proportional colocalization after selecting relevant genetic variants",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\n\nMethodology\nThe study focuses on developing a new frequentist statistical test for colocalization, which contrasts with established Bayesian methods.\n\nProportional Colocalization Model: The core assumption is that the genetic association vector for trait 1 (\\(\\hat{\\beta}_1\\)) is proportional to the genetic association vector for trait 2 (\\(\\hat{\\beta}_2\\)), plus noise (\\(\\epsilon\\)): \\[\\hat{\\beta}_2 = \\alpha \\hat{\\beta}_1 + \\epsilon\\] where \\(\\alpha\\) is the proportionality constant, and the null hypothesis for colocalization is defined as \\(\\alpha \\neq 0\\).\nVariant Selection: The authors first perform a step to select the relevant genetic variants in the region (e.g., using a conditional association approach or fine-mapping) to focus the analysis on the most likely causal SNPs.\nConditional Test (prop-coloc-cond): The innovation lies in deriving a test statistic (a modified F-statistic or Wald test) that is conditioned on the selection procedure. This conditioning step is crucial because performing a standard proportional test after selecting variants based on the data violates statistical assumptions and leads to inflated Type I error rates. The conditional test corrects for this selection bias.\nComparison: The performance of the prop-coloc-cond test was compared to a non-conditional proportional colocalization test and Bayesian colocalization methods using simulations under various genetic architectures (e.g., single causal variant, multiple causal variants).\n\n\n\nData Application\n\nEmpirical Example: The method was applied to investigate the colocalization between gene expression of the GLP1R gene (Glucagon-like peptide 1 receptor) and Type 2 Diabetes (T2D) risk, demonstrating its ability to provide clear statistical evidence in a locus highly relevant to drug development.",
    "crumbs": [
      "finemap-colocalisation",
      "Papers",
      "A frequentist test of proportional colocalization after selecting relevant genetic variants"
    ]
  },
  {
    "objectID": "finemap-colocalisation/patel_2024_240212171.html#conclusions-and-recommendations",
    "href": "finemap-colocalisation/patel_2024_240212171.html#conclusions-and-recommendations",
    "title": "A frequentist test of proportional colocalization after selecting relevant genetic variants",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe paper concludes that the prop-coloc-cond test provides a robust and valuable tool for colocalization analysis, enhancing the ability of researchers to distinguish between shared and distinct genetic etiology of traits.\n\nEnhancing Causal Inference: The test is recommended for use in conjunction with Bayesian methods to provide a more comprehensive and statistically rigorous assessment of colocalization, particularly in critical applications like drug target prioritization where mechanistic certainty is paramount.\nFuture Directions: The authors suggest that the principle of conditional testing used in this work can be extended to develop frequentist approaches for other complex genetic association analyses, such as assessing heterogeneity or pleiotropy, where variant selection is a prerequisite.",
    "crumbs": [
      "finemap-colocalisation",
      "Papers",
      "A frequentist test of proportional colocalization after selecting relevant genetic variants"
    ]
  },
  {
    "objectID": "cancer/qiu_2024_39690287.html",
    "href": "cancer/qiu_2024_39690287.html",
    "title": "Deep profiling of gene expression across 18 human cancers",
    "section": "",
    "text": "PubMed: 39690287 DOI: 10.1038/s41551-024-01290-8 Overview generated by: Gemini 2.5 Flash, 10/12/2025",
    "crumbs": [
      "cancer",
      "Papers",
      "Deep profiling of gene expression across 18 human cancers"
    ]
  },
  {
    "objectID": "cancer/qiu_2024_39690287.html#study-goal-and-deepprofile-framework",
    "href": "cancer/qiu_2024_39690287.html#study-goal-and-deepprofile-framework",
    "title": "Deep profiling of gene expression across 18 human cancers",
    "section": "Study Goal and DeepProfile Framework",
    "text": "Study Goal and DeepProfile Framework\nThis paper introduces DeepProfile, an unsupervised deep-learning framework designed to generate low-dimensional, robust, and biologically interpretable latent spaces from gene-expression data across multiple human cancers. The framework addresses key challenges in applying deep learning to cancer transcriptomics, namely the risk of overfitting, the non-deterministic nature of model training, and the “black box” problem of biological interpretability.\n\nMethods: Deep Learner and Interpreter Components\nThe DeepProfile framework is built upon three main components: Data Collector, Deep Learner, and Interpreter.\n\nData Collector and Cohort: The analysis utilized a large cohort of 50,211 transcriptomes from 1,098 datasets across 18 human cancer types, primarily sourced from the Gene Expression Omnibus (GEO).\nDeep Learner (Ensemble VAE): The core model is an ensemble of Variational Autoencoders (VAEs). The VAE encodes high-dimensional gene expression signals into a low-dimensional latent space, typically composed of 150 latent variables. The ensemble approach integrates signals from hundreds of different VAE runs and latent dimension sizes to enhance model robustness and stability.\nInterpreter (Integrated Gradients): To ensure biological interpretability, DeepProfile incorporates the Integrated Gradients feature attribution method. This tool quantifies how much each input gene contributes to a specific latent variable, allowing for subsequent pathway enrichment tests to define pathway-level attributions.\n\n\n\nKey Findings and Performance\nDeepProfile demonstrated superior performance in capturing biological signals compared to alternative dimensionality-reduction methods, including PCA, ICA, and standard VAEs.\n\nInterpretability Advantage: DeepProfile’s latent variables captured a significantly higher average number of known KEGG, BioCarta, and Reactome pathways, as well as Oncogenic Signatures gene sets, across the 18 cancers compared to other methods (e.g., DeepProfile captured pathways in 106 out of 108 test cases).",
    "crumbs": [
      "cancer",
      "Papers",
      "Deep profiling of gene expression across 18 human cancers"
    ]
  },
  {
    "objectID": "cancer/qiu_2024_39690287.html#pan-cancer-commonality-and-specificity",
    "href": "cancer/qiu_2024_39690287.html#pan-cancer-commonality-and-specificity",
    "title": "Deep profiling of gene expression across 18 human cancers",
    "section": "Pan-Cancer Commonality and Specificity",
    "text": "Pan-Cancer Commonality and Specificity\nAnalysis of the learned latent spaces revealed distinct patterns of gene expression variation:\n\nUniversal Patterns: Genes that are universally important in defining the latent spaces across all 18 cancer types primarily control immune cell activation and aspects of the inflammatory response.\nCancer-Specific Patterns: Cancer-type-specific genes and pathways contribute to the latent spaces for only one particular cancer type, serving to define molecular disease subtypes and reflect tissue-specific biology.",
    "crumbs": [
      "cancer",
      "Papers",
      "Deep profiling of gene expression across 18 human cancers"
    ]
  },
  {
    "objectID": "cancer/qiu_2024_39690287.html#clinical-and-mutational-associations",
    "href": "cancer/qiu_2024_39690287.html#clinical-and-mutational-associations",
    "title": "Deep profiling of gene expression across 18 human cancers",
    "section": "Clinical and Mutational Associations",
    "text": "Clinical and Mutational Associations\nA key methodology was developed to link the DeepProfile latent variable embeddings to patient- and tumour-level clinical characteristics, specifically patient survival and tumour mutation burden (TMB).\n\nTumour Mutation Burden (TMB): TMB was found to be closely associated with the expression of cell-cycle-related pathways across a large majority of cancers.\nPatient Survival: Survival consistently correlated with the activity of the DNA-mismatch repair pathway and the MHC class II antigen presentation pathway.\nCellular Origin of MHC II: The study further identified that tumour-associated macrophages (TAMs) are a source of the survival-correlated MHC class II transcripts.\n\n\nConclusions\nDeepProfile successfully leveraged unsupervised deep learning and an ensemble approach to create robust and highly interpretable gene-expression embeddings for pan-cancer analysis. This methodology facilitated the discovery of shared (immune-related) and specific (subtype-defining) biological patterns across diverse cancers. Furthermore, the ability to link latent variables to clinical phenotypes allowed for the identification of pathways (e.g., DNA-mismatch repair, MHC class II presentation) associated with patient survival and TMB, demonstrating the utility of deep unsupervised learning in generating novel biological insights from large cancer datasets.",
    "crumbs": [
      "cancer",
      "Papers",
      "Deep profiling of gene expression across 18 human cancers"
    ]
  },
  {
    "objectID": "cancer/albrecht_2025_40555635.html",
    "href": "cancer/albrecht_2025_40555635.html",
    "title": "Gastric cancer: from biomarkers to functional precision medicine",
    "section": "",
    "text": "PubMed: 40555635 DOI: 10.1016/j.molmed.2025.05.007 Overview generated by: Gemini 2.5 Flash, 10/12/2025",
    "crumbs": [
      "cancer",
      "Papers",
      "Gastric cancer: from biomarkers to functional precision medicine"
    ]
  },
  {
    "objectID": "cancer/albrecht_2025_40555635.html#overview-and-context",
    "href": "cancer/albrecht_2025_40555635.html#overview-and-context",
    "title": "Gastric cancer: from biomarkers to functional precision medicine",
    "section": "Overview and Context",
    "text": "Overview and Context\nThis review provides an update on the evolving treatment strategies for advanced Gastric Cancer (GC), focusing specifically on immune and targeted therapies, and discusses how the integration of novel molecular and functional approaches can advance personalized medicine. GC remains a deadly disease, and treatment options for advanced stages are limited, prompting a rapid evolution in the therapeutic landscape.",
    "crumbs": [
      "cancer",
      "Papers",
      "Gastric cancer: from biomarkers to functional precision medicine"
    ]
  },
  {
    "objectID": "cancer/albrecht_2025_40555635.html#molecular-subtypes-and-biomarkers",
    "href": "cancer/albrecht_2025_40555635.html#molecular-subtypes-and-biomarkers",
    "title": "Gastric cancer: from biomarkers to functional precision medicine",
    "section": "Molecular Subtypes and Biomarkers",
    "text": "Molecular Subtypes and Biomarkers\nThe molecular classification of GC is essential for biomarker-directed therapy selection. The Cancer Genome Atlas (TCGA) established four primary molecular subtypes:\n\nChromosomal Instability (CIN): Comprising nearly half of GC cases (49.8%), this subtype is marked by frequent TP53 mutations and large-scale genomic rearrangements, including amplifications of Receptor Tyrosine Kinases (RTKs), which are potential actionable targets.\nMicrosatellite Instability (MSI): Accounting for 21.7% of cases, this subtype is hypermutated due to defects in mismatch repair genes, resulting in higher immunogenicity and an enhanced response to immune checkpoint blockade.\nGenomically Stable (GS): This subtype (19.7%) has a low mutational burden, harbors few driver mutations, is often associated with diffuse GC, and typically responds poorly to chemo- and immunotherapy.\nEpstein-Barr Virus (EBV): The smallest subtype (8.8%), characterized by high frequency of PIK3CA alterations.",
    "crumbs": [
      "cancer",
      "Papers",
      "Gastric cancer: from biomarkers to functional precision medicine"
    ]
  },
  {
    "objectID": "cancer/albrecht_2025_40555635.html#current-approved-treatment-landscape",
    "href": "cancer/albrecht_2025_40555635.html#current-approved-treatment-landscape",
    "title": "Gastric cancer: from biomarkers to functional precision medicine",
    "section": "Current Approved Treatment Landscape",
    "text": "Current Approved Treatment Landscape\nThe standard treatment backbone for metastatic GC is a platinum-fluoropyrimidine doublet chemotherapy (e.g., FOLFOX or CapOx), which is complemented by targeted antibodies based on biomarker expression:\n\nHER2-Positive Disease: Trastuzumab combined with chemotherapy is approved for first-line treatment. The anti-PD-1 inhibitor Pembrolizumab is also approved in combination with trastuzumab and chemotherapy for HER2-positive GC with PD-L1 Combined Positivity Score (CPS) $$1. The Antibody-Drug Conjugate (ADC) Trastuzumab Deruxtecan (T-DXd) is approved for second-line therapy in trastuzumab-refractory cases.\nImmune Checkpoint Blockade: PD-1 inhibitors (Pembrolizumab or Nivolumab) are approved in combination with chemotherapy for PD-L1-expressing tumors (CPS $$1 or $$5, depending on the drug and HER2 status).\nCLDN18.2-Positive Disease: Zolbetuximab, an anti-CLDN18.2 antibody, has recently been approved for first-line treatment in HER2-negative cases, based on Phase 3 trials demonstrating significant progression-free survival (PFS) prolongation.\nSecond-Line (HER2-Negative): The anti-VEGFR2 antibody Ramucirumab is standard of care, often combined with paclitaxel.",
    "crumbs": [
      "cancer",
      "Papers",
      "Gastric cancer: from biomarkers to functional precision medicine"
    ]
  },
  {
    "objectID": "cancer/albrecht_2025_40555635.html#emerging-targets-and-future-directions",
    "href": "cancer/albrecht_2025_40555635.html#emerging-targets-and-future-directions",
    "title": "Gastric cancer: from biomarkers to functional precision medicine",
    "section": "Emerging Targets and Future Directions",
    "text": "Emerging Targets and Future Directions\nThe field is rapidly advancing with several promising new drug classes and targets under investigation:\n\nAntibody-Drug Conjugates (ADCs): The success of T-DXd has accelerated the development of ADCs targeting other antigens, including CLDN18.2, DKK1, TROP2, and CEACAM5, offering new avenues for targeted drug delivery.\nFGFR2 Inhibition: The antibody Bemarituzumab, which targets the FGFR2b isoform, showed a compelling increase in median overall survival (24.7 months vs 11.1 months for placebo) in the FGFR2b-positive subgroup of the Phase 2 FIGHT trial, with Phase 3 trials currently ongoing.\nImmunotherapy Combinations: Bispecific antibodies targeting PD-1 and other immunomodulatory proteins (like CTLA-4 or TIGIT) are in clinical trials as a potential next step to enhance antitumor immune responses beyond single checkpoint blockade.\nDNA Damage Response/Cell Cycle: Although Phase 3 trials for PARP inhibitors (Olaparib) failed to meet their primary endpoint for overall survival, inhibitors of DNA damage response (DDR) kinases (ATM/ATR, CHK1/2, WEE1, CDK2) are being explored, particularly for CIN tumors.",
    "crumbs": [
      "cancer",
      "Papers",
      "Gastric cancer: from biomarkers to functional precision medicine"
    ]
  },
  {
    "objectID": "cancer/albrecht_2025_40555635.html#functional-precision-medicine",
    "href": "cancer/albrecht_2025_40555635.html#functional-precision-medicine",
    "title": "Gastric cancer: from biomarkers to functional precision medicine",
    "section": "Functional Precision Medicine",
    "text": "Functional Precision Medicine\nThe ultimate goal is to move toward personalized oncology. This requires the integration of high-throughput molecular diagnostics and functional precision medicine. Functional drug testing on patient-derived models (like organoids or xenografts) is emerging as a critical tool to complement multi-omics analyses and predict therapy response, which is necessary to optimize treatment selection and improve patient survival.",
    "crumbs": [
      "cancer",
      "Papers",
      "Gastric cancer: from biomarkers to functional precision medicine"
    ]
  },
  {
    "objectID": "ml/index.html",
    "href": "ml/index.html",
    "title": "machine learning/clustering/selection",
    "section": "",
    "text": "Bayesian kernel machine regression for estimating the health effects of multi-pollutant mixtures\n\n\n\nObjective: This paper introduced Bayesian Kernel Machine Regression (BKMR), a novel statistical and machine learning method designed to estimate the complex, non-linear, and interactive health effects of multi-pollutant mixtures (e.g., air pollutants or chemicals).\nKey Strengths: BKMR overcomes major limitations of traditional models by effectively handling:\n\nNon-linearity in the exposure-response relationship.\nComplex interactions between pollutants.\nHigh collinearity among the mixture components.\n\nKey Output: The Bayesian framework provides flexible estimates of the overall mixture effect and the relative importance of individual pollutants through Posterior Inclusion Probabilities (PIPs), which helps identify the main drivers of the health outcome within the mixture.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "ml",
      "Papers"
    ]
  },
  {
    "objectID": "genetics/hoerbst_2025_biorxiv.html",
    "href": "genetics/hoerbst_2025_biorxiv.html",
    "title": "What is a Differentially Expressed Gene?",
    "section": "",
    "text": "PubMed: Not Indexed (bioRxiv) DOI: 10.1101/2025.01.31.635902 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "genetics",
      "Papers",
      "What is a Differentially Expressed Gene?"
    ]
  },
  {
    "objectID": "genetics/hoerbst_2025_biorxiv.html#core-problem-reproducibility-in-differential-gene-expression",
    "href": "genetics/hoerbst_2025_biorxiv.html#core-problem-reproducibility-in-differential-gene-expression",
    "title": "What is a Differentially Expressed Gene?",
    "section": "Core Problem: Reproducibility in Differential Gene Expression",
    "text": "Core Problem: Reproducibility in Differential Gene Expression\nThe concept of ‘Differentially Expressed Genes’ (DEGs) is fundamental to RNA-Seq studies, yet their identification is plagued by reproducibility issues. This is primarily attributed to the inherent biological and technical variation in the data, which is poorly captured when using small numbers of replicates. When rigid thresholds for p-values and \\(\\log_{2}\\) fold changes are applied, this variability can lead to inconsistent results and incomplete data description.",
    "crumbs": [
      "genetics",
      "Papers",
      "What is a Differentially Expressed Gene?"
    ]
  },
  {
    "objectID": "genetics/hoerbst_2025_biorxiv.html#methods-comparing-binary-vs.-rank-based-classification",
    "href": "genetics/hoerbst_2025_biorxiv.html#methods-comparing-binary-vs.-rank-based-classification",
    "title": "What is a Differentially Expressed Gene?",
    "section": "Methods: Comparing Binary vs. Rank-Based Classification",
    "text": "Methods: Comparing Binary vs. Rank-Based Classification\nThe study uses a published yeast RNA-Seq dataset comprising over 40 replicates (42 wild-type and 44 SNF2-mutant) to compare traditional DEG identification methods with a new rank-based Bayesian framework called bayexpress.\n\nTraditional Methods: DESeq2 and edgeR, which classify DEGs using a binary approach based on a p-value cutoff and an absolute \\(\\log_{2}\\) fold change threshold (e.g., \\(|log_{2}\\) fold change \\(|&gt;1\\)).\nBayesian Method (bayexpress): Uses Bayes factors (\\(BF_{21}\\) for evidence of change and \\(BF_{k1}\\) for consistency across replicates) to rank genes based on statistical evidence, thereby communicating uncertainty and reducing reliance on arbitrary thresholds.",
    "crumbs": [
      "genetics",
      "Papers",
      "What is a Differentially Expressed Gene?"
    ]
  },
  {
    "objectID": "genetics/hoerbst_2025_biorxiv.html#key-results",
    "href": "genetics/hoerbst_2025_biorxiv.html#key-results",
    "title": "What is a Differentially Expressed Gene?",
    "section": "Key Results",
    "text": "Key Results\n\n1. Fold Change Cut-offs Lead to False Negatives\nThe practice of applying an absolute \\(\\log_{2}\\) fold change cut-off is shown to increase the number of potential false negatives. This occurs because genes with small fold changes but strong statistical evidence (\\(BF_{21}\\) reports strong evidence) and potentially significant biological consequences are often excluded from analysis by binary thresholding. The authors emphasize that the biological impact of an expression change is not necessarily correlated with its magnitude.\n\n\n2. Variability Masquerades as Differential Expression (False Positives)\nA control experiment comparing wild-type vs. wild-type samples demonstrated that inherent variability in gene expression can be wrongly identified as differential expression, generating false positives. * This effect is particularly pronounced in datasets with a limited number of replicates (e.g., 3 replicates). * The analysis identified a set of “consistently inconsistent” genes (those with high variability across replicates, indicated by a positive \\(BF_{k1}\\)), which are most prone to this misclassification in studies that lack sufficient replication to robustly assess natural variability.",
    "crumbs": [
      "genetics",
      "Papers",
      "What is a Differentially Expressed Gene?"
    ]
  },
  {
    "objectID": "genetics/hoerbst_2025_biorxiv.html#conclusions-and-recommendations",
    "href": "genetics/hoerbst_2025_biorxiv.html#conclusions-and-recommendations",
    "title": "What is a Differentially Expressed Gene?",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe findings challenge the current widespread reliance on binary classification criteria for DEG analysis. The study highlights that the choice of thresholds and the number of replicates are major factors contributing to irreproducible results and overlooked genes. The authors advocate for a shift toward rank-based methods and Bayesian statistics (like PIMMS-VAE) to communicate uncertainty and mitigate the limitations of fixed binary thresholds, especially in scenarios with high data variability or small sample sizes.",
    "crumbs": [
      "genetics",
      "Papers",
      "What is a Differentially Expressed Gene?"
    ]
  },
  {
    "objectID": "genetics/spence_2025_41193809.html",
    "href": "genetics/spence_2025_41193809.html",
    "title": "Specificity, length and luck drive gene rankings in association studies",
    "section": "",
    "text": "PubMed: 41193809\nDOI: 10.1038/s41586-025-09703-7\nOverview generated by: Claude Sonnet 4.5, 25/11/2025",
    "crumbs": [
      "genetics",
      "Papers",
      "Specificity, length and luck drive gene rankings in association studies"
    ]
  },
  {
    "objectID": "genetics/spence_2025_41193809.html#key-findings",
    "href": "genetics/spence_2025_41193809.html#key-findings",
    "title": "Specificity, length and luck drive gene rankings in association studies",
    "section": "Key Findings",
    "text": "Key Findings\nThis study provides a systematic comparison of genome-wide association studies (GWAS) and rare variant loss-of-function (LoF) burden tests, revealing fundamental differences in how these methods prioritize genes.\n\nMain Discoveries\n\nDifferent Gene Rankings: GWAS and LoF burden tests systematically prioritize different genes, even when accounting for power differences and gene-mapping issues\nTrait Specificity vs. Importance:\n\nBurden tests prioritize trait-specific genes (genes primarily affecting the studied trait)\nGWAS prioritize trait-specific variants (which can act on pleiotropic genes through context-specific effects)\n\nThree Key Factors:\n\nSpecificity: How specific a gene/variant’s effects are to the trait under study\nLength: Longer genes are more likely to be discovered by burden tests due to more LoF sites\nLuck: Random genetic drift causes variant frequencies to vary, making GWAS rankings partially stochastic",
    "crumbs": [
      "genetics",
      "Papers",
      "Specificity, length and luck drive gene rankings in association studies"
    ]
  },
  {
    "objectID": "genetics/spence_2025_41193809.html#study-design",
    "href": "genetics/spence_2025_41193809.html#study-design",
    "title": "Specificity, length and luck drive gene rankings in association studies",
    "section": "Study Design",
    "text": "Study Design\n\nData Source\n\n209 quantitative traits from UK Biobank\nAnalyzed both GWAS and LoF burden test results\n~360,000 individuals for GWAS\n~450,000 individuals for burden tests\n\n\n\nAnalytical Framework\nThe authors proposed two criteria for gene prioritization:\n\nTrait Importance: How much a gene quantitatively affects a trait (effect size)\nTrait Specificity: The importance of a gene for the studied trait relative to all other traits",
    "crumbs": [
      "genetics",
      "Papers",
      "Specificity, length and luck drive gene rankings in association studies"
    ]
  },
  {
    "objectID": "genetics/spence_2025_41193809.html#major-results",
    "href": "genetics/spence_2025_41193809.html#major-results",
    "title": "Specificity, length and luck drive gene rankings in association studies",
    "section": "Major Results",
    "text": "Major Results\n\n1. Discordant Gene Rankings\n\nOnly 26% of burden test hits fall within top GWAS loci\n74.6% of burden hits are within any GWAS locus, but often ranked much lower\nExample: NPR2 is the 2nd most significant burden test gene for height but contained in the 243rd most significant GWAS locus\n\n\n\n2. Why Burden Tests Prioritize Trait-Specific Genes\nBurden tests aggregate LoF variants within genes. The expected association strength is proportional to:\n\\[\\frac{\\gamma_1^2}{\\sum_t \\gamma_t^2}\\]\nwhere \\(\\gamma_1^2\\) is the gene’s effect on the studied trait and \\(\\sum_t \\gamma_t^2\\) is its total effect across all traits.\nKey mechanisms: - Natural selection acts more strongly on genes with larger total effects across traits - This keeps LoF frequencies lower for pleiotropic genes - Result: Trait-specific genes have more power in burden tests despite potentially smaller effects\n\n\n3. Why GWAS Capture Pleiotropic Genes\nGWAS prioritize trait-specific variants, which can arise in two ways:\n\nVariants affecting trait-specific genes\nContext-specific variants on pleiotropic genes (e.g., tissue-specific regulatory variants)\n\nThe study found: - Variants in tissue-specific ATAC peaks show higher heritability enrichment - Coding variants in specifically expressed genes contribute more to heritability - This explains why GWAS can identify highly pleiotropic genes missed by burden tests\n\n\n4. Gene Length Bias in Burden Tests\n\nLonger genes have more potential LoF sites\nThis increases LoF carrier frequency, boosting statistical power\nEffect: Longer genes appear more significant and more pleiotropic, independent of their true biological importance\n\n\n\n5. The Role of Genetic Drift in GWAS\n\nRandom drift causes variant frequencies to vary widely around their expected values\nFor sufficiently important variants, GWAS rankings become largely random with respect to true effect size\nHigh-frequency variants have more power, creating an apparent “pleiotropy” of top GWAS hits (statistical artifact)",
    "crumbs": [
      "genetics",
      "Papers",
      "Specificity, length and luck drive gene rankings in association studies"
    ]
  },
  {
    "objectID": "genetics/spence_2025_41193809.html#estimating-trait-importance",
    "href": "genetics/spence_2025_41193809.html#estimating-trait-importance",
    "title": "Specificity, length and luck drive gene rankings in association studies",
    "section": "Estimating Trait Importance",
    "text": "Estimating Trait Importance\nNeither method directly ranks genes by trait importance:\n\nBurden tests: Flattening effect - most important genes are most constrained, leading to smallest frequencies and largest standard errors\nGWAS: Individual variant rankings dominated by random frequency variation\n\nSolution: Aggregate signals across multiple variant types - Methods like AMM that combine evidence across many variants per gene - Better correlates with selection coefficients (proxy for importance) - Overcomes the flattening problem",
    "crumbs": [
      "genetics",
      "Papers",
      "Specificity, length and luck drive gene rankings in association studies"
    ]
  },
  {
    "objectID": "genetics/spence_2025_41193809.html#biological-implications",
    "href": "genetics/spence_2025_41193809.html#biological-implications",
    "title": "Specificity, length and luck drive gene rankings in association studies",
    "section": "Biological Implications",
    "text": "Biological Implications\n\nContext-Specific Variants\nThe finding that many GWAS loci lack burden signals suggests context-specific variants acting on pleiotropic genes are major drivers of complex traits. The authors hypothesize these may include: - Developmental genes - Variants that perturb developmental trajectories in trait-specific ways - Tissue-specific regulatory elements\n\n\nDrug Target Discovery\n\nTrait-specific genes (identified by burden tests) may be better drug targets due to fewer side effects\nExplains why LoF burden evidence is more predictive of drug trial success than GWAS evidence\nHowever, if pleiotropic genes can be targeted context-specifically, they may have greater clinical impact",
    "crumbs": [
      "genetics",
      "Papers",
      "Specificity, length and luck drive gene rankings in association studies"
    ]
  },
  {
    "objectID": "genetics/spence_2025_41193809.html#methodological-insights",
    "href": "genetics/spence_2025_41193809.html#methodological-insights",
    "title": "Specificity, length and luck drive gene rankings in association studies",
    "section": "Methodological Insights",
    "text": "Methodological Insights\n\nS-LDSC Analysis\nThe study used stratified LD score regression to show: - Heritability enrichment in tissue-specific ATAC peaks - Coding variants in specifically expressed genes contribute more to heritability - Both axes (gene specificity and variant context-specificity) independently contribute to GWAS signals\n\n\nPopulation Genetics Modeling\nUsed stabilizing selection models to predict: - LoF frequencies inversely proportional to selection coefficient (\\(s_{het}\\)) - Selection coefficient proportional to total trait effects across all traits - Explains observed relationship between constraint and LoF frequency",
    "crumbs": [
      "genetics",
      "Papers",
      "Specificity, length and luck drive gene rankings in association studies"
    ]
  },
  {
    "objectID": "genetics/spence_2025_41193809.html#limitations-and-considerations",
    "href": "genetics/spence_2025_41193809.html#limitations-and-considerations",
    "title": "Specificity, length and luck drive gene rankings in association studies",
    "section": "Limitations and Considerations",
    "text": "Limitations and Considerations\n\nComplexity of gene effects: The simplified model (\\(\\alpha = \\beta \\gamma\\)) may not capture non-linear relationships\nIncomplete pleiotropy landscape: Only 27 traits analyzed, actual pleiotropy may be higher\nContext-specificity: Not all pleiotropic genes can be therapeutically targeted in context-specific ways",
    "crumbs": [
      "genetics",
      "Papers",
      "Specificity, length and luck drive gene rankings in association studies"
    ]
  },
  {
    "objectID": "genetics/spence_2025_41193809.html#practical-recommendations",
    "href": "genetics/spence_2025_41193809.html#practical-recommendations",
    "title": "Specificity, length and luck drive gene rankings in association studies",
    "section": "Practical Recommendations",
    "text": "Practical Recommendations\n\nFor Association Study Interpretation\n\nUse both methods: GWAS and burden tests reveal complementary aspects of trait biology\nConsider gene length: Longer genes in burden test results may be artifacts\nAggregate variants: For trait importance, use methods that combine signals across variants (AMM, MAGMA)\nMind the drift: Top GWAS hits are partially determined by random frequency variation\n\n\n\nFor Drug Development\n\nBurden test hits may indicate better targets for minimizing side effects\nGWAS hits may reveal pleiotropic genes with larger phenotypic impact\nConsider whether context-specific targeting is feasible",
    "crumbs": [
      "genetics",
      "Papers",
      "Specificity, length and luck drive gene rankings in association studies"
    ]
  },
  {
    "objectID": "genetics/spence_2025_41193809.html#conclusions",
    "href": "genetics/spence_2025_41193809.html#conclusions",
    "title": "Specificity, length and luck drive gene rankings in association studies",
    "section": "Conclusions",
    "text": "Conclusions\nGWAS and LoF burden tests systematically prioritize different genes because:\n\nBurden tests rank by gene-level trait specificity, favoring long, trait-specific genes\nGWAS rank by variant-level trait specificity, capturing both trait-specific genes and context-specific variants on pleiotropic genes\n\nNeither method directly ranks by trait importance due to: - Burden tests: Flattening from natural selection - GWAS: Random genetic drift\nBoth methods are valuable and reveal distinct aspects of trait biology. The choice of method depends on the research question and application, with burden tests better for identifying specific biology and GWAS better for comprehensive discovery including pleiotropic mechanisms.",
    "crumbs": [
      "genetics",
      "Papers",
      "Specificity, length and luck drive gene rankings in association studies"
    ]
  },
  {
    "objectID": "genetics/spence_2025_41193809.html#related-concepts",
    "href": "genetics/spence_2025_41193809.html#related-concepts",
    "title": "Specificity, length and luck drive gene rankings in association studies",
    "section": "Related Concepts",
    "text": "Related Concepts\n\nTrait specificity (\\(\\Psi_G\\)): \\(\\gamma_1^2 / \\sum_t \\gamma_t^2\\) for genes\nTrait importance: \\(\\gamma_1^2\\) (squared effect size)\nFlattening: Association strength becomes independent of effect size for highly important genes\nContext-specificity: Variants acting only in certain cellular contexts or developmental stages",
    "crumbs": [
      "genetics",
      "Papers",
      "Specificity, length and luck drive gene rankings in association studies"
    ]
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html",
    "href": "genetics/dewalsche_2025_39792927.html",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "",
    "text": "PubMed: 39792927\nDOI: 10.1371/journal.pgen.1011553\nOverview generated by: Claude Sonnet 4.5, 25/11/2025",
    "crumbs": [
      "genetics",
      "Papers",
      "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis"
    ]
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#key-findings",
    "href": "genetics/dewalsche_2025_39792927.html#key-findings",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Key Findings",
    "text": "Key Findings\nThis study introduces metaGE, a meta-analysis approach for detecting quantitative trait loci (QTL) in multi-environment trial (MET) experiments by jointly analyzing summary statistics from individual environment GWAS, addressing the challenge of genotype-by-environment (GxE) interactions in plant genetics.\n\nMain Discoveries\n\nSuperior Type I error control: metaGE effectively controls false discovery rate (FDR ≤ 0.05) across simulated scenarios, while competing methods (METAL, mash) show severe inflation (FDR &gt; 0.84)\nComputational efficiency: Analyzes large-scale datasets (22 environments × 600K markers) in ~2 minutes, dramatically faster than existing mixed-model approaches\nNovel QTL detection: Identified new loci in Arabidopsis (competition-responsive flowering QTLs) and maize (heat-stress responsive yield QTLs) not detected by original single-environment analyses\nFlexible testing framework: Enables detection of QTLs with stable effects, environment-specific effects, or effects correlated with environmental covariates",
    "crumbs": [
      "genetics",
      "Papers",
      "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis"
    ]
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#study-design",
    "href": "genetics/dewalsche_2025_39792927.html#study-design",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Study Design",
    "text": "Study Design\n\nMethodological Framework\nmetaGE jointly analyzes summary statistics (effect sizes and P-values) from per-environment GWAS without requiring raw phenotypic or genotypic data.\nKey innovations: - Accounts for inter-environment correlations arising from overlapping genotype panels - Supports both controlled (fixed-effect) and uncontrolled (random-effect) environments - Includes meta-regression to detect QTL-environment covariate relationships\n\n\nStatistical Models\nZ-score transformation: \\[Z_{mk} = \\Phi^{-1}(0.5 \\cdot p_{mk}) \\times \\text{sign}(\\beta_{mk})\\]\nwhere β_mk is the marker effect and p_mk is the P-value for marker m in environment k.\nFixed Effect (FE) Model (controlled environments): \\[Z_m = X\\mu_m + E_m\\] \\[E_m \\sim N(0_K, \\Sigma_m)\\]\n\nEnvironments classified into J groups with stable effects within groups\nTests for marker association: H₀: {μ_m = 0_J} vs H₁: {∃j, μ^j_m ≠ 0}\nTests for effect heterogeneity across groups\n\nRandom Effect (RE) Model (uncontrolled environments): \\[Z_m = \\mu_m \\mathbf{1}_K + A_m + E_m\\] \\[A_m \\sim N(0_K, \\tau_m^2 \\Lambda)\\]\n\nRandom marker effects account for heterogeneity\nCorrelation matrices Σ and Λ estimated from data\n\nMeta-Regression Test: \\[H_0: \\{\\text{cov}(\\mu_m \\mathbf{1}_K + A_m, X) = 0\\} \\text{ vs } H_1: \\{\\text{cov}(\\mu_m \\mathbf{1}_K + A_m, X) \\neq 0\\}\\]\nTest statistic: \\(\\frac{Z_m^T X}{\\sqrt{X^T \\Sigma X}} \\sim N(0,1)\\) under H₀\n\n\nCorrelation Matrix Estimation\nTwo filtering approaches to identify H₀ markers:\n\nP-value threshold: Include markers with p_mk &gt; λ in all environments\nPosterior probability (default): Mixture model-based filtering excluding markers with P(H₁) &gt; 0.6\n\nCorrelation estimate: \\[\\hat{\\Sigma}_{k,k'} = \\text{cor}(Z_k, Z_{k'}) = \\frac{\\sum_{m \\in H_0} (Z_{mk} - \\bar{Z}_k)(Z_{mk'} - \\bar{Z}_{k'})}{\\sqrt{\\sum (Z_{mk} - \\bar{Z}_k)^2} \\sqrt{\\sum (Z_{mk'} - \\bar{Z}_{k'})^2}}\\]",
    "crumbs": [
      "genetics",
      "Papers",
      "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis"
    ]
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#simulation-study",
    "href": "genetics/dewalsche_2025_39792927.html#simulation-study",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Simulation Study",
    "text": "Simulation Study\n\nDesign\n\nGenotypes: 247 maize F1 hybrids, 506,460 SNPs\nEnvironments: 22 trials\nQTL types:\n\nFixed effect (constant across environments)\nCompletely random effect\nRandom effects correlated with environment similarities\n\nCovariate-dependent (proportional to Tmax, Tnight, or Psi)\n\nSimulations: 50 runs per QTL type, 12 QTLs per run\nHeritability: 0.5, QTLs explain 44% of genetic variance\n\n\n\nType I Error Control\nFDR on H₀ chromosomes (most stringent):\n\n\n\nMethod\nQTL Type\nFDR_chr\n\n\n\n\nmetaGE_FE\nFixed\n0.00\n\n\nmetaGE_RE\nRandom\n0.04\n\n\nmetaGE_RE\nRandomCov\n0.02\n\n\nmetaGE_MR\nCovariate\n0.00-0.02\n\n\nMETAL_FE\nFixed\n1.00\n\n\nMETAL_RE\nRandom/Cov\n0.88-1.00\n\n\nmash\nAll types\n0.14-0.88\n\n\n\nWhole genome FDR (5 Mb window):\n\n\n\nMethod\nRange across scenarios\n\n\n\n\nmetaGE\n0.09-0.18\n\n\nMETAL\n0.93-0.94\n\n\nmash\n0.32-0.85\n\n\n\n\n\nDetection Power\n5 Mb detection window:\n\n\n\nQTL Type\nmetaGE\nMETAL\nmash\n\n\n\n\nFixed effect\n0.09\n0.98\n0.20\n\n\nRandom effect\n0.51\n0.16\n0.79\n\n\nRandomCov\n0.26\n0.58\n0.53\n\n\nCovariate (avg)\n0.37\n0.06\n0.41\n\n\n\nDespite lower raw power than competitors, metaGE’s proper FDR control makes identified associations reliable.\nMAF effect on power (metaGE_FE): - Low MAF [0.20-0.25]: 0.04 - Medium MAF [0.30-0.35]: 0.08 - High MAF [0.40-0.45]: 0.14\n\n\nMeta-Regression Specificity\nTesting covariate-dependent QTLs:\n\n\n\nMR test\nTarget QTLs detected\nCross-detected\n\n\n\n\nTnight\n34.5% (Tnight QTLs)\n5.5% (Tmax QTLs)\n\n\nTmax\n26.5% (Tmax QTLs)\n7% (Tnight QTLs)\n\n\nPsi\n52% (Psi QTLs)\n&lt;1% (others)\n\n\n\nCross-detection correlated with environmental covariate correlation (r_Tnight-Tmax = 0.71, r_Tnight-Psi = -0.11).",
    "crumbs": [
      "genetics",
      "Papers",
      "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis"
    ]
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#application-i-arabidopsis-competition-response",
    "href": "genetics/dewalsche_2025_39792927.html#application-i-arabidopsis-competition-response",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Application I: Arabidopsis Competition Response",
    "text": "Application I: Arabidopsis Competition Response\n\nDataset\n\n195 accessions, 981,278 SNPs\n6 controlled micro-habitats (3 soils × competition/no competition)\nTrait: Bolting time\nCompetition: Poa annua weed in environments B, D, F\n\n\n\nResults\nmetaGE FE procedure: - 191 SNPs in 61 QTLs identified - 51/61 significant in at least one individual GWAS - Enrichment ratio = 4.13 for candidate flowering genes (q₀.₀₅ = 0.066, q₀.₉₅ = 3.2)\nComparison with METAL: - METAL: &gt;165,000 P-values &lt;0.01 (expected ~10,000 under H₀) - Declared 15% of markers significant (severe inflation)\nContrasted FE test (competition vs. no competition): - 221 SNPs in 72 QTLs with environment-specific effects - 160 candidate genes enriched for: - Development (P = 8.9×10⁻³) - Cell processes (P = 1.5×10⁻³) - Tetrapyrrole synthesis (P = 0.020) - 71/72 QTLs were novel (not detected by standard FE test)\n\n\nMajor Finding: QTL5_22.0\nLocation: Chromosome 5, AtCNGC4 genomic region - 22 markers with sign-switching effects based on competition - Positive effects without competition, negative/null with competition - AtCNGC4 known roles: - Floral transition regulation - Plant immunity impairment - Consistent with development-defense tradeoffs",
    "crumbs": [
      "genetics",
      "Papers",
      "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis"
    ]
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#application-ii-maize-drought-response",
    "href": "genetics/dewalsche_2025_39792927.html#application-ii-maize-drought-response",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Application II: Maize Drought Response",
    "text": "Application II: Maize Drought Response\n\nDataset\n\n244 dent maize lines (as hybrids), 602,356 SNPs\n22 environments (location × year × treatment)\nTrait: Grain yield\nEnvironmental covariates: Psi, Tmax, Tnight, Rad, VPDmax, ET0, Tnight.Fill\n\n\n\nmetaGE RE Results\n52 genomic regions identified, including:\n\n\n\nQTL\nChr\nLocal Score\nDetection status\n\n\n\n\nQTL3_120.0\n3\n38\nPreviously reported\n\n\nQTL6_20.3\n6\n415\nPreviously reported\n\n\nQTL7_41.4\n7\n18\nNovel\n\n\n\nQTL6_20.3 analysis: - Strong effects in 6 environments with severe heatwaves: - Night temperature ~22°C - Maximum temperature &gt;36°C - High evaporative demand (3.6 KPa) - All 6 environments: P-values &lt;1×10⁻⁶ - Colocalizes with 2.4 Mb presence/absence variant - Contains ABA-induced genes for water deficit response - Shows selection signatures during domestication/improvement\nQTL7_41.4 (novel): - Moderate positive effects across ~10/22 environments - Significant in only 2 individual GWAS (P &lt;0.01 in 10) - Harbors QTLs for plant growth rate and biomass under water deficit - Demonstrates power gain from meta-analysis\n\n\nMeta-Regression Results\nEvapotranspiration (ET0): 14 QTLs detected\nKey finding - QTL2_153.8 (marker AX-91538480): - Effects vary linearly from negative to positive with ET0 - Colocalizes with aquaporin eQTLs (PIP2.2, PIP2.1) - Related to water use efficiency and stomatal conductance\nNight temperature during flowering (Tnight): 21 QTLs - Main association &lt;0.6 Mb from QTL6_20.3 - Corroborates previous findings on heat stress response\nNight temperature during grain filling (Tnight.Fill): 15 QTLs\nExample - QTL9_28.6 (marker AX-91123283): - Positive effects on cool nights - Negative effects on hot nights - Dramatic effect reversal with temperature",
    "crumbs": [
      "genetics",
      "Papers",
      "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis"
    ]
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#application-iii-multi-parent-population",
    "href": "genetics/dewalsche_2025_39792927.html#application-iii-multi-parent-population",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Application III: Multi-Parent Population",
    "text": "Application III: Multi-Parent Population\n\nEU-NAM Flint Dataset\n\n11 biparental populations (8 analyzed)\n5,263 SNPs, double haploid lines\n4 locations: La Coruna, Roggenstein, Einbeck, Ploudaniel\nTrait: Biomass dry matter yield\n32 analyses (8 populations × 4 locations)\n\n\n\nResults\n16 QTLs identified, including: - 2 major QTLs also found in original publication (Garin et al.): - QTL1_117.6: Consistent across populations except F2 - QTL6_84.2: Ancestral allele (6 parents) with strong negative effect in TUM\n10 novel QTLs, including: - 5 QTLs with effect inversions between populations\nExample - QTL5_23.9: - Positive effect in F03802 population - Negative effect in F64 population - Suggests genetic background effects or allelic series\n3 QTLs associated with flowering time: - Flowering time is simpler trait and yield driver - Correlation with yield varies by environment (negative/null/positive)\n\n\nAdvantages Over Original Analysis\nOriginal study (Garin et al.): - Limited to 2/4 locations - Analyzed with computationally intensive mixed models\nmetaGE approach: - Included all 4 locations - Revealed 10 additional QTLs - Completed in 12 seconds vs. hours for mixed models",
    "crumbs": [
      "genetics",
      "Papers",
      "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis"
    ]
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#application-iv-wheat-supplementary",
    "href": "genetics/dewalsche_2025_39792927.html#application-iv-wheat-supplementary",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Application IV: Wheat (Supplementary)",
    "text": "Application IV: Wheat (Supplementary)\n\nDataset\n\n210 wheat lines, 108,410 SNPs\n16 environments (location × year × treatment)\nTrait: Grain yield\n\n\n\nKey Findings\n\nAll QTLs identified by metaGE RE were not significant in any single environment\nDemonstrates power gain for complex traits with small-effect QTLs\nHighlights importance of joint analysis for yield traits",
    "crumbs": [
      "genetics",
      "Papers",
      "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis"
    ]
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#computational-performance",
    "href": "genetics/dewalsche_2025_39792927.html#computational-performance",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Computational Performance",
    "text": "Computational Performance\nRuntime comparison (dataset: marker count):\n\n\n\nDataset\nEnvironments\nmetaGE\nMETAL\nmash\n\n\n\n\nSimulation (500K)\n22\n49s (31s*)\n2.6min\n16.6min\n\n\nArabidopsis (1M)\n6\n1.2min (26s*)\n2.6min\n29s\n\n\nMaize (600K)\n22\n2.25min (41s*)\n3.3min\n25.3min\n\n\nEU-NAM (6K)\n32\n12s (8s*)\n3s\n1.8min\n\n\nWheat (100K)\n16\n47s (30s*)\n22s\n3.3min\n\n\n\n*Time for correlation matrix inference (needs to be done only once)\nMemory efficiency: - Handles 10⁵-10⁶ markers efficiently - Single correlation matrix estimation per analysis - Independent processing of multiple hypotheses without re-estimation",
    "crumbs": [
      "genetics",
      "Papers",
      "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis"
    ]
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#methodological-advantages",
    "href": "genetics/dewalsche_2025_39792927.html#methodological-advantages",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Methodological Advantages",
    "text": "Methodological Advantages\n\nOver Classical Meta-Analysis (METAL)\nDependency handling: - METAL assumes independence between GWAS - Ignoring dependencies in MET causes severe FDR inflation (&gt;0.84) - metaGE explicitly models inter-environment correlations\nResult: METAL unusable for MET analysis due to Type I error inflation\n\n\nOver Mixture Models (mash)\nEnvironmental factors: - mash models different effect patterns but not environmental influences - Cannot incorporate environmental covariates - Limited ability to test specific biological hypotheses about GxE\nResult: mash suitable for pleiotropy but not designed for MET analysis\n\n\nOver Mixed Models\nScalability: - Mixed models computationally prohibitive for large-scale GWAS - Require raw phenotypic and genotypic data - metaGE: summary statistics only, minutes vs. hours/days\nFlexibility: - Easy addition/removal of environments - Handles missing data (monomorphic markers in subpopulations) - Supports unbalanced/incomplete designs without imputation\n\n\nComparison to Subgroup Meta-Analysis\nPrevious work (human genetics): - Subgroup MA and meta-regression developed for independent studies - Not adapted to correlated studies (MET with overlapping panels)\nmetaGE contribution: - First adaptation of these approaches to non-independent studies - Enables plant genetics applications",
    "crumbs": [
      "genetics",
      "Papers",
      "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis"
    ]
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#novel-testing-capabilities",
    "href": "genetics/dewalsche_2025_39792927.html#novel-testing-capabilities",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Novel Testing Capabilities",
    "text": "Novel Testing Capabilities\n\n1. Standard Association Test\nH₀: {μ_m = 0} - marker has no effect in any environment - Detects QTLs with any non-zero effect\n\n\n2. Heterogeneity Test\nH₀: {μ¹_m = μ²_m = … = μᴶ_m} - effects constant across groups - Identifies environment-dependent QTLs\n\n\n3. Contrast Test\nTests specific hypotheses about effect patterns - Example: Competition vs. no competition in Arabidopsis - Detected 71 new QTLs missed by standard test\n\n\n4. Meta-Regression\nGenome-wide scan for QTL-covariate relationships - Quantifies how QTL effects vary with environmental variables - Identifies adaptive QTLs responding to specific stresses",
    "crumbs": [
      "genetics",
      "Papers",
      "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis"
    ]
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#biological-insights",
    "href": "genetics/dewalsche_2025_39792927.html#biological-insights",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Biological Insights",
    "text": "Biological Insights\n\nPower Gain Through Joint Analysis\nArabidopsis AtCNGC4 region: - Not genome-wide significant in individual environments - Highly significant in joint analysis - Biological relevance confirmed (floral transition, immunity)\nMaize QTL7_41.4: - Significant in only 2/22 environments individually - Detected through meta-analysis - Contains known water deficit response QTLs\nWheat QTLs: - None significant in individual environments - Multiple QTLs detected jointly - Critical for complex yield traits\n\n\nInterpreting Effect Variability\nCompetition response (Arabidopsis): - Sign-switching effects indicate context-dependent gene function - Development-defense tradeoffs - Identifies condition-specific adaptive alleles\nHeat stress response (Maize): - QTL6_20.3 effects clustered in heatwave environments - Presence/absence variant under selection - Adaptive response to temperature stress\nCovariate-dependent effects: - Linear relationships between effects and ET0, temperature - Aquaporin-mediated water transport regulation - Plant growth sensitivity to water potential",
    "crumbs": [
      "genetics",
      "Papers",
      "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis"
    ]
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#data-sharing-and-privacy",
    "href": "genetics/dewalsche_2025_39792927.html#data-sharing-and-privacy",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Data Sharing and Privacy",
    "text": "Data Sharing and Privacy\n\nAdvantages of Summary Statistics\nConfidentiality: - No raw phenotypic or genotypic data required - Only effect sizes and P-values needed - Enables data sharing between private breeding programs\nParallel to human genetics: - Global Biobank Meta-analysis Initiative (2.2M participants, 24 BioBanks) - Consortium approach without individual data sharing\nPlant breeding applications: - Private companies can share GWAS results - Preserve competitive advantages - Collaborative QTL discovery\n\n\nTechnical Benefits\nUnbalanced designs: - Different markers tested per environment - Missing data due to monomorphism in subpopulations - No imputation required\nScale flexibility: - Different technologies/sequencing depths - Easy environment addition/removal - Post-hoc quality control\nMulti-parent populations: - Different marker sets per family - Handles genetic background effects - Detects allelic series and epistasis",
    "crumbs": [
      "genetics",
      "Papers",
      "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis"
    ]
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#practical-recommendations",
    "href": "genetics/dewalsche_2025_39792927.html#practical-recommendations",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Practical Recommendations",
    "text": "Practical Recommendations\n\nWhen to Use metaGE\nIdeal scenarios: - MET experiments with overlapping genotype panels - Need to control Type I error rate - Testing specific GxE hypotheses - Limited computational resources - Data privacy concerns\nNot recommended: - Single environment analysis (use standard GWAS) - Completely independent populations (classical MA sufficient) - Need individual-level covariate adjustments\n\n\nModel Selection\nFixed Effect (FE) model: - Controlled environments with a priori classification - Testing specific group contrasts - Example: Stress vs. control treatments\nRandom Effect (RE) model: - Uncontrolled field conditions - Unknown/complex environment relationships - Heterogeneous QTL effects expected\nMeta-Regression: - Quantitative environmental covariates available - Hypothesis about specific environmental drivers - Want to identify adaptive QTLs\n\n\nMultiple Testing Control\nLocal score approach (default): - Controls FDR while accounting for LD - Accumulates evidence across linked markers - Threshold ξ typically 3-4 - Chromosome-specific significance thresholds\nAlternative: Adaptive Benjamini-Hochberg - For low-density markers (e.g., MPP with &lt;10K SNPs) - When LD structure unknown",
    "crumbs": [
      "genetics",
      "Papers",
      "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis"
    ]
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#implementation-details",
    "href": "genetics/dewalsche_2025_39792927.html#implementation-details",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Implementation Details",
    "text": "Implementation Details\n\nR Package: metaGE\nAvailable on CRAN\nKey functions: - Fixed effect meta-analysis - Random effect meta-analysis\n- Contrast testing - Meta-regression - Local score multiple testing correction\nInput requirements: - Per-environment GWAS summary statistics (effects, P-values) - Marker positions - Optional: Environmental covariates\nOutputs: - Meta-analysis P-values - Estimated correlation matrices - Significant genomic regions - Effect size estimates per environment/group",
    "crumbs": [
      "genetics",
      "Papers",
      "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis"
    ]
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#limitations-and-considerations",
    "href": "genetics/dewalsche_2025_39792927.html#limitations-and-considerations",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Limitations and Considerations",
    "text": "Limitations and Considerations\n\nStatistical Assumptions\n\nMarker independence: Assumes unlinked markers\n\nAddressed by local score accounting for LD\n\nCorrelation matrix: Assumed common across markers\n\nReasonable for inter-environment correlations\nReduces computational burden\n\nNormal distribution: Z-scores assumed Gaussian under H₀\n\nStandard assumption in GWAS\nViolated if P-values not uniformly distributed under null\n\n\n\n\nDesign Considerations\nEnvironment classification: - FE model requires a priori grouping - Misclassification reduces power - RE model robust to classification uncertainty\nSample size: - Power increases with more environments - Individual environment sample sizes affect P-value quality - Minimum ~5-10 environments recommended\nCovariate correlation: - Meta-regression may detect QTLs correlated with related covariates - Careful interpretation needed with high covariate correlation - Consider testing multiple covariates independently",
    "crumbs": [
      "genetics",
      "Papers",
      "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis"
    ]
  },
  {
    "objectID": "genetics/dewalsche_2025_39792927.html#related-concepts",
    "href": "genetics/dewalsche_2025_39792927.html#related-concepts",
    "title": "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis",
    "section": "Related Concepts",
    "text": "Related Concepts\n\nMulti-environment trial (MET): Same genotypes evaluated across locations/years\nGenotype-by-environment (GxE) interaction: Differential genotypic response to environments\nZ-score: Standardized test statistic combining effect sign and P-value\nInter-environment correlation: Similarity between environments due to overlapping panels\nLocal score: LD-aware multiple testing procedure accumulating evidence\nMeta-regression: Modeling association between study-level covariates and effect sizes\nAllelic series: Multiple haplotypes at a locus with varying effects",
    "crumbs": [
      "genetics",
      "Papers",
      "metaGE: Investigating genotype x environment interactions through GWAS meta-analysis"
    ]
  },
  {
    "objectID": "genetics/tambets_2024_39210598.html",
    "href": "genetics/tambets_2024_39210598.html",
    "title": "Extensive co-regulation of neighboring genes complicates the use of eQTLs in target gene prioritization",
    "section": "",
    "text": "PubMed: 39210598 DOI: 10.1016/j.xhgg.2024.100348 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "genetics",
      "Papers",
      "Extensive co-regulation of neighboring genes complicates the use of eQTLs in target gene prioritization"
    ]
  },
  {
    "objectID": "genetics/tambets_2024_39210598.html#key-findings-limitations-of-eqtl-colocalization-in-causal-gene-prioritization",
    "href": "genetics/tambets_2024_39210598.html#key-findings-limitations-of-eqtl-colocalization-in-causal-gene-prioritization",
    "title": "Extensive co-regulation of neighboring genes complicates the use of eQTLs in target gene prioritization",
    "section": "Key Findings: Limitations of eQTL Colocalization in Causal Gene Prioritization",
    "text": "Key Findings: Limitations of eQTL Colocalization in Causal Gene Prioritization\nThe study addresses the fundamental problem in human genetics of linking non-coding regulatory variants from genome-wide association studies (GWASs) to their causal target genes. It establishes that the practice of using gene expression quantitative trait loci (eQTLs) for colocalization analysis to prioritize targets is complicated by the extensive co-regulation of neighboring genes. The authors found that existing colocalization methods are generally outperformed by a much simpler heuristic: assigning the fine-mapped variant to the closest protein-coding gene.",
    "crumbs": [
      "genetics",
      "Papers",
      "Extensive co-regulation of neighboring genes complicates the use of eQTLs in target gene prioritization"
    ]
  },
  {
    "objectID": "genetics/tambets_2024_39210598.html#study-design-and-benchmarking-methods",
    "href": "genetics/tambets_2024_39210598.html#study-design-and-benchmarking-methods",
    "title": "Extensive co-regulation of neighboring genes complicates the use of eQTLs in target gene prioritization",
    "section": "Study Design and Benchmarking Methods",
    "text": "Study Design and Benchmarking Methods\n\nGround Truth Dataset\nThe researchers created a large ground truth dataset by re-analyzing fine-mapped plasma protein QTL (pQTL) data from 3,301 individuals in the INTERVAL cohort. Focusing on variants located within or close to the affected protein (cis-pQTLs), they assumed that the gene coding for the protein was the most likely causal gene for 793 proteins. This assumption provided a robust ground truth set for systematic benchmarking.\n\n\nMethods Evaluated\nThe study systematically compared three Bayesian colocalization methods and five Mendelian Randomization (MR) approaches:\n\nColocalization Methods:\n\ncoloc.susie (supports multiple causal variants).\ncoloc.abf (assumes a single causal variant).\nCLPP (colocalization posterior probability defined at the variant level).\n\nMendelian Randomization (MR) Methods: Five varieties were tested, including standard inverse-variance weighted MR (IVW-MR), MR-RAPS, and MRLocus.",
    "crumbs": [
      "genetics",
      "Papers",
      "Extensive co-regulation of neighboring genes complicates the use of eQTLs in target gene prioritization"
    ]
  },
  {
    "objectID": "genetics/tambets_2024_39210598.html#results",
    "href": "genetics/tambets_2024_39210598.html#results",
    "title": "Extensive co-regulation of neighboring genes complicates the use of eQTLs in target gene prioritization",
    "section": "Results",
    "text": "Results\n\nPerformance of Prioritization Strategies\n\nClosest Gene Heuristic: Assigning pQTLs to their closest protein coding gene achieved the highest performance overall: 76.9% recall and 71.9% precision.\nColocalization Methods: When comparing performance using only the strongest signal, coloc.susie had the highest recall (46.3%) but lowest precision (45.1%). Conversely, CLPP was the most precise (68.5%) but yielded the correct gene for less than a fifth of the proteins (17.5% recall).\nColocalization + MR: Combining colocalization evidence with MR—restricting analysis to cases with two or more independent colocalizing signals—increased precision substantially to 81%. However, this led to a massive reduction in recall to just 7.1%, primarily due to the limited power of eQTL datasets to detect secondary eQTL signals.\n\n\n\nMR Assumption Violations\nThe standard inverse-variance-weighted MR often produced many false positives. The authors found that cis-eQTLs frequently violated MR assumptions, underscoring that using robust inference methods to account for these violations is essential to avoid inaccurate results.",
    "crumbs": [
      "genetics",
      "Papers",
      "Extensive co-regulation of neighboring genes complicates the use of eQTLs in target gene prioritization"
    ]
  },
  {
    "objectID": "genetics/tambets_2024_39210598.html#conclusions-and-recommendations",
    "href": "genetics/tambets_2024_39210598.html#conclusions-and-recommendations",
    "title": "Extensive co-regulation of neighboring genes complicates the use of eQTLs in target gene prioritization",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study demonstrates that colocalization methods using eQTL data alone are insufficient and less effective than expected for accurately prioritizing causal genes underlying GWAS variants. The complexity of co-regulation highlights the need for robust methods. Ultimately, prioritizing novel targets in human genetics requires triangulation of evidence from multiple sources (e.g., integrating pQTLs, eQTLs, and other functional data) rather than relying on a single data modality.",
    "crumbs": [
      "genetics",
      "Papers",
      "Extensive co-regulation of neighboring genes complicates the use of eQTLs in target gene prioritization"
    ]
  },
  {
    "objectID": "genetics/index.html",
    "href": "genetics/index.html",
    "title": "Genetics",
    "section": "",
    "text": "Post-transcriptional regulation across human tissues\n\n\n\nCore Finding: Scaled mRNA levels predict overall mean protein abundance across different genes (mean-level variability) but are poor predictors of the same protein’s level across different tissues (across-tissues variability).\nStatistical Insight: The overall high mRNA-protein correlation (\\(R_T\\)) is misleading, as it represents an instance of Simpson’s paradox where the strong inter-gene variability masks the weak intra-gene/across-tissue correlation (\\(R_P\\)).\nConclusion: The reproducible, concerted variability in protein-to-mRNA ratios across tissues confirms that post-transcriptional regulation is a substantial and tissue-specific factor, likely contributing approximately 50% of the across-tissues protein variance.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nWhat is a Differentially Expressed Gene?\n\n\n\nCentral Problem: Traditional binary classification of Differentially Expressed Genes (DEGs) using p-value and \\(\\log_{2}\\) fold change thresholds suffers from reproducibility issues due to biological and technical variation, especially with low replicate numbers.\nFindings: The use of rigid \\(\\log_{2}\\) fold change cut-offs leads to false negatives (missing small but significant changes), and inherent gene variability, particularly in small sample sets, leads to false positives (misinterpreting noise as a change).\nRecommendation: The authors advocate for rank-based methods grounded in Bayesian statistics (using Bayes factors) over traditional tools like DESeq2 and edgeR to reduce reliance on arbitrary thresholds and better communicate the uncertainty of differential expression.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nGenetic architecture: the shape of the genetic contribution to human traits and disease\n\n\n\nThis review defines genetic architecture by four components: the number of causal variants (polygenicity), the distribution of their effect sizes, their allele frequency spectrum, and the types of genetic and environmental interactions (dominance, epistasis, GxE).\nIt highlights that complex traits are highly polygenic and influenced by variants across the entire frequency spectrum, addressing “missing heritability” by pointing to the role of rare variants, non-additive effects, and Gene-by-Environment (GxE) interactions.\nThe authors emphasize that pleiotropy (one variant affecting multiple traits) is widespread among common variants, discussing how techniques like Mendelian Randomization (MR) are essential for distinguishing causation from pleiotropy in the complex genetic landscape.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nSpecificity, length and luck drive gene rankings in association studies\n\n\n\nSystematic comparison of GWAS and rare variant burden tests across 209 UK Biobank traits revealing they prioritize different genes through distinct mechanisms\nBurden tests favor trait-specific genes while GWAS capture both trait-specific genes and context-specific variants on pleiotropic genes\nGene length and genetic drift are major confounders affecting rankings in burden tests and GWAS respectively\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nA phenome-wide comparative analysis of genetic discordance between obesity and type 2 diabetes\n\n\n\nCore Finding: The study used genetic data to define two distinct obesity profiles that exhibit highly contrasting, or discordant, effects on the risk of developing type 2 diabetes (T2D).\nPhenotypic Differences: The two profiles showed key differences across a wide range of clinical and molecular traits, including cardiovascular mortality, liver metabolism, specific lipid fractions, and blood pressure.\nMechanism: Instrumental analyses highlighted the prominent causal role of factors like waist-to-hip ratio and blood pressure in driving the differences in T2D risk between the two genetic obesity subtypes.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nmetaGE: Investigating genotype x environment interactions through GWAS meta-analysis\n\n\n\nNovel meta-analysis approach for multi-environment trials (METs) that jointly analyzes GWAS summary statistics while accounting for inter-environment correlations\nControls Type I error effectively (FDR ≤0.05) where competing methods fail severely (METAL FDR &gt;0.84), with computational efficiency enabling analysis of 600K markers × 22 environments in ~2 minutes\nIdentified novel competition-responsive flowering QTLs in Arabidopsis and heat-stress yield QTLs in maize through contrast tests and meta-regression with environmental covariates\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nJoint analysis of GWAS and multi-omics QTL summary statistics reveals a large fraction of GWAS signals shared with molecular phenotypes\n\n\n\nObjective: The new method, OPERA, was developed to integrate GWAS and multi-omics QTL (xQTL) summary statistics to quantify the proportion of complex trait genetic signals mediated by molecular phenotypes.\nKey Finding: The study found that approximately 50% of genetic signals identified in GWAS are shared with (and likely mediated by) at least one molecular phenotype, with eQTLs (gene expression QTLs) being the most dominant mediators.\nImpact: OPERA led to the discovery of 89 novel genes for 11 complex traits, confirming the approach’s ability to significantly enhance gene discovery and fine-mapping by linking genetic variants to their underlying molecular regulatory mechanisms.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nExtensive co-regulation of neighboring genes complicates the use of eQTLs in target gene prioritization\n\n\n\nResearch Problem: Systematic benchmarking showed that using eQTL colocalization methods to prioritize causal genes for GWAS hits is often complicated by extensive co-regulation of neighboring genes and is less effective than simpler heuristics.\nKey Result: The simple strategy of assigning fine-mapped pQTLs to the closest protein coding gene significantly outperformed all tested Bayesian colocalization methods, achieving 76.9% recall and 71.9% precision.\nConclusion: Linking GWAS variants to target genes remains challenging using eQTL evidence alone, and robust gene prioritization requires the triangulation of evidence from multiple functional sources to improve confidence.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nThe contribution of genetic determinants of blood gene expression and splicing to molecular phenotypes and health outcomes\n\n\n\nTopic: Investigating the gene-regulatory mechanisms (eQTLs and sQTLs) of nonprotein-coding genetic variants in blood and their causal contribution to 3,430 molecular phenotypes (proteins, metabolites, lipids) and health outcomes.\nMethod: Mapped eQTLs and sQTLs in 4,732 individuals and used colocalization and mediation analyses to link these regulatory variants to downstream molecular and disease traits.\nImpact: Identified 222 molecular phenotypes significantly mediated by gene expression or splicing, providing mechanistic insights into diseases (e.g., \\(WARS1\\) in hypertension) and offering a valuable public resource for human genetic etiology.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nA biobank-scale test of marginal epistasis reveals genome-wide signals of polygenic interaction effects\n\n\n\nMethodology: The paper introduces FAME (FAst Marginal Epistasis test), a new, computationally efficient statistical method designed to detect marginal epistasis—the aggregate interaction effect between a single SNP and the entire polygenic background—in large biobanks.\nKey Finding: Applying FAME to 53 quantitative traits in the UK Biobank, the study identified 16 significant marginal epistasis signals across 12 traits, providing the first systematic, genome-wide evidence of polygenic interaction effects.\nImplication: The findings confirm that genetic interactions are a measurable component of complex trait architecture, suggesting that current additive GWAS models are incomplete and that marginal epistasis may contribute to the “missing heritability.”\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nValid inference for machine learning-assisted genome-wide association studies\n\n\n\nObjective: This paper addressed the critical issue of invalid statistical inference and false-positive risks in Machine Learning (ML)-assisted Genome-Wide Association Studies (GWAS), which use ML-imputed phenotypes. It introduced a new statistical framework called Post-Prediction GWAS (POP-GWAS) to ensure valid results.\nProblem: The study demonstrated that performing a standard GWAS on an ML-imputed phenotype systematically biases the results, leading to inflated p-values and a high risk of false-positive associations.\nSolution (POP-GWAS): POP-GWAS corrects for this bias by redesigning the GWAS test statistic to explicitly account for the variance introduced by the ML imputation model, requiring only GWAS summary statistics and information on the prediction model’s performance.\nValidation: Application to a GWAS of bone mineral density (BMD) revealed that POP-GWAS successfully corrected the inflated p-values and led to the identification of 89 new loci that were missed or deemed unreliable by the naive approach.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nAdjusting for Heritable Covariates Can Bias Effect Estimates in Genome-Wide Association Studies\n\n\n\nAdjusting a Genome-Wide Association Study (GWAS) for a heritable covariate (a correlated, genetically influenced trait) introduces an unintended collider bias, which distorts SNP effect estimates and can lead to false positive associations.\nThe bias is approximately proportional to the product of the genetic effect on the covariate and the phenotypic correlation between the traits, and was empirically confirmed by finding a significant enrichment of SNPs with opposite effects in the WHR adjusted for BMI GWAS (\\(p=0.005\\)).\nThe authors strongly caution against interpreting adjusted results as true direct genetic effects, recommending unadjusted GWAS for total effect discovery and bivariate methods for power gains without inducing collider bias.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nUsing GWAS summary data to impute traits for genotyped individuals\n\n\n\nNovel nonparametric LS-imputation method recovers genetic components of traits from GWAS summary statistics and individual genotypes, enabling nonlinear association analyses impossible with summary data alone\nPerfectly recovers trait values when test genotypes match training genotypes (correlation &gt;0.999), capturing nonlinear SNP-trait information despite using only linear marginal associations\nOutperforms PRS-CS for association analyses in UK Biobank HDL data: successfully detects non-additive genetic effects, SNP-SNP interactions, and trains nonlinear prediction models (random forests) while PRS-CS shows severe false positive inflation\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nProtein-metabolite association studies identify novel proteomic determinants of metabolite levels in human plasma\n\n\n\nApproach: A large-scale multi-omics resource was created by meta-analyzing proteomic and metabolomic data from three cohorts, followed by the use of Mendelian Randomization (MR) with pQTLs to infer causal relationships.\nCausal Findings: The study identified 224 putative causal associations between 95 proteins and 96 metabolites, including novel links like the causal role of ADAMTSL3 in regulating BCAA metabolites.\nValidation: Over 50% of the top causal findings were experimentally validated through metabolomic profiling of mouse knockout strains, providing strong biological proof for the in-silico MR results.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nGlobal quantification of mammalian gene expression control\n\n\n\nCore Discovery: This seminal study, using parallel metabolic pulse labeling and absolute quantification in mammalian cells, concluded that the cellular abundance of proteins is predominantly controlled at the level of translation, not transcription.\nQuantification: The study found a strong correlation between mRNA and protein abundance (\\(R \\approx 0.73\\)) but no correlation between the half-lives (turnover rates) of corresponding mRNA and proteins.\nMechanism: Protein synthesis rates (translation) were found to be the most variable component, serving as the primary determinant of protein steady-state levels, while degradation rates primarily determine the kinetics and response time of the system.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nGenome-wide association scans for secondary traits using case-control samples\n\n\n\nThis statistical methodology paper examines the bias introduced when a case-control GWAS (designed for a primary disease \\(D\\)) is used to analyze a secondary quantitative trait (\\(T\\)).\nIt demonstrates that naïve analysis (ignoring case-control ascertainment) leads to biased effect estimates for the marker-secondary trait association (\\(G-T\\)) specifically when both the marker \\(G\\) and the trait \\(T\\) are independently associated with the primary disease \\(D\\).\nThe authors propose using Inverse-Probability-of-Sampling-Weighted (IPW) regression as the robust method, which provides unbiased estimates in all scenarios, though at the cost of reduced statistical power, recommending naïve analysis for markers not associated with the primary disease.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nCausal modelling of gene effects from regulators to programs to traits\n\n\n\nFramework: This paper proposes a novel statistical approach to infer causal mechanistic pathways that link genes to traits by combining gene-trait effect sizes (from GWAS LoF burden tests) with gene-regulatory relationships (from Perturb-seq experiments).\nCausal Hierarchy: The model establishes a three-step causal graph: Gene \\(\\longrightarrow\\) Regulatory Programs \\(\\longrightarrow\\) Trait, allowing researchers to explain gene-trait associations via intermediate functional steps.\nProof of Concept: Applied to three blood traits (RDW, MCH, IRF) using human HSPC Perturb-seq data, the model successfully identified the regulatory programs (e.g., ribosomal genes) and directionally predicted how gene perturbations causally influence the traits.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "genetics",
      "Papers"
    ]
  },
  {
    "objectID": "genetics/miao_2024_39349818.html",
    "href": "genetics/miao_2024_39349818.html",
    "title": "Valid inference for machine learning-assisted genome-wide association studies",
    "section": "",
    "text": "PubMed: 39349818 DOI: 10.1038/s41588-024-01934-0 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "genetics",
      "Papers",
      "Valid inference for machine learning-assisted genome-wide association studies"
    ]
  },
  {
    "objectID": "genetics/miao_2024_39349818.html#key-findings-validating-ml-assisted-gwas",
    "href": "genetics/miao_2024_39349818.html#key-findings-validating-ml-assisted-gwas",
    "title": "Valid inference for machine learning-assisted genome-wide association studies",
    "section": "Key Findings: Validating ML-Assisted GWAS",
    "text": "Key Findings: Validating ML-Assisted GWAS\nThis paper addresses the critical issue of invalid statistical inference and pervasive false-positive risks when performing Genome-Wide Association Studies (GWAS) using Machine Learning (ML)-imputed phenotypes. It introduces a novel statistical framework called Post-Prediction GWAS (POP-GWAS) to resolve this issue and enable valid and powerful inference.\n\nThe Problem of ML-Assisted GWAS: ML-assisted GWAS involves using sophisticated ML techniques (like deep learning or penalized regression) to predict or impute a complex phenotype (e.g., from image data or electronic health records) and then running a GWAS on the predicted phenotype. The study demonstrates that this procedure introduces a systematic bias, leading to an inflated risk of false-positive associations and consequently, invalid p-values, irrespective of the quality of the ML imputation model.\nThe Solution: POP-GWAS Framework: POP-GWAS is a statistical framework designed to perform GWAS directly on ML-imputed outcomes while maintaining valid statistical inference.\n\nMechanism: The framework redesigns the GWAS association test by explicitly accounting for the covariance structure between the true phenotype, the imputed phenotype, and the genetic markers. It requires only summary statistics from the original GWAS (on the imputed outcome) and knowledge of the prediction model (or its cross-validation performance) as input.\n\nEmpirical Validation and Novel Loci: The study validated POP-GWAS using simulations and applied it to a GWAS of bone mineral density (BMD) derived from dual-energy X-ray absorptiometry (DXA) imaging, which is a common ML-assisted approach.\n\nNew Discoveries: POP-GWAS successfully identified 89 new loci for BMD at 14 skeletal sites, many of which displayed skeletal site-specific genetic architecture.\nBias Correction: It demonstrated that the standard (naive) ML-assisted GWAS p-values were significantly inflated, whereas POP-GWAS restored valid p-values and statistical control.\n\nGeneral Applicability: The POP-GWAS framework is agnostic to the specific ML imputation algorithm used, meaning it can be applied universally across different ML techniques (e.g., deep learning, random forest, linear models) and different GWAS settings.",
    "crumbs": [
      "genetics",
      "Papers",
      "Valid inference for machine learning-assisted genome-wide association studies"
    ]
  },
  {
    "objectID": "genetics/miao_2024_39349818.html#study-design-and-methods",
    "href": "genetics/miao_2024_39349818.html#study-design-and-methods",
    "title": "Valid inference for machine learning-assisted genome-wide association studies",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\n\nStudy Design\nThis was a methodological study focused on statistical genetics and machine learning. The authors used simulation studies and real-world data application to demonstrate the bias in naive ML-assisted GWAS and validate the performance of the proposed POP-GWAS framework.\n\n\nMethodology: POP-GWAS\n\nImputed Outcome (\\(\\hat{Y}\\)): The GWAS is initially performed on the ML-imputed phenotype (\\(\\hat{Y}\\)) derived from the original predictors (\\(X\\)).\nStandard GWAS Bias: The authors mathematically derived the source of the bias in the standard GWAS association test when using \\(\\hat{Y}\\) instead of the true phenotype \\(Y\\).\nPOP-GWAS Test Statistic: POP-GWAS introduces a new, corrected test statistic that leverages the properties of the imputed phenotype. This correction factor is derived from the summary statistics of the standard GWAS and the phenotype prediction variance (a measure of the imputation quality, often estimated via cross-validation). \\[Z_{\\text{POP-GWAS}} = \\frac{Z_{\\text{naive}}}{\\sqrt{\\text{Correction Factor}}}\\] This correction ensures that the variance of the test statistic is correctly estimated, thus producing valid p-values.\n\n\n\nData Application\n\nBMD GWAS: The framework was applied to a large-scale GWAS of Bone Mineral Density (BMD), where BMD phenotypes were often derived from complex imaging data using ML-based prediction methods. The results highlighted the importance of site-specific analysis, finding different causal genes at different skeletal sites.",
    "crumbs": [
      "genetics",
      "Papers",
      "Valid inference for machine learning-assisted genome-wide association studies"
    ]
  },
  {
    "objectID": "genetics/miao_2024_39349818.html#conclusions-and-recommendations",
    "href": "genetics/miao_2024_39349818.html#conclusions-and-recommendations",
    "title": "Valid inference for machine learning-assisted genome-wide association studies",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study provides a critical warning regarding the statistical validity of naive ML-assisted GWAS and offers a concrete, generalized solution through the POP-GWAS framework.\n\nEssential for ML-GWAS: The authors strongly recommend that all GWAS conducted on ML-imputed phenotypes should utilize the POP-GWAS framework to ensure valid statistical inference and prevent the proliferation of false-positive genetic associations.\nEnabling Discovery: By correcting for bias and maintaining statistical power, POP-GWAS is poised to accelerate genetic discovery in fields that rely heavily on automated phenotyping, such as medical imaging and electronic health records.\nFuture Work: The authors suggest future developments could extend POP-GWAS to handle complex designs, such as family-based studies or Mendelian randomization using imputed phenotypes.",
    "crumbs": [
      "genetics",
      "Papers",
      "Valid inference for machine learning-assisted genome-wide association studies"
    ]
  },
  {
    "objectID": "genetics/ren_2023_37181332.html",
    "href": "genetics/ren_2023_37181332.html",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "",
    "text": "PubMed: 37181332\nDOI: 10.1016/j.xhgg.2023.100197\nOverview generated by: Claude Sonnet 4.5, 26/11/2025",
    "crumbs": [
      "genetics",
      "Papers",
      "Using GWAS summary data to impute traits for genotyped individuals"
    ]
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#key-findings",
    "href": "genetics/ren_2023_37181332.html#key-findings",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Key Findings",
    "text": "Key Findings\nThis study introduces LS-imputation, a nonparametric method that uses GWAS summary statistics combined with individual-level genotypes to impute trait values, enabling nonlinear SNP-trait association analyses and machine learning applications that are impossible with summary statistics alone.\n\nMain Discoveries\n\nNovel imputation approach: First method to recover genetic components of traits from GWAS summary data for nonlinear association analysis\nPerfect recovery property: When test genotypes match training genotypes (X = X*), the method perfectly recovers (centered) trait values, capturing nonlinear SNP-trait information despite using only linear marginal associations\nSuperior performance for association analysis: LS-imputation outperforms state-of-the-art PRS method (PRS-CS) for subsequent association analyses under non-additive models and SNP-SNP interaction detection\nEnables new analyses: Makes possible three applications currently impossible with GWAS summary data: non-additive genetic models, SNP-SNP interaction detection, and nonlinear prediction models",
    "crumbs": [
      "genetics",
      "Papers",
      "Using GWAS summary data to impute traits for genotyped individuals"
    ]
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#study-design",
    "href": "genetics/ren_2023_37181332.html#study-design",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Study Design",
    "text": "Study Design\n\nCore Problem\nGWAS summary statistics measure only linear marginal SNP-trait associations, limiting their use to linear analyses. The method addresses: How to use summary data for nonlinear SNP-trait analyses?\n\n\nGenetic Model\nAssumes unspecified functional form: \\[y = E(y|x) + \\varepsilon = g(x) + \\varepsilon\\]\nwhere: - \\(g(x)\\) is the unknown genetic component (possibly nonlinear) - \\(\\varepsilon\\) captures environmental effects and noise - No parametric assumptions on \\(g(\\cdot)\\)\n\n\nMethod Overview\nInput: - GWAS summary data: \\(\\{(\\hat{\\beta}_j^*, s_j^*): j=1,\\ldots,p\\}\\) from training data \\((X^*, Y^*)\\) - Test genotype matrix: \\(X\\) (\\(n_2 \\times p\\))\nOutput: Imputed trait values \\(\\hat{Y}\\) for test individuals\nKey insight: With large samples, \\(\\hat{\\beta}^* \\approx \\hat{\\beta}\\) (both estimate same true \\(\\beta\\)), which can be used to formulate least-squares problem.",
    "crumbs": [
      "genetics",
      "Papers",
      "Using GWAS summary data to impute traits for genotyped individuals"
    ]
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#the-ls-imputation-method",
    "href": "genetics/ren_2023_37181332.html#the-ls-imputation-method",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "The LS-Imputation Method",
    "text": "The LS-Imputation Method\n\nFormulation\nIf \\(Y\\) were available, marginal association estimates would be: \\[\\hat{\\beta} = \\frac{1}{n_2-1}X'Y\\]\nSince \\(\\hat{\\beta}^*\\) (from training) \\(\\approx \\hat{\\beta}\\) (from test), solve:\n\\[\\hat{Y} = \\arg\\min_Y \\|\\hat{\\beta}^* - \\frac{1}{n_2-1}X'Y\\|^2\\]\nSolution: \\[\\hat{Y} = (n_2-1)(XX')^+X\\hat{\\beta}^*\\]\nwhere \\((XX')^+\\) is the Moore-Penrose generalized inverse (due to centering of SNPs).\n\n\nRegularized Implementation\nFor computational stability, use ridge regularization: \\[\\hat{Y}(\\lambda) = (n_2-1)(XX' + \\lambda I)^{-1}X\\hat{\\beta}^*\\]\nDefault: \\(\\lambda = 10^{-6}\\) (computationally fast and stable)\n\n\nBatch Processing\nFor large \\(n_2\\): - Divide test data into batches of size \\(m\\) - Apply method to each batch separately - Requires \\(p &gt; m\\) (preferably both \\(n_1\\) and \\(p\\) large) - Choose \\(m\\) giving marginal association results similar to training data",
    "crumbs": [
      "genetics",
      "Papers",
      "Using GWAS summary data to impute traits for genotyped individuals"
    ]
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#uk-biobank-application",
    "href": "genetics/ren_2023_37181332.html#uk-biobank-application",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "UK Biobank Application",
    "text": "UK Biobank Application\n\nDataset\n\nTrait: HDL cholesterol\nTotal individuals: 356,351 (White British ancestry)\nSNPs: 715,783 (after QC: MAF&gt;0.05, missing&lt;10%, HWE p&gt;0.001, LD pruning r²&lt;0.8)\nSplit:\n\nTraining: \\(n_1 = 178,175\\)\nTest: \\(n_2 = 178,176\\)\n\nImplementation: 50,000 SNPs (p&lt;0.05 in training), 9 batches (8×20K + 1×18K individuals)\n\n\n\nPerfect Recovery Test\nWhen \\(X = X^*\\) (same genotypes as training): - LS-imputation: Correlation with true values = 0.999+ - PRS-CS: Correlation &lt; 0.5 (imperfect recovery)\nDemonstrates unique property: LS-imputation can perfectly recover trait values for training genotypes, capturing nonlinear information.\n\n\nTest Data Imputation Performance\nCorrelation between observed and imputed HDL:\n\n\n\nMethod\nCorrelation (unadjusted)\nCorrelation (adjusted*)\n\n\n\n\nLS-imputation\n0.177\n0.204\n\n\nPRS-CS\n0.279\n0.313\n\n\n\n*Adjusted for sex and age\nInterpretation: PRS-CS shows higher correlation (expected, as linear effects dominate heritability), but LS-imputation better preserves information for association analyses.",
    "crumbs": [
      "genetics",
      "Papers",
      "Using GWAS summary data to impute traits for genotyped individuals"
    ]
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#application-i-non-additive-genetic-models",
    "href": "genetics/ren_2023_37181332.html#application-i-non-additive-genetic-models",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Application I: Non-Additive Genetic Models",
    "text": "Application I: Non-Additive Genetic Models\n\nAdditive Model Results\nComparison of significant SNPs at genome-wide threshold (\\(5\\times10^{-8}\\)):\n\n\n\n\n\n\n\n\nAnalysis\nApproach\nPerformance\n\n\n\n\nTraining (observed)\nStandard GWAS\nBaseline\n\n\nTest (observed)\nStandard GWAS\nSimilar to training\n\n\nTest (LS-imputed)\nOur method\nSimilar to observed, slightly conservative\n\n\nTest (PRS-CS-imputed)\nPRS method\nWay too many significant SNPs\n\n\n\nManhattan plot patterns: LS-imputation closely matched observed data distribution, while PRS-CS identified excessive associations (any SNP in PRS model or LD with them becomes significant).\n\n\nRecessive Model Results\nTesting SNPs under recessive genetic model:\nLS-imputation: - Distribution of significant SNPs similar to observed - Slightly more conservative (fewer false positives) - Effect size estimates highly correlated with true estimates\nPRS-CS: - Severe inflation of significant associations - Not suitable for non-additive model testing\n\n\nDominant Model Results\nSimilar pattern observed (Supplementary results): - LS-imputation: Good agreement with observed - PRS-CS: Excessive false positives\n\n\nQuantitative Comparison\nEffect size correlations (50,000 SNPs):\n\n\n\nModel\nLS vs. Observed\nPRS-CS vs. Observed\n\n\n\n\nAdditive\n0.90+\n0.40-0.60\n\n\nRecessive\n0.85+\n0.30-0.50\n\n\n\nConclusion: LS-imputation preserves information needed for non-additive model testing; PRS-CS does not.",
    "crumbs": [
      "genetics",
      "Papers",
      "Using GWAS summary data to impute traits for genotyped individuals"
    ]
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#application-ii-snp-snp-interaction-detection",
    "href": "genetics/ren_2023_37181332.html#application-ii-snp-snp-interaction-detection",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Application II: SNP-SNP Interaction Detection",
    "text": "Application II: SNP-SNP Interaction Detection\n\nAnalysis Strategy\n\nIdentified 1,758 marginally significant SNPs (p&lt;10⁻⁶) in training data\nRemoved high-LD SNPs (r²&gt;0.99) → 1,652 SNPs\nTested all pairwise interactions: \\(\\binom{1652}{2} = 1,364,026\\) tests\n\nModel for each pair: \\[Y_i = \\alpha_0 + \\text{SNP}_{1i} \\times \\alpha_1 + \\text{SNP}_{2i} \\times \\alpha_2 + \\text{SNP}_{1i} \\times \\text{SNP}_{2i} \\times \\alpha_{12} + \\varepsilon_i\\]\nTest: \\(H_0: \\alpha_{12} = 0\\) (Wald test)\nSignificance threshold: \\(2.5 \\times 10^{-8}\\) (Bonferroni correction)\n\n\nResults\nInteraction effect estimates:\n\n\n\nComparison\nCorrelation\n\n\n\n\nTraining (observed) vs. Test (observed)\nBaseline\n\n\nTest (observed) vs. Test (LS-imputed)\n0.95+\n\n\nTest (observed) vs. Test (PRS-CS-imputed)\n0.60-0.70\n\n\n\nP-value correlations: Similar pattern, with LS-imputation showing strong agreement with observed data.\n\n\nSignificant Interactions Identified\nSNP-SNP pairs (Bonferroni p&lt;\\(2.5\\times10^{-8}\\)):\n\n\n\nDataset\nSignificant pairs\nAgreement with observed\n\n\n\n\nTraining (observed)\nBaseline\n-\n\n\nTest (observed)\nSimilar\nReference\n\n\nTest (LS-imputed)\nHigh overlap\n85-90%\n\n\nTest (PRS-CS-imputed)\nModerate overlap\n60-70%\n\n\n\nLocus-locus interactions: Defined using 1,703 independent LD blocks - LS-imputation: High concordance with observed - Differences between LS-imputed and observed ≤ differences between training and test (both observed)\nConclusion: LS-imputation successfully detects SNP-SNP interactions; PRS-CS less suitable.",
    "crumbs": [
      "genetics",
      "Papers",
      "Using GWAS summary data to impute traits for genotyped individuals"
    ]
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#application-iii-nonlinear-trait-prediction",
    "href": "genetics/ren_2023_37181332.html#application-iii-nonlinear-trait-prediction",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Application III: Nonlinear Trait Prediction",
    "text": "Application III: Nonlinear Trait Prediction\n\nRandom Forest Setup\nTraining: 70% of test data (random subset) Validation: Remaining 30% Features: 1,652 marginally significant SNPs\nGoal: Compare RF predictions using observed vs. imputed traits for training\n\n\nResults\nCorrelation of RF predictions on validation data:\n\n\n\nTraining data\nCorrelation with true trait\n\n\n\n\nObserved traits\nBaseline\n\n\nLS-imputed traits\n0.722\n\n\nPRS-CS-imputed traits\n0.658\n\n\n\nInterpretation: LS-imputed traits retain more information about SNP-trait associations (possibly nonlinear) than PRS-CS-imputed traits.\nWhy LS-imputation performs better: - Captures nonlinear relationships in training data - No parametric model assumptions - Information borrowing across similar individuals",
    "crumbs": [
      "genetics",
      "Papers",
      "Using GWAS summary data to impute traits for genotyped individuals"
    ]
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#statistical-properties",
    "href": "genetics/ren_2023_37181332.html#statistical-properties",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Statistical Properties",
    "text": "Statistical Properties\n\nInformation Content\nFor test genotypes \\(X\\), imputed trait is: \\[\\hat{Y} \\approx \\frac{n_2-1}{n_1-1}(XX'/p)C_{n_1}Y^*\\]\nwhere: - \\(XX'/p\\) measures genotypic similarities - \\(C_{n_1} = I - 11'/n_1\\) is centering matrix - \\(Y^*\\) are training trait values\nImplication: Imputed trait is weighted average of training traits, weights determined by genotypic similarity.\n\n\nSpecial Case: Perfect Recovery\nWhen \\(X = X^*\\): \\[\\hat{Y} \\rightarrow^P C_{n_1}Y^*\\]\nAs \\(p \\rightarrow \\infty\\), imputed values converge to centered training trait values, which contain nonlinear SNP-trait information.\n\n\nVariance Properties\nFor imputed trait: \\[\\text{Var}(\\hat{Y}) = (n_2-1)^2(XX')^+X\\text{Var}(\\hat{\\beta}^*)X'(XX')^+\\]\nKey points: - Elements of \\(\\hat{Y}\\) are correlated (not iid) - Variances unequal across individuals - Practical solution: Treat as independent in subsequent analyses (simplification) - Choose appropriate batch size \\(m\\) to minimize bias-variance trade-off\n\n\nAsymptotic Behavior\nWith iid normal X (simplified case):\nSmall \\(n_2\\) (fixed): \\[\\text{Var}(\\hat{Y}_j) \\approx n_2\\tau^2/p\\]\nLarge \\(n_2\\) (with \\(n_2/p \\rightarrow c \\in (0,1)\\)): \\[\\text{Var}(\\hat{Y}_j) = n_2 O(1)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)\\tau^2\\]\nRecommendation: Use smaller \\(n_2\\) (or batch size \\(m\\)) for smaller variances.",
    "crumbs": [
      "genetics",
      "Papers",
      "Using GWAS summary data to impute traits for genotyped individuals"
    ]
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#comparison-ls-imputation-vs.-prs-cs",
    "href": "genetics/ren_2023_37181332.html#comparison-ls-imputation-vs.-prs-cs",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Comparison: LS-Imputation vs. PRS-CS",
    "text": "Comparison: LS-Imputation vs. PRS-CS\n\nPrediction Performance\nTrait value correlation (higher is better for prediction): - PRS-CS &gt; LS-imputation - Expected: Linear effects dominate heritability - PRS-CS optimized for prediction\n\n\nAssociation Analysis Performance\nEffect size estimation (for subsequent GWAS): - LS-imputation &gt;&gt; PRS-CS - Critical for non-additive models - Essential for interaction detection\n\n\nWhy PRS Methods Fail for Association Analysis\nPRS-CS assumptions: \\[Y = X\\beta + \\varepsilon\\] \\[\\beta \\sim \\text{ContinuousShrinkage}(\\text{prior})\\]\nProblems: 1. Assumes linear model with specific SNPs 2. Imputed traits reflect estimated linear effects only 3. Any SNP in model (or LD with them) will be “significant” by definition 4. Not suitable for testing associations\n\n\nFundamental Difference\n\n\n\nFeature\nLS-imputation\nPRS-CS\n\n\n\n\nModel\nNonparametric\nParametric linear\n\n\nCaptures nonlinearity\nYes\nNo\n\n\nPerfect recovery\nYes\nNo\n\n\nPrediction\nGood\nBetter\n\n\nAssociation analysis\nBest\nPoor\n\n\nInteraction detection\nBest\nPoor",
    "crumbs": [
      "genetics",
      "Papers",
      "Using GWAS summary data to impute traits for genotyped individuals"
    ]
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#implementation-details",
    "href": "genetics/ren_2023_37181332.html#implementation-details",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Implementation Details",
    "text": "Implementation Details\n\nComputational Considerations\nRequirements: - \\(p &gt; n_2\\) (or \\(p &gt; m\\) if batches used) - More constraints than unknowns - Unique solution (up to centering)\nMatrix inversion: - Used linalg.inv from Python numpy - Default \\(\\lambda = 10^{-6}\\) for regularization - Fast and stable computation\nMemory management: - Batch processing for large \\(n_2\\) - Typical batch: \\(m = 20,000\\) individuals - Trade-off: smaller \\(m\\) → smaller variance, but information loss between batches\n\n\nParameter Selection\nSNP number (\\(p\\)): - Larger is better (more constraints) - Example: Used 50,000 SNPs (p&lt;0.05 in training) - Can use all available (memory permitting)\nTraining sample (\\(n_1\\)): - Larger is better (more accurate \\(\\hat{\\beta}^*\\)) - Example: 178,175 individuals\nBatch size (\\(m\\)): - Choose to give marginal results similar to training - Not too large (information loss between batches) - Not too small (computational inefficiency) - Example: 20,000 individuals per batch\n\n\nQuality Control Strategy\nRecommended: Choose \\(m\\) where imputed trait gives: 1. Marginal effect estimates ≈ training estimates 2. Standard errors ≈ training SEs (after rescaling)\nSE rescaling: \\(\\sqrt{n_2/n_1} \\times SE_{test}\\) for comparison",
    "crumbs": [
      "genetics",
      "Papers",
      "Using GWAS summary data to impute traits for genotyped individuals"
    ]
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#extensions-and-variations",
    "href": "genetics/ren_2023_37181332.html#extensions-and-variations",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Extensions and Variations",
    "text": "Extensions and Variations\n\nBinary Traits\nMethod extended to binary outcomes (Supplementary): - Similar formulation - Logistic regression framework - Applied to UK Biobank hypertension data - Promising preliminary results\n\n\nWeighted Least Squares\nAlternative to ordinary least squares: \\[\\hat{Y}_{WLS} = \\arg\\min_Y (\\hat{\\beta}^* - \\frac{1}{n_2-1}X'Y)'W(\\hat{\\beta}^* - \\frac{1}{n_2-1}X'Y)\\]\nwhere \\(W\\) = diagonal matrix with weights \\(\\propto 1/\\text{Var}(\\hat{\\beta}_j^*)\\)\nResult: Similar to OLS (Supplementary), not pursued further.\n\n\nIntercept Known Case\nIf intercept \\(\\alpha_0\\) available for each SNP: - No centering needed - \\(X\\) is full rank - \\((XX')^+ = (XX')^{-1}\\) - Simpler interpretation\n\n\nSample Size Sensitivity\nTraining sample \\(n_1\\): Larger always better SNP number \\(p\\): Larger always better Test sample \\(n_2\\): Results stable for \\(n_2 \\geq 25,000\\) Batch size \\(m\\): Complex trade-off, choose empirically",
    "crumbs": [
      "genetics",
      "Papers",
      "Using GWAS summary data to impute traits for genotyped individuals"
    ]
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#practical-applications",
    "href": "genetics/ren_2023_37181332.html#practical-applications",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Practical Applications",
    "text": "Practical Applications\n\nUse Case 1: Augmenting Incomplete Data\nScenario: Biobank with genotypes but missing trait - Late-onset disease (e.g., Alzheimer’s) not yet manifested - Expensive/difficult-to-measure phenotype - Large GWAS summary data available externally\nSolution: Impute trait values to augment analyses\n\n\nUse Case 2: Cross-Study Integration\nScenario: Multiple related studies - Different traits measured - Want to analyze trait not measured in focal study - GWAS summary available from other studies\nSolution: Use summary data to impute unmeasured traits\n\n\nUse Case 3: Privacy-Preserving Collaboration\nScenario: Private breeding programs or clinical cohorts - Cannot share individual-level data - Can share summary statistics - Want to conduct joint analyses\nSolution: Each site imputes traits using others’ summaries\n\n\nUse Case 4: Nonlinear Model Development\nScenario: Develop complex prediction models - Neural networks, deep learning, gradient boosting - Require individual-level data - Only summary data available\nSolution: Impute traits to enable nonlinear model training",
    "crumbs": [
      "genetics",
      "Papers",
      "Using GWAS summary data to impute traits for genotyped individuals"
    ]
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#limitations-and-considerations",
    "href": "genetics/ren_2023_37181332.html#limitations-and-considerations",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Limitations and Considerations",
    "text": "Limitations and Considerations\n\nCurrent Limitations\n\nVariance structure: Elements of \\(\\hat{Y}\\) are correlated and have unequal variances\n\nCurrently ignored in subsequent analyses\nMay cause slight over/under-estimation of SEs\nAccounting for correlations computationally prohibitive\n\nCentering effects: Each batch centered at mean 0\n\nInformation loss between batches (relative levels)\nMitigated by using larger batches\nTrade-off with variance considerations\n\nConstraint requirement: Needs \\(p &gt; m\\)\n\nMust have more SNPs than individuals per batch\nMay limit applicability in some scenarios\n\nRare variant use: Current implementation uses common variants only\n\nRare variants have lower genotyping quality\nExpected to contain less heritability information\nCould be explored with sequencing data\n\n\n\n\nStatistical Assumptions\n\nSame population: Training and test from same population\nUnrelated individuals: No close relatives in test data\nWhite noise errors: \\(\\varepsilon\\) independent of genotypes\nLarge samples: Asymptotic properties require large \\(n_1\\), \\(p\\)\n\n\n\nInterpretation Caveats\nImputed values: - Represent genetic components only - Do not capture environmental variation - Centered (mean 0 within each batch) - Should not be treated as observed phenotypes in all contexts\nSubsequent analyses: - Slightly conservative p-values (good for Type I error control) - Effect size estimates unbiased - Power may be slightly reduced vs. observed data",
    "crumbs": [
      "genetics",
      "Papers",
      "Using GWAS summary data to impute traits for genotyped individuals"
    ]
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#comparison-to-alternative-approaches",
    "href": "genetics/ren_2023_37181332.html#comparison-to-alternative-approaches",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Comparison to Alternative Approaches",
    "text": "Comparison to Alternative Approaches\n\nvs. PRS Methods\nAll existing PRS methods for summary data: - Assume linear models - Optimize for prediction - Not designed for association analysis - Cannot detect nonlinear effects\nLS-imputation: - Nonparametric - Optimizes for association analysis - Can detect nonlinear effects - Less optimal for pure prediction\n\n\nvs. Multi-Trait Imputation\nPrevious methods (Dahl et al., Hormozdiari et al.): - Impute focal trait using other measured traits - Problem: Any variants associated with imputation traits will appear associated with focal trait - Loss of specificity\nLS-imputation: - Uses only genotypes and summary data for focal trait - Maintains specificity to focal trait - Suitable for association analysis\n\n\nvs. Direct GWAS Summary Analysis\nStandard approach with summary data: - Can only test linear marginal associations - Cannot detect non-additive effects - Cannot detect interactions - Cannot use nonlinear prediction models\nLS-imputation approach: - Enables all of the above - More flexible for exploratory analysis - Can combine with machine learning methods",
    "crumbs": [
      "genetics",
      "Papers",
      "Using GWAS summary data to impute traits for genotyped individuals"
    ]
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#future-directions",
    "href": "genetics/ren_2023_37181332.html#future-directions",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Future Directions",
    "text": "Future Directions\n\nMethodological Extensions\n\nEfficient algorithms: Handle larger datasets without batching\nGeneralized least squares: Account for correlated marginal estimates\nBetter variance estimation: Properly handle correlated imputed values\nRare variant integration: Extend to sequencing-based data\n\n\n\nAdditional Applications\n\nMulti-trait analysis: Jointly impute multiple correlated traits\nTranscriptome-wide studies: Impute gene expression for TWAS\nPolygenic score development: Use imputed traits to train complex nonlinear PRS models\nPathway analysis: Enable pathway-level nonlinear analyses\n\n\n\nPractical Improvements\n\nAutomated parameter selection: Data-driven choice of \\(m\\), \\(p\\)\nDistributed computing: Parallel batch processing\nMemory optimization: More efficient matrix operations\nQuality metrics: Better diagnostics for imputation quality",
    "crumbs": [
      "genetics",
      "Papers",
      "Using GWAS summary data to impute traits for genotyped individuals"
    ]
  },
  {
    "objectID": "genetics/ren_2023_37181332.html#related-concepts",
    "href": "genetics/ren_2023_37181332.html#related-concepts",
    "title": "Using GWAS summary data to impute traits for genotyped individuals",
    "section": "Related Concepts",
    "text": "Related Concepts\n\nMoore-Penrose inverse: Generalized inverse for non-full-rank matrices\nCentering matrix: \\(C_n = I - 11'/n\\) projects to zero-mean subspace\nGenotypic similarity: Measured by \\(XX'/p\\) (normalized dot products)\nInformation borrowing: Imputed value uses data from similar individuals\nLeast squares: Optimization framework minimizing squared residuals\nRegularization: Adding penalty (ridge) for numerical stability\nBatch processing: Dividing large datasets for computational efficiency",
    "crumbs": [
      "genetics",
      "Papers",
      "Using GWAS summary data to impute traits for genotyped individuals"
    ]
  },
  {
    "objectID": "genetics/schwanhausser_2011_21593866.html",
    "href": "genetics/schwanhausser_2011_21593866.html",
    "title": "Global quantification of mammalian gene expression control",
    "section": "",
    "text": "PubMed: 21593866 DOI: 10.1038/nature10098 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "genetics",
      "Papers",
      "Global quantification of mammalian gene expression control"
    ]
  },
  {
    "objectID": "genetics/schwanhausser_2011_21593866.html#key-findings-translation-is-the-primary-control-point-of-protein-abundance",
    "href": "genetics/schwanhausser_2011_21593866.html#key-findings-translation-is-the-primary-control-point-of-protein-abundance",
    "title": "Global quantification of mammalian gene expression control",
    "section": "Key Findings: Translation is the Primary Control Point of Protein Abundance",
    "text": "Key Findings: Translation is the Primary Control Point of Protein Abundance\nThis seminal study provides the first genome-scale, absolute quantification of the entire gene expression cascade—including mRNA abundance, protein abundance, and their respective synthesis and degradation rates—in a mammalian cell line (NIH3T3 fibroblasts).\nThe main conclusion fundamentally changes the view of gene regulation: the cellular abundance of proteins is predominantly controlled at the level of translation, not transcription.\n\nQuantification Results\n\nmRNA vs. Protein Abundance: The study found a better-than-expected correlation between mRNA and protein levels across the genome, with a correlation coefficient (\\(R\\)) of approximately 0.73. This is higher than previously estimated.\nAbundance vs. Turnover: There was no correlation observed between the half-lives (turnover rates) of corresponding mRNAs and proteins. This means a stable mRNA does not necessarily encode a stable protein, and vice versa.\nSynthesis Rates: The quantitative model allowed for the prediction of synthesis rates for over 5,000 genes. This revealed that the large variation in protein abundance is primarily achieved through highly variable translational efficiency (the number of protein molecules produced per mRNA molecule) rather than changes in mRNA levels.",
    "crumbs": [
      "genetics",
      "Papers",
      "Global quantification of mammalian gene expression control"
    ]
  },
  {
    "objectID": "genetics/schwanhausser_2011_21593866.html#methods-parallel-metabolic-pulse-labeling-and-absolute-quantification",
    "href": "genetics/schwanhausser_2011_21593866.html#methods-parallel-metabolic-pulse-labeling-and-absolute-quantification",
    "title": "Global quantification of mammalian gene expression control",
    "section": "Methods: Parallel Metabolic Pulse Labeling and Absolute Quantification",
    "text": "Methods: Parallel Metabolic Pulse Labeling and Absolute Quantification\nThe study developed and utilized a quantitative approach involving parallel metabolic pulse labeling and mass spectrometry.\n\nExperimental Design\n\nStable Isotope Labeling: NIH3T3 cells were cultured with heavy amino acids (for protein labeling) and heavy nucleosides (for mRNA labeling) for a defined period (pulse labeling).\nParallel Measurement:\n\nProteomics: Liquid chromatography and tandem mass spectrometry (LC-MS/MS) were used to measure the absolute number of protein molecules and their turnover (half-lives).\nTranscriptomics: Microarrays were used to measure the absolute number of mRNA molecules and their turnover (half-lives).\n\n\n\n\nQuantitative Modeling\nThe absolute measurements of abundance and turnover were integrated into a quantitative model that mathematically links the four fundamental processes of gene expression:\n\\[\n\\text{Protein Abundance} \\propto \\frac{\\text{mRNA Abundance} \\times \\text{Translation Rate}}{\\text{Protein Degradation Rate}}\n\\]\nThis model allowed the derivation of previously unknown parameters: the mRNA synthesis rate (transcription rate) and the protein synthesis rate (translation rate).",
    "crumbs": [
      "genetics",
      "Papers",
      "Global quantification of mammalian gene expression control"
    ]
  },
  {
    "objectID": "genetics/schwanhausser_2011_21593866.html#implications-the-design-principles-of-gene-expression",
    "href": "genetics/schwanhausser_2011_21593866.html#implications-the-design-principles-of-gene-expression",
    "title": "Global quantification of mammalian gene expression control",
    "section": "Implications: The Design Principles of Gene Expression",
    "text": "Implications: The Design Principles of Gene Expression\nThe quantitative data supports a model where cells use degradation/turnover rates to fine-tune the functional properties of proteins.\n\nStability and Function\n\nUnstable Components: Highly abundant components of essential molecular machinery (e.g., ribosomes) were found to be both abundant and stable (long half-lives), which minimizes the energetic cost of replacement.\nRegulatory/Signaling Components: Proteins with key regulatory or signalling functions (e.g., transcription factors, cell-cycle regulators) tend to have short half-lives and are often less abundant. This allows the cell to achieve rapid and dynamic changes in response to environmental cues.\n\n\n\nControl Mechanism Summary\n\nAbundance Control: Protein steady-state abundance is mostly controlled by translation rates.\nKinetic Control: Protein and mRNA turnover rates (stability) control the time scale over which the protein or mRNA abundance can respond to changes in their synthesis rates.",
    "crumbs": [
      "genetics",
      "Papers",
      "Global quantification of mammalian gene expression control"
    ]
  },
  {
    "objectID": "genetics/schwanhausser_2011_21593866.html#conclusions",
    "href": "genetics/schwanhausser_2011_21593866.html#conclusions",
    "title": "Global quantification of mammalian gene expression control",
    "section": "Conclusions",
    "text": "Conclusions\nThis study provides an unprecedented quantitative atlas of gene expression in a mammalian cell. The discovery that translational control is the dominant mechanism for determining steady-state protein levels establishes a critical point of regulation in the central dogma of biology and provides a foundational resource for systems biology and computational modeling.",
    "crumbs": [
      "genetics",
      "Papers",
      "Global quantification of mammalian gene expression control"
    ]
  },
  {
    "objectID": "genetics/ota_2025_41372418.html",
    "href": "genetics/ota_2025_41372418.html",
    "title": "Causal modelling of gene effects from regulators to programs to traits",
    "section": "",
    "text": "PubMed: 41372418 DOI: 10.1038/s41586-025-09866-3 Overview generated by: Gemini 2.5 Flash, 10/12/2025",
    "crumbs": [
      "genetics",
      "Papers",
      "Causal modelling of gene effects from regulators to programs to traits"
    ]
  },
  {
    "objectID": "genetics/ota_2025_41372418.html#research-goal-and-causal-framework",
    "href": "genetics/ota_2025_41372418.html#research-goal-and-causal-framework",
    "title": "Causal modelling of gene effects from regulators to programs to traits",
    "section": "Research Goal and Causal Framework",
    "text": "Research Goal and Causal Framework\nThis paper addresses a critical gap in human genetics: the lack of genome-scale approaches to infer causal mechanistic pathways linking genes to cellular functions and ultimately to complex human traits. The authors propose a novel framework that bridges this gap by combining quantitative genetic association data with regulatory information from cellular perturbation experiments.\n\nThe Proposed Causal Model\nThe core of the study is the construction of a causal graph that models the directional associations of genes with a trait. The model posits a three-step causal hierarchy:\n\\[ \\text{Gene} \\longrightarrow \\text{Regulatory Programs} \\longrightarrow \\text{Trait} \\]\nThe approach uses two main data inputs: 1. Gene-Trait Relationships (\\(\\gamma\\)): Quantitative estimates of gene effects on traits, derived from Loss-of-Function (LoF) burden tests in large-scale genetic studies (GWAS). 2. Gene-Regulatory Relationships (\\(\\beta\\)): Quantitative estimates of a gene’s regulatory effects on cellular programs, inferred from high-throughput cellular perturbation assays, such as Perturb-seq experiments in relevant cell types.\nThe model combines these two forms of data using a statistical framework to infer the causal path, allowing gene-trait associations to be explained by regulatory effects on intermediate biological programs or by direct effects on the trait.",
    "crumbs": [
      "genetics",
      "Papers",
      "Causal modelling of gene effects from regulators to programs to traits"
    ]
  },
  {
    "objectID": "genetics/ota_2025_41372418.html#proof-of-concept-blood-trait-analysis",
    "href": "genetics/ota_2025_41372418.html#proof-of-concept-blood-trait-analysis",
    "title": "Causal modelling of gene effects from regulators to programs to traits",
    "section": "Proof of Concept: Blood Trait Analysis",
    "text": "Proof of Concept: Blood Trait Analysis\nThe authors applied this causal framework as a proof of concept to jointly model three partially co-regulated blood traits: Red Blood Cell Distribution Width (RDW), Mean Corpuscular Hemoglobin (MCH), and Immature Reticulocyte Fraction (IRF).\n\nMethods and Data Integration\n\nPerturb-seq Data: Used pooled Perturb-seq experiments (CRISPR-based perturbation combined with single-cell RNA sequencing) in human hematopoietic stem and progenitor cells (HSPCs), which are the trait-relevant cell type, to generate a comprehensive map of gene-regulatory connections (\\(\\beta\\)).\nGWAS Data: Used publicly available GWAS data to calculate gene-level effect sizes (\\(\\gamma\\)) for LoF burden on the three blood traits.\nCausal Graph Construction: The combined data were used to construct a causal graph of the gene-regulatory hierarchy underlying the traits.\n\n\n\nKey Results\n\nIdentifying Regulatory Programs: The model successfully identified regulatory programs (groups of co-regulated genes) that serve as intermediate causal steps. For instance, the model identified a program involving ribosomal genes that causally links gene perturbations to RDW and MCH.\nDirectional Causality: The framework was able to determine the directional relationships, showing how the perturbation of a regulatory gene affects a specific program, which in turn affects the trait.\nNovel Gene-Trait Links: The model provided functional validation for known associations and suggested new mechanisms. For example, the regulatory effect of \\(GATA1\\) perturbation on multiple programs was successfully linked to observed effects on all three blood traits.\nCross-Trait Comparisons: The model was used to predict cross-trait relationships of gene effects, showing how a gene’s regulatory effect can lead to correlated or inversely correlated outcomes across different traits.",
    "crumbs": [
      "genetics",
      "Papers",
      "Causal modelling of gene effects from regulators to programs to traits"
    ]
  },
  {
    "objectID": "genetics/ota_2025_41372418.html#conclusions-and-future-directions",
    "href": "genetics/ota_2025_41372418.html#conclusions-and-future-directions",
    "title": "Causal modelling of gene effects from regulators to programs to traits",
    "section": "Conclusions and Future Directions",
    "text": "Conclusions and Future Directions\nThe study demonstrates a novel and powerful strategy for leveraging cellular perturbation screens in conjunction with human genetic data to move beyond simple association and systematically infer the mechanistic chain of causality from genes to biological programs to complex traits.\nThe approach is scalable and suggests that performing Perturb-seq experiments in additional trait-relevant cell types, coupled with robust gene-level effect size estimation from GWAS, represents a critical future direction for illuminating the biological mechanisms underlying the results of large-scale genetic association studies.",
    "crumbs": [
      "genetics",
      "Papers",
      "Causal modelling of gene effects from regulators to programs to traits"
    ]
  },
  {
    "objectID": "expression/index.html",
    "href": "expression/index.html",
    "title": "gene expression",
    "section": "",
    "text": "Exploring public cancer gene expression signatures across bulk, single-cell and spatial transcriptomics data with signifinder Bioconductor package\n\n\n\nProblem: Gene expression signatures (GESs) derived from bulk RNA-Seq often show inconsistent performance when applied to single-cell (scRNA-Seq) or spatial transcriptomics (ST) data due to differences in data sparsity and resolution.\nSolution: The study introduces the signifinder Bioconductor package, a workflow that standardizes the testing and scoring of GESs across bulk, scRNA-Seq (using methods like AUCell), and ST platforms.\nFindings: Application to breast cancer signatures revealed that only a small subset were highly robust across all three modalities. The package successfully mapped expression signatures to spatial tissue regions, enabling a more precise, high-resolution analysis of the tumor microenvironment (TME).\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "expression",
      "Papers"
    ]
  },
  {
    "objectID": "proteomics/webel_2024_38926340.html",
    "href": "proteomics/webel_2024_38926340.html",
    "title": "Imputation of label-free quantitative mass spectrometry-based proteomics data using self-supervised deep learning",
    "section": "",
    "text": "PubMed: 38926340 DOI: 10.1038/s41467-024-48711-5 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "proteomics",
      "Papers",
      "Imputation of label-free quantitative mass spectrometry-based proteomics data using self-supervised deep learning"
    ]
  },
  {
    "objectID": "proteomics/webel_2024_38926340.html#key-findings-pimms-for-imputation",
    "href": "proteomics/webel_2024_38926340.html#key-findings-pimms-for-imputation",
    "title": "Imputation of label-free quantitative mass spectrometry-based proteomics data using self-supervised deep learning",
    "section": "Key Findings: PIMMS for Imputation",
    "text": "Key Findings: PIMMS for Imputation\nThe central finding is the development and validation of PIMMS (Proteomics Imputation Modeling Mass Spectrometry), a framework utilizing self-supervised deep learning (DL) models for the imputation of missing values in label-free quantification (LFQ) mass spectrometry (MS) proteomics data.\nPIMMS demonstrated superior performance compared to conventional and other machine learning methods (e.g., KNN, Random Forest, median imputation) on simulated missing values, achieving Mean Absolute Error (MAE) values between 0.54 and 0.58 on protein groups for the large HeLa dataset, significantly better than median imputation (MAE 1.24).\nCrucially, when applied to a clinical cohort: * PIMMS-VAE (Variational Autoencoder) recovered 15 out of 17 differentially abundant protein groups that were lost when comparing a reduced (80%) dataset to the full non-imputed dataset. * When analyzing the full Alcohol-related Liver Disease (ALD) dataset, PIMMS-VAE identified 30 additional proteins (+13.2%) that were significantly differentially abundant across disease stages compared to no imputation, and these proteins were found to be predictive of ALD progression.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Imputation of label-free quantitative mass spectrometry-based proteomics data using self-supervised deep learning"
    ]
  },
  {
    "objectID": "proteomics/webel_2024_38926340.html#study-design-and-datasets",
    "href": "proteomics/webel_2024_38926340.html#study-design-and-datasets",
    "title": "Imputation of label-free quantitative mass spectrometry-based proteomics data using self-supervised deep learning",
    "section": "Study Design and Datasets",
    "text": "Study Design and Datasets\nThe authors employed a dual-dataset approach for model development and application:\n\nModel Development and Evaluation\n\nData Source: A large dataset of HeLa cell line tryptic lysates consisting of 564 runs acquired over roughly two years on a single Q Executive HF-X Orbitrap. A smaller subset of 50 runs was also used to test performance dependence on sample size.\nEvaluation Strategy: Missing values were simulated as Missing Not At Random (MNAR) (25%, 50%, or 75%) using the Lazar et al. procedure to ensure lower intensities were sufficiently represented.\nComparison: The three DL models (CF, DAE, VAE) were compared against 27 other imputation approaches, including common methods like k-nearest neighbors (KNN) and random forest (RF).\n\n\n\nClinical Application (Case Study)\n\nData Source: Blood plasma proteomics data from an Alcohol-related Liver Disease (ALD) cohort involving 358 individuals.\nGoal: To assess the impact of DL imputation on downstream biological conclusions, specifically differential abundance analysis and the prediction of disease progression.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Imputation of label-free quantitative mass spectrometry-based proteomics data using self-supervised deep learning"
    ]
  },
  {
    "objectID": "proteomics/webel_2024_38926340.html#methods-self-supervised-deep-learning-models",
    "href": "proteomics/webel_2024_38926340.html#methods-self-supervised-deep-learning-models",
    "title": "Imputation of label-free quantitative mass spectrometry-based proteomics data using self-supervised deep learning",
    "section": "Methods: Self-Supervised Deep Learning Models",
    "text": "Methods: Self-Supervised Deep Learning Models\nThe PIMMS framework incorporates three self-supervised deep learning architectures to model and impute missing intensities at the precursor, aggregated peptide, or protein group level. The objective for the autoencoder-based models and collaborative filtering is reconstruction, while the VAE is a generative model.\n\nCollaborative Filtering (CF)\n\nMechanism: Assigns a trainable embedding vector to each sample and each feature (e.g., protein group). The intensity prediction is generated from the combination of these embeddings.\nLoss Function: Mean-Squared Error (MSE) reconstruction loss.\n\n\n\nDenoising Autoencoder (DAE)\n\nMechanism: An autoencoder architecture where the model learns to reconstruct the original data from a corrupted (masked) input, forcing it to learn a useful deterministic latent representation of the sample.\nLoss Function: Mean-Squared Error (MSE) reconstruction loss.\n\n\n\nVariational Autoencoder (VAE)\n\nMechanism: A generative model that encodes a stochastic latent representation, typically a high-dimensional Gaussian distribution. It adds a regularization constraint (Kullback–Leibler divergence loss) on the latent space in addition to reconstruction.\nLoss Function: A probabilistic loss that accounts for both reconstruction and the latent space constraint.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Imputation of label-free quantitative mass spectrometry-based proteomics data using self-supervised deep learning"
    ]
  },
  {
    "objectID": "proteomics/webel_2024_38926340.html#detailed-results-and-performance",
    "href": "proteomics/webel_2024_38926340.html#detailed-results-and-performance",
    "title": "Imputation of label-free quantitative mass spectrometry-based proteomics data using self-supervised deep learning",
    "section": "Detailed Results and Performance",
    "text": "Detailed Results and Performance\n\nPerformance by Feature Level and Abundance\n\nDL models were effective at all levels of MS data (precursors, peptides, and protein groups). Lower-level data (precursors) were found to be easier to learn due to less aggregation.\nThe imputation accuracy depended on the protein group’s prevalence: MAE was significantly lower (better) for protein groups observed in more than 80% of samples (MAE below 0.4) than for those observed in 25–80% of samples (MAE 0.6–0.8).\n\n\n\nScalability\n\nThe deep learning methods, unlike some R-based packages evaluated, scaled well to high-dimensional data (large sample size and many features), which is critical for analyzing precursors or peptides rather than just aggregated protein groups.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Imputation of label-free quantitative mass spectrometry-based proteomics data using self-supervised deep learning"
    ]
  },
  {
    "objectID": "proteomics/webel_2024_38926340.html#conclusions-and-recommendations",
    "href": "proteomics/webel_2024_38926340.html#conclusions-and-recommendations",
    "title": "Imputation of label-free quantitative mass spectrometry-based proteomics data using self-supervised deep learning",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study concludes that deep learning models, particularly the VAE component of PIMMS, provide a powerful, holistic, and scalable approach to handling missing values in LFQ MS data.\nThe traditional reliance on heuristic approaches (like drawing from a down-shifted normal distribution) can be misleading, as not all missing values are due to the limit of detection. Deep learning models avoid this rigid assumption by learning the complex patterns from the data distribution itself.\nThe authors recommend the use of deep learning for imputation in MS-based proteomics, especially on larger datasets, as it improves the sensitivity of differential abundance analysis and can lead to more robust biological conclusions by identifying more significant findings. The PIMMS workflows and code are publicly shared to facilitate reproducibility and adoption.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Imputation of label-free quantitative mass spectrometry-based proteomics data using self-supervised deep learning"
    ]
  },
  {
    "objectID": "proteomics/carrascozanini_2024_39039249.html",
    "href": "proteomics/carrascozanini_2024_39039249.html",
    "title": "Proteomic signatures improve risk prediction for common and rare diseases",
    "section": "",
    "text": "PubMed: 39039249 DOI: 10.1038/s41591-024-03142-z Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "proteomics",
      "Papers",
      "Proteomic signatures improve risk prediction for common and rare diseases"
    ]
  },
  {
    "objectID": "proteomics/carrascozanini_2024_39039249.html#key-findings-proteomics-enhances-disease-risk-prediction",
    "href": "proteomics/carrascozanini_2024_39039249.html#key-findings-proteomics-enhances-disease-risk-prediction",
    "title": "Proteomic signatures improve risk prediction for common and rare diseases",
    "section": "Key Findings: Proteomics Enhances Disease Risk Prediction",
    "text": "Key Findings: Proteomics Enhances Disease Risk Prediction\nThis large-scale study, utilizing the United Kingdom Biobank Pharma Proteomics Project (UKB-PPP), demonstrates that incorporating plasma proteomic signatures significantly improves the prediction of the 10-year incidence risk for a wide range of common and rare diseases compared to models based solely on clinical information or polygenic risk scores (PRS).\n\nSuperior Predictive Performance\nThe core finding is that sparse prediction models using as few as 5 to 20 plasma proteins were superior to models built on basic clinical information alone for 67 pathologically diverse diseases. The median improvement in predictive performance (delta C-index) was 0.07, with gains as high as 0.31 for certain conditions.\n\n\nOutperformance over Clinical Assays\nCrucially, the protein models also outperformed models that combined basic clinical information with data from 37 commonly used clinical assays for 52 diseases. This indicates that the plasma proteome captures unique biological signals not fully reflected in standard clinical blood tests. Diseases where protein models showed significant improvement include: * Hematological Cancers: Multiple myeloma and non-Hodgkin lymphoma. * Neurodegenerative/Neurological Disorders: Motor neuron disease. * Cardiopulmonary Diseases: Pulmonary fibrosis and dilated cardiomyopathy.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Proteomic signatures improve risk prediction for common and rare diseases"
    ]
  },
  {
    "objectID": "proteomics/carrascozanini_2024_39039249.html#study-design-and-methods",
    "href": "proteomics/carrascozanini_2024_39039249.html#study-design-and-methods",
    "title": "Proteomic signatures improve risk prediction for common and rare diseases",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\nThe study integrated deep proteomic data with extensive clinical follow-up data from the UK Biobank cohort.\n\nData and Cohort\n\nCohort: 41,931 individuals from the UKB-PPP.\nProteome: Measurements for approximately 3,000 plasma proteins using Olink Proximity Extension Assays.\nTarget Outcomes: 10-year incidence for 218 common and rare diseases.\n\n\n\nStatistical Modeling\n\nModel Type: Sparse prediction models (using elastic net regression) were developed to predict disease incidence.\nBaseline Model: All models were compared against a clinical model incorporating basic demographic information (age, sex, BMI, smoking status) and a comorbidity score.\nThree Tiers of Models:\n\nClinical model only.\nClinical model + 5 to 20 selected proteins (sparse proteomic signature).\nClinical model + data from 37 clinical assays.\n\nComparison: The performance of these models was quantified using the C-index (Area Under the Curve, AUC) for time-to-event data.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Proteomic signatures improve risk prediction for common and rare diseases"
    ]
  },
  {
    "objectID": "proteomics/carrascozanini_2024_39039249.html#results-disease-specific-insights",
    "href": "proteomics/carrascozanini_2024_39039249.html#results-disease-specific-insights",
    "title": "Proteomic signatures improve risk prediction for common and rare diseases",
    "section": "Results: Disease-Specific Insights",
    "text": "Results: Disease-Specific Insights\n\nMultiple Myeloma\nFor multiple myeloma, a plasma cell cancer, a proteomic signature improved the C-index by 0.28 over the clinical model. The top predictor was Immunoglobulin free light chain kappa (IGFKL), a known marker of plasma cell activity, demonstrating the model’s ability to highlight clinically relevant biomarkers.\n\n\nMotor Neuron Disease (MND)\nFor MND, a fatal neurological disorder, the model’s C-index improved by 0.17. The primary predictors were TDP-43 (Tar-DNA binding protein 43) and proteins associated with neuroinflammation, providing evidence for distinct biological pathways at play years before diagnosis.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Proteomic signatures improve risk prediction for common and rare diseases"
    ]
  },
  {
    "objectID": "proteomics/carrascozanini_2024_39039249.html#robustness-confounding-and-determinants",
    "href": "proteomics/carrascozanini_2024_39039249.html#robustness-confounding-and-determinants",
    "title": "Proteomic signatures improve risk prediction for common and rare diseases",
    "section": "Robustness, Confounding, and Determinants",
    "text": "Robustness, Confounding, and Determinants\nThe authors stress the importance of understanding the determinants of protein variation to avoid spurious associations and Type 1 errors.\n\nImpact of Residual Confounding\nPrioritization analyses revealed that after regressing out variance explained by demographic and other clinical factors, over 80% of non-null associations attenuated, strongly suggesting the presence of residual confounding in protein-outcome relationships. However, robust associations highlighted well-established clinical markers, such as Prostate-Specific Antigen (PSA), demonstrating the method’s ability to retain true biological signals when accounting for confounders.\n\n\nAncestral and Sex Differences\n\nAncestry: The study noted significant ancestral differences in the variance of protein levels explained by genetic factors (pQTLs), primarily driven by cis- and trans-pQTL effects (e.g., differences in minor allele frequency, MAF), which suggests differential effects for the same variant across ancestral groups. These differences were not solely attributable to differences in sample size (N).\nSex: Although few sex-differential genetic effects were found (consistent with findings from large-scale GWASs), approximately one-third of the tested proteins exhibited differences in levels due to varying participant characteristics between males and females (e.g., medication use).\n\n\n\nComparison to Polygenic Risk Scores (PRS) and MR\nThe study confirmed that the proteomic signature provided a greater improvement in prediction over the clinical model than the corresponding PRS for most diseases. However, when attempting to compare protein prediction using genetically imputed plasma protein levels (e.g., in Mendelian Randomization analyses) versus directly measured protein levels with outcomes, there was very little consistency, even after adjusting for protein factors. This highlights a limitation in using imputed protein levels for causal inference in this context.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Proteomic signatures improve risk prediction for common and rare diseases"
    ]
  },
  {
    "objectID": "proteomics/carrascozanini_2024_39039249.html#conclusions-and-future-directions",
    "href": "proteomics/carrascozanini_2024_39039249.html#conclusions-and-future-directions",
    "title": "Proteomic signatures improve risk prediction for common and rare diseases",
    "section": "Conclusions and Future Directions",
    "text": "Conclusions and Future Directions\nThe study strongly supports the use of high-throughput plasma proteomics as an objective, non-invasive method to improve disease risk prediction across a diverse array of conditions. The sparse signatures identified represent potential targets for early intervention and monitoring. The findings underscore the critical importance of characterizing and accounting for protein determinants to distinguish biologically relevant findings from false positives and to ensure accurate risk stratification in clinical applications. The authors suggest that integrating these proteomic biomarkers into clinical practice could facilitate earlier diagnosis and personalized screening programs.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Proteomic signatures improve risk prediction for common and rare diseases"
    ]
  },
  {
    "objectID": "proteomics/kirsher_2025_40999057.html",
    "href": "proteomics/kirsher_2025_40999057.html",
    "title": "Current landscape of plasma proteomics from technical innovations to biological insights and biomarker discovery",
    "section": "",
    "text": "PubMed: 40999057 DOI: 10.1038/s42004-025-01665-1 Overview generated by: Gemini 2.5 Flash, 27/11/2025",
    "crumbs": [
      "proteomics",
      "Papers",
      "Current landscape of plasma proteomics from technical innovations to biological insights and biomarker discovery"
    ]
  },
  {
    "objectID": "proteomics/kirsher_2025_40999057.html#background-and-objective",
    "href": "proteomics/kirsher_2025_40999057.html#background-and-objective",
    "title": "Current landscape of plasma proteomics from technical innovations to biological insights and biomarker discovery",
    "section": "Background and Objective",
    "text": "Background and Objective\nPlasma proteins are critical biomolecules that reflect health and disease states, driving the rapidly expanding field of plasma proteome profiling for biomarker discovery. However, researchers often lack comprehensive, head-to-head comparisons of the diverse and evolving plasma proteomics platforms available.\nThe objective of this study was to address this gap by conducting a direct, systematic comparison of multiple state-of-the-art proteomics platforms to assess their performance, identify key differences, and provide valuable guidance for researchers in selecting appropriate technologies for biomarker discovery and clinical applications.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Current landscape of plasma proteomics from technical innovations to biological insights and biomarker discovery"
    ]
  },
  {
    "objectID": "proteomics/kirsher_2025_40999057.html#methods-platform-benchmarking-on-a-shared-cohort",
    "href": "proteomics/kirsher_2025_40999057.html#methods-platform-benchmarking-on-a-shared-cohort",
    "title": "Current landscape of plasma proteomics from technical innovations to biological insights and biomarker discovery",
    "section": "Methods: Platform Benchmarking on a Shared Cohort",
    "text": "Methods: Platform Benchmarking on a Shared Cohort\nThe study performed a rigorous technical evaluation of plasma proteomics technologies:\n\nPlatform Selection: Eight different plasma proteomics platforms were directly compared. These platforms represented both affinity-based methods (e.g., proximity extension assays) and various mass spectrometry (MS) approaches.\nCoverage: The platforms collectively covered over 13,000 proteins.\nExperimental Design: All platforms were applied to the same human cohort to ensure a direct and unbiased assessment of performance, allowing for systematic comparison of their capabilities, including protein coverage, dynamic range, and reproducibility.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Current landscape of plasma proteomics from technical innovations to biological insights and biomarker discovery"
    ]
  },
  {
    "objectID": "proteomics/kirsher_2025_40999057.html#key-results-and-significance",
    "href": "proteomics/kirsher_2025_40999057.html#key-results-and-significance",
    "title": "Current landscape of plasma proteomics from technical innovations to biological insights and biomarker discovery",
    "section": "Key Results and Significance",
    "text": "Key Results and Significance\nThe systematic assessment provided critical insights into the performance characteristics and trade-offs of the different technologies:\n\nKey Differences and Complementary Strengths: The comparison identified significant differences in platform performance, as well as complementary strengths, highlighting that different platforms are best suited for different research goals.\nTrade-offs in Coverage: The findings specifically illustrated the trade-offs in protein coverage—the number of quantifiable proteins—versus other performance metrics like sensitivity and throughput.\nEssential Resource: This study serves as an essential resource for the research community, offering both technical evaluation and biological insights to support the development of novel diagnostics and therapeutics by informing the critical choice of technology for plasma proteomics profiling.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Current landscape of plasma proteomics from technical innovations to biological insights and biomarker discovery"
    ]
  },
  {
    "objectID": "proteomics/tsuo_2025_40909825.html",
    "href": "proteomics/tsuo_2025_40909825.html",
    "title": "Proteomic prediction of disease largely reflects environmental risk exposure",
    "section": "",
    "text": "PubMed: 40909825 DOI: 10.1101/2025.08.27.25334571 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "proteomics",
      "Papers",
      "Proteomic prediction of disease largely reflects environmental risk exposure"
    ]
  },
  {
    "objectID": "proteomics/tsuo_2025_40909825.html#introduction-and-study-goal",
    "href": "proteomics/tsuo_2025_40909825.html#introduction-and-study-goal",
    "title": "Proteomic prediction of disease largely reflects environmental risk exposure",
    "section": "Introduction and Study Goal",
    "text": "Introduction and Study Goal\nPlasma proteomic signatures are highly effective at predicting disease risk, but the mechanisms behind their predictive value—whether they are causal drivers of disease or non-causal predictors reflecting upstream influences—are largely unknown. This study aims to characterize thousands of protein-disease associations to delineate proteins as potential therapeutic targets (drivers) or objective biomarkers of environmental risk factors.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Proteomic prediction of disease largely reflects environmental risk exposure"
    ]
  },
  {
    "objectID": "proteomics/tsuo_2025_40909825.html#study-design-and-methods",
    "href": "proteomics/tsuo_2025_40909825.html#study-design-and-methods",
    "title": "Proteomic prediction of disease largely reflects environmental risk exposure",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\nThe study utilized blood proteomic data from a subset of the UK Biobank Pharma Proteomics Project (UKB-PPP) (N=45,438) to investigate associations between 2,923 unique plasma proteins and 23 age-related incident disease outcomes.\n\nPartitioning Proteomic Biomarkers\nThe researchers employed a two-pronged approach to categorize protein-disease associations:\n\nCausal Assessment (Drivers): They applied two-sample Mendelian Randomization (MR) using cis-pQTLs as genetic instruments to identify which associations represented a potential causal effect of the protein on the disease.\nEnvironmental Assessment (Predictors): They tested non-causal proteins for associations with major modifiable environmental risk factors, focusing specifically on smoking and alcohol intake due to their large effects on common diseases.\n\n\n\nExposure Quantification Scores\nTo quantify the sensitivity of the plasma proteome to lifestyle factors, the team developed a Proteomic Score for Smoking (SmokingPS) and an AlcoholPS using LASSO regression models trained on the UKB-PPP cohort. These scores were then tested for their ability to predict disease incidence.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Proteomic prediction of disease largely reflects environmental risk exposure"
    ]
  },
  {
    "objectID": "proteomics/tsuo_2025_40909825.html#key-findings-the-dominance-of-environmental-signals",
    "href": "proteomics/tsuo_2025_40909825.html#key-findings-the-dominance-of-environmental-signals",
    "title": "Proteomic prediction of disease largely reflects environmental risk exposure",
    "section": "Key Findings: The Dominance of Environmental Signals",
    "text": "Key Findings: The Dominance of Environmental Signals\n\n1. Causal Drivers are Rare and Disease-Specific\nInitial analysis using Cox proportional hazards models identified a large number of associations (9,308 significant pairs involving 2,122 proteins and 22 diseases). However, MR analysis revealed that only a small subset—8%—of the protein-disease associations tested showed suggestive evidence for a causal relationship.\n\nSpecificity: The proteins identified as putatively causal drivers were generally more likely to pertain to only a single disease, suggesting they represent disease-specific biological signals.\nTherapeutic Implication: These few MR-nominated proteins are critical for mechanistic characterization as they constitute promising therapeutic targets.\n\n\n\n2. Predictive Value Reflects Environmental Exposure\nThe vast majority of protein-disease associations were classified as non-causal predictors. The study found that these proteins often broadly associate with incident disease because they are highly perturbed by environmental risk factors, suggesting their predictive value is as an “environmental sensor.”\n\nSmoking as a Major Driver: The authors discovered that the vast majority (more than 90%) of proteins associated with diseases like lung cancer and COPD are also significantly associated with smoking.\nQuantitative Exposure Readouts: The newly developed proteomic scores demonstrated high accuracy in quantifying environmental factors, achieving an Area Under the Curve (AUC) of 0.96 for smoking and 0.98 for alcohol intake, confirming the plasma proteome’s sensitivity as an objective index of exposure behavior.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Proteomic prediction of disease largely reflects environmental risk exposure"
    ]
  },
  {
    "objectID": "proteomics/tsuo_2025_40909825.html#conclusions-and-recommendations",
    "href": "proteomics/tsuo_2025_40909825.html#conclusions-and-recommendations",
    "title": "Proteomic prediction of disease largely reflects environmental risk exposure",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe study concludes that the plasma proteome’s ability to predict disease is largely driven by its capacity to serve as a quantitative readout of upstream environmental risk factors rather than reflecting only disease-specific or causal processes. This work has significant implications for precision medicine:\n\nInterpretation: Researchers must clarify the roles of plasma protein measurements. Putatively causal proteins are candidates for therapeutic targets, while non-causal, exposure-associated proteins serve as valuable biomarkers for monitoring the impacts of lifestyle and environment.\nDisease Prevention: Proteomic assays offer a path toward measuring the effects of the environment on human health using objective, quantitative, and reproducible methods, which can help guide interventions for disease prevention.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Proteomic prediction of disease largely reflects environmental risk exposure"
    ]
  },
  {
    "objectID": "proteomics/suhre_2024_38412862.html",
    "href": "proteomics/suhre_2024_38412862.html",
    "title": "Genetic associations with ratios between protein levels detect new pQTLs and reveal protein-protein interactions",
    "section": "",
    "text": "PubMed: 38412862 DOI: 10.1016/j.xgen.2024.100506 Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "proteomics",
      "Papers",
      "Genetic associations with ratios between protein levels detect new pQTLs and reveal protein-protein interactions"
    ]
  },
  {
    "objectID": "proteomics/suhre_2024_38412862.html#key-findings-the-power-of-ratio-qtls-rqtls",
    "href": "proteomics/suhre_2024_38412862.html#key-findings-the-power-of-ratio-qtls-rqtls",
    "title": "Genetic associations with ratios between protein levels detect new pQTLs and reveal protein-protein interactions",
    "section": "Key Findings: The Power of Ratio-QTLs (rQTLs)",
    "text": "Key Findings: The Power of Ratio-QTLs (rQTLs)\nThis study introduces a novel concept in proteomic genetics: the systematic analysis of ratios between protein levels to identify ratio quantitative trait loci (rQTLs). The central finding is that using protein ratios dramatically enhances the statistical power to detect genetic variants influencing protein regulation, particularly those linked to functional biological mechanisms like Protein-Protein Interactions (PPIs).\n\nEnhanced Detection and Biological Relevance\n\nIncreased Association Strength: Analyzing ratios between protein pairs, rather than individual protein levels, strengthened associations at known pQTL loci by several hundred orders of magnitude (p-gain).\nNew Loci: The use of ratios increased the number of proteogenomic loci by 25% and helped detect 39 previously unidentified cis-pQTLs.\nPPI Enrichment: The identified rQTLs were 7.6-fold enriched in established protein-protein interactions (PPIs) from the BioGRID database. This high enrichment indicates that the ratio method is particularly effective at capturing genetic effects that modulate the functional interaction or shared regulation of two proteins.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Genetic associations with ratios between protein levels detect new pQTLs and reveal protein-protein interactions"
    ]
  },
  {
    "objectID": "proteomics/suhre_2024_38412862.html#methods-and-study-design",
    "href": "proteomics/suhre_2024_38412862.html#methods-and-study-design",
    "title": "Genetic associations with ratios between protein levels detect new pQTLs and reveal protein-protein interactions",
    "section": "Methods and Study Design",
    "text": "Methods and Study Design\n\nData and Cohort\n\nCohort: A large-scale analysis using proteomic and genetic data from 52,705 samples in the UK Biobank.\nProteome: Measured levels for 1,473 plasma proteins using the Olink Proximity Extension Assay (PEA).\nStatistical Approach: The study analyzed approximately 1.1 million protein ratios, testing each ratio against genetic variants (SNPs) to find rQTLs.\n\n\n\nTheoretical Interpretation of rQTLs\nAn rQTL signal between two proteins (\\(P_1/P_2\\)) can arise from three primary scenarios:\n\nShared Confounder: A genetic variant influences a common confounding factor (e.g., cell type composition, inflammation) that affects both proteins similarly.\nShared Pathway: A variant affects a shared regulatory pathway (e.g., transcription factor) controlling both proteins.\nProtein-Protein Interaction (PPI): A variant directly or indirectly alters the functional or physical interaction between \\(P_1\\) and \\(P_2\\). The strong enrichment of rQTLs in known PPIs suggests this method excels at identifying the latter two, especially PPIs.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Genetic associations with ratios between protein levels detect new pQTLs and reveal protein-protein interactions"
    ]
  },
  {
    "objectID": "proteomics/suhre_2024_38412862.html#results-case-studies-and-functional-insights",
    "href": "proteomics/suhre_2024_38412862.html#results-case-studies-and-functional-insights",
    "title": "Genetic associations with ratios between protein levels detect new pQTLs and reveal protein-protein interactions",
    "section": "Results: Case Studies and Functional Insights",
    "text": "Results: Case Studies and Functional Insights\n\nHigh-Impact rQTLs\nThe study highlighted several high-impact rQTLs with strong biological relevance:\n\nComplement Components: The strongest associations involved proteins in the complement system, an essential part of the innate immune response. The rQTL analysis revealed variants impacting the balance of activation and regulation components, such as C3/C4 and C5/C9.\nDuffy Blood Group (ACKR1): A classic example was the rQTL signal for the Duffy blood type variant (rs12075 in ACKR1). While this variant is known to affect multiple cytokine levels individually, the ratio analysis significantly amplified the association, indicating a shared, highly coordinated regulatory effect on a panel of cytokines.\nNFATC1 Pathway: The analysis revealed rQTLs regulating the ratio of NFATC1 with several other proteins, pointing to complex regulatory mechanisms related to the NFATC1-mediated signaling pathway.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Genetic associations with ratios between protein levels detect new pQTLs and reveal protein-protein interactions"
    ]
  },
  {
    "objectID": "proteomics/suhre_2024_38412862.html#conclusions-and-recommendations",
    "href": "proteomics/suhre_2024_38412862.html#conclusions-and-recommendations",
    "title": "Genetic associations with ratios between protein levels detect new pQTLs and reveal protein-protein interactions",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nThe concept of rQTLs provides a powerful and generalizable theoretical framework for detecting subtle genetic effects that modulate the relative abundance of protein pairs. By focusing on ratios, researchers can effectively filter out much of the biological noise and shared technical variation that obscures signals in single-protein pQTL analyses. The high enrichment for known PPIs validates rQTLs as a superior method for identifying functional protein relationships and should be a standard approach in future proteogenomic studies for drug target discovery. The authors recommend further theoretical development and generalization of the ratio concept.",
    "crumbs": [
      "proteomics",
      "Papers",
      "Genetic associations with ratios between protein levels detect new pQTLs and reveal protein-protein interactions"
    ]
  },
  {
    "objectID": "proteomics/brandes_2020_32665031.html",
    "href": "proteomics/brandes_2020_32665031.html",
    "title": "PWAS: proteome-wide association study-linking genes and phenotypes by functional variation in proteins",
    "section": "",
    "text": "PubMed: 32665031 DOI: 10.1186/s13059-020-02089-x Overview generated by: Gemini 2.5 Flash, 28/11/2025",
    "crumbs": [
      "proteomics",
      "Papers",
      "PWAS: proteome-wide association study-linking genes and phenotypes by functional variation in proteins"
    ]
  },
  {
    "objectID": "proteomics/brandes_2020_32665031.html#key-findings-pwas-as-a-protein-centric-association-method",
    "href": "proteomics/brandes_2020_32665031.html#key-findings-pwas-as-a-protein-centric-association-method",
    "title": "PWAS: proteome-wide association study-linking genes and phenotypes by functional variation in proteins",
    "section": "Key Findings: PWAS as a Protein-Centric Association Method",
    "text": "Key Findings: PWAS as a Protein-Centric Association Method\nThe authors introduce Proteome-Wide Association Study (PWAS), a novel, protein-centric computational method designed to detect gene-phenotype associations that are mediated by alterations in protein function.\nPWAS offers several key advantages over traditional genome-wide association studies (GWAS) and other gene-level methods:\n\nAggregation and Power: It aggregates the signal from all coding variants jointly affecting a protein-coding gene, allowing it to uncover associations that are too weak or spread out to be detected by per-variant GWAS, especially those involving rare variants.\nRecessive Inheritance: PWAS is explicitly designed to model both dominant and recessive modes of heritability, which the authors show to be substantial in complex traits, addressing a mode often neglected in traditional GWAS.\nFunctional Interpretability: By explicitly quantifying the functional damage to the protein, PWAS provides highly interpretable results and is better posed to suggest a causal relationship between the gene and the phenotype.",
    "crumbs": [
      "proteomics",
      "Papers",
      "PWAS: proteome-wide association study-linking genes and phenotypes by functional variation in proteins"
    ]
  },
  {
    "objectID": "proteomics/brandes_2020_32665031.html#study-design-and-methods",
    "href": "proteomics/brandes_2020_32665031.html#study-design-and-methods",
    "title": "PWAS: proteome-wide association study-linking genes and phenotypes by functional variation in proteins",
    "section": "Study Design and Methods",
    "text": "Study Design and Methods\nPWAS is implemented as a two-stage association pipeline using genetic and phenotypic data from large cohorts, such as the UK Biobank (UKBB).\n\nStage 1: Quantifying Protein Functional Damage\n\nVariant Selection: PWAS considers all variants that affect the coding regions of genes (e.g., missense, nonsense, frameshift).\nDamage Prediction: For each variant, a pre-trained machine learning model called FIRM (Functional Interpretation of Rare Missense variants) is used to estimate a variant effect score, which is interpreted as the probability of the protein retaining its function (a score between 0 and 1).\n\n\n\nStage 2: Gene-Level Association Testing\n\nScore Aggregation: The variant effect scores are combined with individual genotype data to create per-gene functional effect scores for each person in the cohort.\nInheritance Modeling: Two separate effect scores are calculated for each gene, explicitly covering dominant (at least one damaging hit) and recessive (at least two damaging hits) inheritance models.\nStatistical Test: These per-gene scores are then statistically tested against the phenotype of interest (binary or continuous) alongside covariates (e.g., sex, age, principal components) to identify significant associations.",
    "crumbs": [
      "proteomics",
      "Papers",
      "PWAS: proteome-wide association study-linking genes and phenotypes by functional variation in proteins"
    ]
  },
  {
    "objectID": "proteomics/brandes_2020_32665031.html#results",
    "href": "proteomics/brandes_2020_32665031.html#results",
    "title": "PWAS: proteome-wide association study-linking genes and phenotypes by functional variation in proteins",
    "section": "Results",
    "text": "Results\n\nPerformance and Comparison\n\nSimulation Analysis: Simulations, based on a protein-centric causal model, demonstrated that PWAS’s advantage is particularly pronounced in detecting associations under the recessive inheritance model.\nReal Data Application (UKBB): PWAS was applied to 49 diverse phenotypes using a cohort of 333,424 filtered UKBB samples.\nComparison to GWAS: The method discovered 2743 gene-phenotype associations that were missed by standard GWAS, which represents 22% of all PWAS-discovered associations.\nComparison to SKAT: PWAS was found to be complementary to SKAT (Sequentially-adjusted association Test), another popular gene-level association method. PWAS recovered more high-quality, known gene-disease associations from the OMIM database (12 associations compared to 7 for SKAT).\n\n\n\nCase Study: Colorectal Cancer\n\nIn a case study of colorectal cancer (2822 cases, 260,127 controls), the well-known predisposition gene MUTYH was not found to be exome-wide significant by standard GWAS (p-values were 6.3E-04 and 1.2E-03).\nIn contrast, PWAS detected the MUTYH association with overwhelming significance (FDR q-value = 2.3E-06) by aggregating the signal from multiple variants. Crucially, the association was found to be significant only under the recessive model, which is consistent with the gene’s known biallelic mutation mechanism for increased cancer risk.",
    "crumbs": [
      "proteomics",
      "Papers",
      "PWAS: proteome-wide association study-linking genes and phenotypes by functional variation in proteins"
    ]
  },
  {
    "objectID": "proteomics/brandes_2020_32665031.html#conclusions-and-recommendations",
    "href": "proteomics/brandes_2020_32665031.html#conclusions-and-recommendations",
    "title": "PWAS: proteome-wide association study-linking genes and phenotypes by functional variation in proteins",
    "section": "Conclusions and Recommendations",
    "text": "Conclusions and Recommendations\nPWAS represents a shift toward using detailed, functional machine learning models to improve gene-phenotype association studies. By focusing on protein function and explicitly modeling different inheritance modes, especially recessive effects, it can recover causal protein-coding genes that are typically missed by variant-centric or expression-centric methods. The authors recommend PWAS as an effective, complementary tool for genetic association studies, providing functionally interpretable results without the need for post-analysis fine-mapping.",
    "crumbs": [
      "proteomics",
      "Papers",
      "PWAS: proteome-wide association study-linking genes and phenotypes by functional variation in proteins"
    ]
  },
  {
    "objectID": "interaction/index.html",
    "href": "interaction/index.html",
    "title": "interaction",
    "section": "",
    "text": "Pathway polygenic risk scores (pPRS) for the analysis of gene-environment interaction\n\n\n\nNovel Method: The paper introduces Pathway Polygenic Risk Scores (pPRS), a targeted method for GxE analysis that restricts genetic variants to specific, biologically informed genomic pathways.\nIncreased Power: Simulations and empirical analysis demonstrated that pPRS yields substantially greater statistical power to detect true GxE interactions compared to using a standard, overall PRS.\nEmpirical Example: The method identified a significant interaction between pPRS based on the TGF-\\(\\beta\\)/GRHR pathway and NSAID use for colorectal cancer (CRC) risk, suggesting the strongest protective effect of NSAIDs in those with high pathway-specific genetic risk.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\n\n\nPIGEON: a statistical framework for estimating gene-environment interaction for polygenic traits\n\n\n\nPIGEON Framework: The paper introduces PIGEON, a unified statistical framework that uses a variance component analytical approach to quantify polygenic Gene-Environment Interaction (GxE).\nInput and Scalability: PIGEON is highly scalable as its estimation procedure requires only GWAS and GWIS summary statistics (not individual-level data).\nKey Objectives: The framework provides rigorous methods for both detecting the presence of GxE (by estimating GxE variance) and interpreting its mechanism (by estimating Oracle PGSxE), demonstrating its utility across multiple empirical settings including gene-by-sex and gene-by-education studies.\n\n\n\n\n\n\n23 January 2026\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "interaction",
      "Papers"
    ]
  }
]